{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#from Levenshtein import distance\n",
    "import psycopg2\n",
    "import random\n",
    "import json\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама_NOUN_мама_10_1_1', 'мыла_NOUN_мыло_10_1_1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merhaba_noun_merhap_1_1_1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_pos_lemma_id_list_tur(clean_ngramm,word_id, ref_id, set_id):\n",
    "    headers_tr = {\n",
    "    'cache-control': 'no-cache',\n",
    "    'content-type': 'application/json',\n",
    "    'postman-token': 'c18af364-c1cb-cc41-0903-063547ac7fce',\n",
    "    }\n",
    "    url_tr = 'http://localhost:8081/analyze'\n",
    "    \n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        data = {\"tokens\" : [word]}\n",
    "        data_dump = json.dumps(data)\n",
    "        r = requests.post(url = url_tr, data=data_dump, headers=headers_tr)\n",
    "        lemma = stemmer.stem(word)\n",
    "        pos = r.text.split(\"+\")[1].lower()\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list_tur(\"merhaba\", 1,1,1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"/Users/lilyakhoang/input/pt_core_news_sm-2.1.0/pt_core_news_sm/pt_core_news_sm-2.1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ferara_VERB_ferara_1_1_1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_pos_lemma_id_list_port(clean_ngramm,word_id, ref_id, set_id):\n",
    "    \n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        for token in nlp(word):\n",
    "            pos = token.pos_\n",
    "            lemma = token.lemma_\n",
    "            break\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list_port(\"ferara\", 1,1,1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_id_list_rus(clean_ngramm,word_id, ref_id, set_id):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        p = morph.parse(word)[0]\n",
    "        lemma = p.normal_form\n",
    "        pos = p.tag.POS\n",
    "        #print(lemma,pos)\n",
    "        if not pos:\n",
    "            pos = \"nodetected\"\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list(\"мама мыла\", 10,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_lemmatized_popular_ngramm(popular_ngramm, popular_ngramm_ids,popular_ngramm_ref_ids,popular_ngramm_set_ids, debug = False):\n",
    "    ngramm_list = []\n",
    "    for word_ind in tqdm(range(len(popular_ngramm))):\n",
    "        word_id = popular_ngramm_ids[word_ind]\n",
    "        ngramm = popular_ngramm[word_ind]\n",
    "        ref_id = popular_ngramm_ref_ids[word_ind]\n",
    "        set_id = popular_ngramm_set_ids[word_ind]\n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation: #punctuation_small_set\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        if debug: print(clean_ngramm)\n",
    "        final_ngramm = get_word_pos_lemma_id_list_port(clean_ngramm,word_id,ref_id,set_id)\n",
    "        ngramm_list.append(final_ngramm)\n",
    "\n",
    "    return ngramm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alph_dict(ngramms_pos_lemm_list):\n",
    "    alph_dict = {}\n",
    "    for ngramm in ngramms_pos_lemm_list:\n",
    "        for elemetn in ngramm:\n",
    "            word = elemetn.split(\"_\")[0]\n",
    "            first_two_el = min(2,len(word))\n",
    "            first_two_letters = word[:first_two_el]\n",
    "            if first_two_letters in alph_dict:\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "            else:\n",
    "                alph_dict[first_two_letters] = []\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "    return alph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prop(db):\n",
    "    popular_qgramms = pd.read_csv(db)\n",
    "    popular_qgramms_list = list(popular_qgramms['translate'])\n",
    "    ppopular_qgramms_list_id = list(popular_qgramms['word_id'])\n",
    "    popular_qgramms_ref_id = list(popular_qgramms['ref_id'])\n",
    "    popular_qgramms_set_id = list(popular_qgramms['setting_id'])\n",
    "    print(popular_qgramms.head())\n",
    "    return popular_qgramms_list, ppopular_qgramms_list_id, popular_qgramms_ref_id, popular_qgramms_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reference(prefix, word_list, id_list, ref_id_list, set_id_list):\n",
    "    ngramm = get_lemmatized_popular_ngramm(word_list,id_list,ref_id_list,set_id_list)\n",
    "    ngramm_dict = get_alph_dict(ngramm)\n",
    "    with open(prefix +\"gramms_top.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm, f, indent = 4, ensure_ascii=False)\n",
    "    with open(prefix + \"gramms_alpha_dict.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm_dict, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_return_ref_files(prefix, db):\n",
    "    word_list, id_list, ref_id_list, set_id_list= get_top_prop(db)\n",
    "    save_reference(prefix, word_list, id_list, ref_id_list, set_id_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/4198 [00:00<01:54, 36.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                translate\n",
      "0    77689       3           1            guarda-costas\n",
      "1   474461       3           1     professor particular\n",
      "2   474466       3           1  representante comercial\n",
      "3   474468       3           1     químico farmacêutico\n",
      "4   474509       3           1         educação musical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4198/4198 [02:04<00:00, 33.62it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"bi\", \"top_10k_bigr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5783 [00:00<05:07, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                     translate\n",
      "0     4434       3           1                   dono de bar\n",
      "1   474448       3           1            fiscal de impostos\n",
      "2   474458       3           1  trabalhador sem qualificação\n",
      "3   474464       3           1             agente de seguros\n",
      "4    38974       3           1           comissária de bordo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5783/5783 [03:34<00:00, 26.99it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"tri\", \"top_10k_trigramm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2008 [00:00<01:44, 19.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                   translate\n",
      "0   296473       3           1  livro de frases essenciais\n",
      "1   728231       3           1      viajar para o exterior\n",
      "2   475869       3           1        salto de pára-quedas\n",
      "3  8816453       3           1      Estar no caminho certo\n",
      "4  4756594       3           1            Estar por um fio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [01:33<00:00, 21.40it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"q\", \"top_10k_qgramm.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
