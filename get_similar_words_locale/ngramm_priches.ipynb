{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#from Levenshtein import distance\n",
    "import psycopg2\n",
    "import random\n",
    "import json\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама_NOUN_мама_10_1_1', 'мыла_NOUN_мыло_10_1_1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['merhaba_noun_merhap_1_1_1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_pos_lemma_id_list_tur(clean_ngramm,word_id, ref_id, set_id):\n",
    "    headers_tr = {\n",
    "    'cache-control': 'no-cache',\n",
    "    'content-type': 'application/json',\n",
    "    'postman-token': 'c18af364-c1cb-cc41-0903-063547ac7fce',\n",
    "    }\n",
    "    url_tr = 'http://localhost:8081/analyze'\n",
    "    \n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        data = {\"tokens\" : [word]}\n",
    "        data_dump = json.dumps(data)\n",
    "        r = requests.post(url = url_tr, data=data_dump, headers=headers_tr)\n",
    "        lemma = stemmer.stem(word)\n",
    "        pos = r.text.split(\"+\")[1].lower()\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list_tur(\"merhaba\", 1,1,1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_id_list_rus(clean_ngramm,word_id, ref_id, set_id):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        p = morph.parse(word)[0]\n",
    "        lemma = p.normal_form\n",
    "        pos = p.tag.POS\n",
    "        #print(lemma,pos)\n",
    "        if not pos:\n",
    "            pos = \"nodetected\"\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list(\"мама мыла\", 10,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_lemmatized_popular_ngramm(popular_ngramm, popular_ngramm_ids,popular_ngramm_ref_ids,popular_ngramm_set_ids, debug = False):\n",
    "    ngramm_list = []\n",
    "    for word_ind in tqdm(range(len(popular_ngramm))):\n",
    "        word_id = popular_ngramm_ids[word_ind]\n",
    "        ngramm = popular_ngramm[word_ind]\n",
    "        ref_id = popular_ngramm_ref_ids[word_ind]\n",
    "        set_id = popular_ngramm_set_ids[word_ind]\n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation: #punctuation_small_set\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        if debug: print(clean_ngramm)\n",
    "        final_ngramm = get_word_pos_lemma_id_list_tur(clean_ngramm,word_id,ref_id,set_id)\n",
    "        ngramm_list.append(final_ngramm)\n",
    "\n",
    "    return ngramm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alph_dict(ngramms_pos_lemm_list):\n",
    "    alph_dict = {}\n",
    "    for ngramm in ngramms_pos_lemm_list:\n",
    "        for elemetn in ngramm:\n",
    "            word = elemetn.split(\"_\")[0]\n",
    "            first_two_el = min(2,len(word))\n",
    "            first_two_letters = word[:first_two_el]\n",
    "            if first_two_letters in alph_dict:\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "            else:\n",
    "                alph_dict[first_two_letters] = []\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "    return alph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prop(db):\n",
    "    popular_qgramms = pd.read_csv(db)\n",
    "    popular_qgramms_list = list(popular_qgramms['translate'])\n",
    "    ppopular_qgramms_list_id = list(popular_qgramms['word_id'])\n",
    "    popular_qgramms_ref_id = list(popular_qgramms['ref_id'])\n",
    "    popular_qgramms_set_id = list(popular_qgramms['setting_id'])\n",
    "    print(popular_qgramms.head())\n",
    "    return popular_qgramms_list, ppopular_qgramms_list_id, popular_qgramms_ref_id, popular_qgramms_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reference(prefix, word_list, id_list, ref_id_list, set_id_list):\n",
    "    ngramm = get_lemmatized_popular_ngramm(word_list,id_list,ref_id_list,set_id_list)\n",
    "    ngramm_dict = get_alph_dict(ngramm)\n",
    "    with open(prefix +\"gramms_top.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm, f, indent = 4, ensure_ascii=False)\n",
    "    with open(prefix + \"gramms_alpha_dict.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm_dict, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_return_ref_files(prefix, db):\n",
    "    word_list, id_list, ref_id_list, set_id_list= get_top_prop(db)\n",
    "    save_reference(prefix, word_list, id_list, ref_id_list, set_id_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id          translate\n",
      "0    85345       2           1  dağıtmak, serpmek\n",
      "1   235754       2           1       Frenk soğanı\n",
      "2    17211       2           1     ateş püskürmek\n",
      "3    35736       2           1    eğitimle ilgili\n",
      "4   188514       2           1       resim yazısı\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [09:42<00:00, 19.17it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"bi\", \"top_10k_bigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5444 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                   translate\n",
      "0    23447       2           1  başlatmak, piyasaya sürmek\n",
      "1    36537       2           1        hizmet etmek, sunmak\n",
      "2     1908       2           1       davranmak, rol yapmak\n",
      "3  1042433       2           1   yasa tasarısını reddetmek\n",
      "4    23522       2           1         götürmek, yol açmak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5444/5444 [09:00<00:00, 10.07it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"tri\", \"top_10k_trigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2355 [00:00<04:56,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                                translate\n",
      "0    23715       2           1              izin vermek, olanak tanımak\n",
      "1  6190377       2           1                  Her şey yoluna girecek.\n",
      "2  6260909       2           1              Henüz bu konuyu düşünmedim.\n",
      "3  6191274       2           1  Doğru yoldasınız. Doğru düşünüyorsunuz.\n",
      "4     1641       2           1                (gemi, uçak, tren) içinde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2355/2355 [04:33<00:00,  8.60it/s]\n"
     ]
    }
   ],
   "source": [
    "get_db_return_ref_files(\"q\", \"top_10k_qgram.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
