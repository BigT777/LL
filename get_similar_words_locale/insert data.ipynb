{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        port=pg_credential.port,\n",
    "                        user=pg_credential.username,\n",
    "                        password=pg_credential.password,\n",
    "                        database=pg_credential.path[1:]) # To remove slash\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"INSERT INTO a_table (c1, c2, c3) VALUES(%s, %s, %s)\", (v1, v2, v3))\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_sql_func(directory, strat_index = 0):\n",
    "    reconnect_count = 0 \n",
    "    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "    cursor = conn.cursor()\n",
    "    files_list = os.listdir(directory)\n",
    "    for unigr_index in tqdm(range(strat_index, len(files_list))):\n",
    "        unigr = files_list[unigr_index]\n",
    "        if unigr.endswith(\"json\"):\n",
    "            #print(unigr)\n",
    "            sim_words_list = []\n",
    "            with open(os.path.join(directory,unigr)) as f:\n",
    "                main_words = json.load(f)\n",
    "                for main_word in main_words:\n",
    "                    word = {}\n",
    "                    #print(main_word['word'])\n",
    "                    #main_word_id = main_word['word_id']\n",
    "                    #main_ref = main_word['ref_id']\n",
    "                    #main_set = main_word['setting_id']\n",
    "                    #orig_word = main_word['word']\n",
    "                    word['word_id'] = main_word['word_id']\n",
    "                    word['ref_id'] = main_word['ref_id']\n",
    "                    word['setting_id'] = main_word['setting_id']\n",
    "                    word['ngramm'] = main_word['ngramm']\n",
    "                    word['simlar_words'] = []\n",
    "                    for similar_word in main_word['simlar_words']:\n",
    "                        if similar_word['word_id'] != word['word_id'] and word['ngramm'] != similar_word['ngramm']:\n",
    "                            sim_word = {}\n",
    "                            sim_word['word_id'] = similar_word['word_id']\n",
    "                            sim_word['ref_id'] = similar_word['ref_id']\n",
    "                            sim_word['setting_id'] = similar_word['setting_id']\n",
    "                            sim_word['ngramm'] = similar_word['ngramm']\n",
    "                            word['simlar_words'].append(sim_word)\n",
    "                        #sim_words_list.append((main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #print(main_word_id, main_ref, main_set, sim_word)\n",
    "                        #cursor.execute(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", (main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #conn.commit() \n",
    "                        #break\n",
    "                        \n",
    "                    #print(word)\n",
    "                    #print(word)\n",
    "                    sim_words_list.append(word)\n",
    "                    #print(len(sim_words_list))\n",
    "                    #break\n",
    "            #for sim_el in sim_words_list:\n",
    "                #print(sim_el)\n",
    "                #print()\n",
    "            #break\n",
    "            #print(unigr,\"all words collected, going to insert\")\n",
    "            #cursor.executemany(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", sim_words_list)\n",
    "            #conn.commit()\n",
    "            #print(\"inseted completed\")\n",
    "            #print(unigr,\"all words collected, going to insert\") \n",
    "\n",
    "            try:\n",
    "                cursor.execute(\"SELECT linguist.add_locales_ngrams( %s ::jsonb)\",(Json(sim_words_list),))\n",
    "                conn.commit() \n",
    "            except:\n",
    "                print(unigr_index, \"crashed\")\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "                reconnect_count += 1\n",
    "                if reconnect_count < 30:\n",
    "                    time.sleep(5)\n",
    "                    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "                    cursor = conn.cursor()\n",
    "                else:\n",
    "                    print(\"reconnection limit\")\n",
    "            #print(\"insert completed\")\n",
    "        \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    #return sim_words_list\n",
    "        \n",
    "#words = insert_sql_func(\"./save_unigr_rus\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 690/3187 [07:59<37:54,  1.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1658/3187 [19:45<14:33,  1.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2011/3187 [24:20<13:30,  1.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3187/3187 [38:27<00:00,  1.01it/s]  \n"
     ]
    }
   ],
   "source": [
    "words_2 = insert_sql_func(\"./save_unigr_rus_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 133/1838 [02:17<27:22,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 477/1838 [08:20<21:30,  1.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 658/1838 [12:23<23:32,  1.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 993/1838 [19:57<19:33,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1472/1838 [31:29<07:48,  1.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1838/1838 [39:47<00:00,  1.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_bigramm_rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:24<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_bigramm_rus_2\", strat_index = 1190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 102/891 [01:25<10:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 891/891 [12:52<00:00,  1.25it/s]  \n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_trigramm_rus\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 174/704 [02:08<06:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [09:35<00:00,  1.65it/s]  \n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_trigramm_rus_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [07:21<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_qgramm_rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [05:01<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_qgramm_rus_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:11<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_unigr_tr_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [06:00<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_unigr_tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:08<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_bigr_tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:47<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_bigr_tr_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:46<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_trigr_tr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:16<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_trigr_tr_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:24<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_qgr_tr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT linguist.add_locales_ngrams('[\n",
    "   {\n",
    "       \"word_id\":10003175,\n",
    "       \"ngramm\":\"неопределенно угрожающий\",\n",
    "       \"ref_id\":1,\n",
    "       \"setting_id\":1,\n",
    "       \"simlar_words\":[\n",
    "           {\n",
    "               \"word_id\":\"17720\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"1\",\n",
    "               \"ngramm\":\"искренне неподдельно\"\n",
    "           },\n",
    "           {\n",
    "               \"word_id\":\"21761\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"2\",\n",
    "               \"ngramm\":\"учреждение заведение\"\n",
    "           }\n",
    "      ]\n",
    "   },\n",
    "   {\n",
    "       \"word_id\":10003491,\n",
    "       \"ngramm\":\"деревянные клинья\",\n",
    "       \"ref_id\":1,\n",
    "       \"setting_id\":1,\n",
    "       \"simlar_words\":[\n",
    "           {\n",
    "               \"word_id\":\"24813\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"1\",\n",
    "               \"ngramm\":\"деревянный молоток\"\n",
    "           },\n",
    "           {\n",
    "               \"word_id\":\"53843\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"4\",\n",
    "               \"ngramm\":\"ужасный страшный\" }\n",
    "        ]\n",
    "}\n",
    "]'::jsonb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert(directory):\n",
    "    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "    cursor = conn.cursor()\n",
    "    files_list = os.listdir(directory)\n",
    "    sim_words_list = []\n",
    "    for unigr_index in tqdm(range(len(files_list))):\n",
    "        unigr = files_list[unigr_index]\n",
    "        if unigr.endswith(\"json\"):\n",
    "            with open(os.path.join(directory,unigr)) as f:\n",
    "                main_words = json.load(f)\n",
    "                \n",
    "                for main_word in main_words:\n",
    "                    main_word_id = main_word['word_id']\n",
    "                    main_ref = main_word['ref_id']\n",
    "                    main_set = main_word['setting_id']\n",
    "                    orig_word = main_word['word']\n",
    "                    \n",
    "                    for sim_word in main_word['simlar_words']:\n",
    "                        sim_words_list.append((main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #print(main_word_id, main_ref, main_set, sim_word)\n",
    "                        #cursor.execute(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", (main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #conn.commit() \n",
    "                        #break\n",
    "            #print(unigr,\"all words collected, going to insert\")\n",
    "            #cursor.executemany(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", sim_words_list)\n",
    "            #conn.commit()\n",
    "            #print(\"inseted completed\")\n",
    "                    #break\n",
    "        #break\n",
    "    #cursor.close()\n",
    "    #conn.close()\n",
    "    return sim_words_list\n",
    "        \n",
    "words = insert(\"./save_unigr_rus_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_first = insert(\"./save_unigr_rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.DataFrame(words_first, columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.to_csv(\"unigr_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df_orig), 500000):\n",
    "    print(i,i+500000)\n",
    "    df = pd.DataFrame(words_first[i:i+500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n",
    "    df.to_csv(\"./unigr2(actual1)/unigr1_\" + str(i) + \"-\" + str(i+500000) + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.DataFrame(words, columns =['word_id', 'ref_id', 'setting_id', 'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500000, len(df_big), 500000):\n",
    "    print(i,i+500000)\n",
    "    df = pd.DataFrame(words[i:i+500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n",
    "    df.to_csv(\"./unigr1(actual_2)/unigr1_\" + str(i) + \"-\" + str(i+500000) + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(words[1000000:1500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"unigr1_1000-1500.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
