{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        port=pg_credential.port,\n",
    "                        user=pg_credential.username,\n",
    "                        password=pg_credential.password,\n",
    "                        database=pg_credential.path[1:]) # To remove slash\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"INSERT INTO a_table (c1, c2, c3) VALUES(%s, %s, %s)\", (v1, v2, v3))\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_sql_func(directory):\n",
    "    reconnect_count = 0 \n",
    "    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "    cursor = conn.cursor()\n",
    "    files_list = os.listdir(directory)\n",
    "    for unigr_index in tqdm(range(len(files_list))):\n",
    "        unigr = files_list[unigr_index]\n",
    "        if unigr.endswith(\"json\"):\n",
    "            #print(unigr)\n",
    "            sim_words_list = []\n",
    "            with open(os.path.join(directory,unigr)) as f:\n",
    "                main_words = json.load(f)\n",
    "                for main_word in main_words:\n",
    "                    word = {}\n",
    "                    #print(main_word['word'])\n",
    "                    #main_word_id = main_word['word_id']\n",
    "                    #main_ref = main_word['ref_id']\n",
    "                    #main_set = main_word['setting_id']\n",
    "                    #orig_word = main_word['word']\n",
    "                    word['word_id'] = main_word['word_id']\n",
    "                    word['ref_id'] = main_word['ref_id']\n",
    "                    word['setting_id'] = main_word['setting_id']\n",
    "                    word['ngramm'] = main_word['ngramm']\n",
    "                    word['simlar_words'] = []\n",
    "                    for similar_word in main_word['simlar_words']:\n",
    "                        if similar_word['word_id'] != word['word_id']:\n",
    "                            sim_word = {}\n",
    "                            sim_word['word_id'] = similar_word['word_id']\n",
    "                            sim_word['ref_id'] = similar_word['ref_id']\n",
    "                            sim_word['setting_id'] = similar_word['setting_id']\n",
    "                            sim_word['ngramm'] = similar_word\n",
    "                            word['simlar_words'].append(sim_word)\n",
    "                        #sim_words_list.append((main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #print(main_word_id, main_ref, main_set, sim_word)\n",
    "                        #cursor.execute(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", (main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #conn.commit() \n",
    "                        #break\n",
    "                        \n",
    "                    #print(word)\n",
    "                    #print(word)\n",
    "                    sim_words_list.append(word)\n",
    "                    #print(len(sim_words_list))\n",
    "                    #break\n",
    "            #for sim_el in sim_words_list:\n",
    "                #print(sim_el)\n",
    "                #print()\n",
    "            #break\n",
    "            #print(unigr,\"all words collected, going to insert\")\n",
    "            #cursor.executemany(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", sim_words_list)\n",
    "            #conn.commit()\n",
    "            #print(\"inseted completed\")\n",
    "            #print(unigr,\"all words collected, going to insert\") \n",
    "\n",
    "            try:\n",
    "                cursor.execute(\"SELECT linguist.add_locales_ngrams( %s ::jsonb)\",(Json(sim_words_list),))\n",
    "                conn.commit() \n",
    "            except:\n",
    "                print(unigr_index, \"crashed\")\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "                reconnect_count += 1\n",
    "                if reconnect_count < 30:\n",
    "                    time.sleep(5)\n",
    "                    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "                    cursor = conn.cursor()\n",
    "                else:\n",
    "                    print(\"reconnection limit\")\n",
    "            #print(\"insert completed\")\n",
    "        \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    #return sim_words_list\n",
    "        \n",
    "#words = insert_sql_func(\"./save_unigr_rus\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 690/3187 [07:59<37:54,  1.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1658/3187 [19:45<14:33,  1.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2011/3187 [24:20<13:30,  1.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 crashed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3187/3187 [38:27<00:00,  1.01it/s]  \n"
     ]
    }
   ],
   "source": [
    "words_2 = insert_sql_func(\"./save_unigr_rus_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1838 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e47e91ad49a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minsert_sql_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./save_results_bigramm_rus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-63979707d626>\u001b[0m in \u001b[0;36minsert_sql_func\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ref_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ref_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'setting_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'setting_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ngramm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simlar_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0msimilar_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmain_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simlar_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'word'"
     ]
    }
   ],
   "source": [
    "insert_sql_func(\"./save_results_bigramm_rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT linguist.add_locales_ngrams('[\n",
    "   {\n",
    "       \"word_id\":10003175,\n",
    "       \"ngramm\":\"неопределенно угрожающий\",\n",
    "       \"ref_id\":1,\n",
    "       \"setting_id\":1,\n",
    "       \"simlar_words\":[\n",
    "           {\n",
    "               \"word_id\":\"17720\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"1\",\n",
    "               \"ngramm\":\"искренне неподдельно\"\n",
    "           },\n",
    "           {\n",
    "               \"word_id\":\"21761\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"2\",\n",
    "               \"ngramm\":\"учреждение заведение\"\n",
    "           }\n",
    "      ]\n",
    "   },\n",
    "   {\n",
    "       \"word_id\":10003491,\n",
    "       \"ngramm\":\"деревянные клинья\",\n",
    "       \"ref_id\":1,\n",
    "       \"setting_id\":1,\n",
    "       \"simlar_words\":[\n",
    "           {\n",
    "               \"word_id\":\"24813\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"1\",\n",
    "               \"ngramm\":\"деревянный молоток\"\n",
    "           },\n",
    "           {\n",
    "               \"word_id\":\"53843\",\n",
    "               \"ref_id\":\"1\",\n",
    "               \"setting_id\":\"4\",\n",
    "               \"ngramm\":\"ужасный страшный\" }\n",
    "        ]\n",
    "}\n",
    "]'::jsonb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert(directory):\n",
    "    conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "    cursor = conn.cursor()\n",
    "    files_list = os.listdir(directory)\n",
    "    sim_words_list = []\n",
    "    for unigr_index in tqdm(range(len(files_list))):\n",
    "        unigr = files_list[unigr_index]\n",
    "        if unigr.endswith(\"json\"):\n",
    "            with open(os.path.join(directory,unigr)) as f:\n",
    "                main_words = json.load(f)\n",
    "                \n",
    "                for main_word in main_words:\n",
    "                    main_word_id = main_word['word_id']\n",
    "                    main_ref = main_word['ref_id']\n",
    "                    main_set = main_word['setting_id']\n",
    "                    orig_word = main_word['word']\n",
    "                    \n",
    "                    for sim_word in main_word['simlar_words']:\n",
    "                        sim_words_list.append((main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #print(main_word_id, main_ref, main_set, sim_word)\n",
    "                        #cursor.execute(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", (main_word_id, main_ref, main_set, 0, 0,0,sim_word,orig_word))\n",
    "                        #conn.commit() \n",
    "                        #break\n",
    "            #print(unigr,\"all words collected, going to insert\")\n",
    "            #cursor.executemany(\"INSERT INTO linguist.mix_locales (word_id, ref_id, setting_id, mix_word_id, mix_ref_id, mix_setting_id, mix_word,orig_word) VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\", sim_words_list)\n",
    "            #conn.commit()\n",
    "            #print(\"inseted completed\")\n",
    "                    #break\n",
    "        #break\n",
    "    #cursor.close()\n",
    "    #conn.close()\n",
    "    return sim_words_list\n",
    "        \n",
    "words = insert(\"./save_unigr_rus_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_first = insert(\"./save_unigr_rus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.DataFrame(words_first, columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.to_csv(\"unigr_2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df_orig), 500000):\n",
    "    print(i,i+500000)\n",
    "    df = pd.DataFrame(words_first[i:i+500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n",
    "    df.to_csv(\"./unigr2(actual1)/unigr1_\" + str(i) + \"-\" + str(i+500000) + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.DataFrame(words, columns =['word_id', 'ref_id', 'setting_id', 'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500000, len(df_big), 500000):\n",
    "    print(i,i+500000)\n",
    "    df = pd.DataFrame(words[i:i+500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n",
    "    df.to_csv(\"./unigr1(actual_2)/unigr1_\" + str(i) + \"-\" + str(i+500000) + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(words[1000000:1500000], columns =['word_id', 'ref_id', 'setting_id', \n",
    "                                   'mix_word_id', 'mix_ref_id', 'mix_setting_id', 'mix_word','orig_word']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"unigr1_1000-1500.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
