{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#from Levenshtein import distance\n",
    "import psycopg2\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lilyakhoang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation_small_set = punctuation.replace(\"-\",\"\")\n",
    "punctuation_small_set = punctuation_small_set.replace(\"'\",\"\")\n",
    "punctuation_small_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sister-in-law', 'is', 'here']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"sister-in-law  is here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister-in-law', 'NN'), ('is', 'VBZ'), ('here', 'RB')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(\"sister-in-law  is here\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_list(clean_ngramm):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    word_pos_list = nltk.pos_tag(tkn)\n",
    "    for word_pos in word_pos_list:\n",
    "        wordnet_pos = get_wordnet_pos(word_pos[1]) \n",
    "        if (wordnet_pos):\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0], pos = wordnet_pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0])\n",
    "        element = word_pos[0] + \"_\" + word_pos[1] + \"_\" + lemma\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_lemmatized_popular_ngramm(popular_ngramm, popular_ngramm_ids, debug = False):\n",
    "    ngramm_list = []\n",
    "    for word_ind in tqdm(range(len(popular_ngramm))):\n",
    "        word_id = popular_ngramm_ids[word_ind]\n",
    "        ngramm = popular_ngramm[word_ind]\n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation: #punctuation_small_set\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        if debug: print(clean_ngramm)\n",
    "        final_ngramm = get_word_pos_lemma_id_list(clean_ngramm,word_id)\n",
    "        ngramm_list.append(final_ngramm)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            final_ngramm = get_word_pos_lemma_id_list(clean_ngramm,word_id)\n",
    "            ngramm_list.append(final_ngramm)\n",
    "            if debug:print(final_ngramm)\n",
    "        except:\n",
    "            if debug:print(\"FAILED\",ngramm)\n",
    "            pass\n",
    "        \"\"\"\n",
    "    return ngramm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_id_list(clean_ngramm,word_id):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    word_pos_list = nltk.pos_tag(tkn)\n",
    "    for word_pos in word_pos_list:\n",
    "        wordnet_pos = get_wordnet_pos(word_pos[1]) \n",
    "        if (wordnet_pos):\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0], pos = wordnet_pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0])\n",
    "        element = word_pos[0] + \"_\" + word_pos[1] + \"_\" + lemma + '_' + str(word_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alph_dict(ngramms_pos_lemm_list):\n",
    "    alph_dict = {}\n",
    "    for ngramm in ngramms_pos_lemm_list:\n",
    "        for elemetn in ngramm:\n",
    "            word = elemetn.split(\"_\")[0]\n",
    "            first_two_el = min(2,len(word))\n",
    "            first_two_letters = word[:first_two_el]\n",
    "            if first_two_letters in alph_dict:\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "            else:\n",
    "                alph_dict[first_two_letters] = []\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "    return alph_dict\n",
    "#bigr_dict = get_alph_dict(bigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_lemma</th>\n",
       "      <th>word_rating</th>\n",
       "      <th>word_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186891</td>\n",
       "      <td>186891.0</td>\n",
       "      <td>1058668</td>\n",
       "      <td>ex-wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18897</td>\n",
       "      <td>18897.0</td>\n",
       "      <td>874133</td>\n",
       "      <td>half-sister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>915447</td>\n",
       "      <td>915447.0</td>\n",
       "      <td>792505</td>\n",
       "      <td>hot pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161361</td>\n",
       "      <td>161361.0</td>\n",
       "      <td>775861</td>\n",
       "      <td>light-pink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1079425</td>\n",
       "      <td>4046161.0</td>\n",
       "      <td>757468</td>\n",
       "      <td>cold colours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id  word_lemma  word_rating    word_value\n",
       "0   186891    186891.0      1058668       ex-wife\n",
       "1    18897     18897.0       874133   half-sister\n",
       "2   915447    915447.0       792505      hot pink\n",
       "3   161361    161361.0       775861    light-pink\n",
       "4  1079425   4046161.0       757468  cold colours"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_db = pd.read_csv(\"top_rating_bigramm.csv\")\n",
    "popular_bigramms = list(b_db['word_value'])\n",
    "popular_bigramms_id = list(b_db['word_id'])\n",
    "b_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1833.93it/s]\n"
     ]
    }
   ],
   "source": [
    "bigramm = det_lemmatized_popular_ngramm(popular_bigramms,popular_bigramms_id, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr_dict = get_alph_dict(bigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ex_JJ_ex_186891', 'wife_NN_wife_186891']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bigramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(bigramm, f, indent = 4)\n",
    "with open(\"bigramms_alpha_dict.json\" ,\"w\") as f:\n",
    "    json.dump(bigr_dict, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_lemma</th>\n",
       "      <th>word_rating</th>\n",
       "      <th>word_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37289</td>\n",
       "      <td>37289.0</td>\n",
       "      <td>1161496</td>\n",
       "      <td>sister-in-law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>286204</td>\n",
       "      <td>286204.0</td>\n",
       "      <td>509262</td>\n",
       "      <td>let, let, let</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228455</td>\n",
       "      <td>228455.0</td>\n",
       "      <td>387085</td>\n",
       "      <td>choose, chose, chosen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>286684</td>\n",
       "      <td>286684.0</td>\n",
       "      <td>349906</td>\n",
       "      <td>sense of humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>487434</td>\n",
       "      <td>487434.0</td>\n",
       "      <td>347649</td>\n",
       "      <td>slide, slid, slid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_id  word_lemma  word_rating             word_value\n",
       "0    37289     37289.0      1161496          sister-in-law\n",
       "1   286204    286204.0       509262          let, let, let\n",
       "2   228455    228455.0       387085  choose, chose, chosen\n",
       "3   286684    286684.0       349906         sense of humor\n",
       "4   487434    487434.0       347649      slide, slid, slid"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_trigramms = pd.read_csv(\"top_rating_trigramm.csv\")\n",
    "popular_trigramms_list = list(popular_trigramms['word_value'])\n",
    "popular_trigramms_list_id = list(popular_trigramms['word_id'])\n",
    "popular_trigramms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 978.95it/s] \n"
     ]
    }
   ],
   "source": [
    "trigramm = det_lemmatized_popular_ngramm(popular_trigramms_list,popular_trigramms_list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramm_dict = get_alph_dict(trigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trigramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(trigramm, f, indent = 4)\n",
    "with open(\"trigramms_dict.json\" ,\"w\") as f:\n",
    "    json.dump(trigramm_dict, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word_lemma</th>\n",
       "      <th>word_rating</th>\n",
       "      <th>word_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98546187</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>to be a ninja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98472100</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>to paint smth black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98220013</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>To be all downhill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97879252</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>on a similar path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97183435</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>to take a richard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_id  word_lemma  word_rating           word_value\n",
       "0  98546187           0           20        to be a ninja\n",
       "1  98472100           0           25  to paint smth black\n",
       "2  98220013           0           28   To be all downhill\n",
       "3  97879252           0           19    on a similar path\n",
       "4  97183435           0           31    to take a richard"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_qgramms = pd.read_csv(\"top_rating_qgramms.csv\")\n",
    "popular_qgramms_list = list(popular_qgramms['word_value'])\n",
    "popular_qgramms_list_id = list(popular_qgramms['word_id'])\n",
    "popular_qgramms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 362.13it/s]\n"
     ]
    }
   ],
   "source": [
    "qgramm = det_lemmatized_popular_ngramm(popular_qgramms_list,popular_qgramms_list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to_TO_to_98546187',\n",
       " 'be_VB_be_98546187',\n",
       " 'a_DT_a_98546187',\n",
       " 'ninja_NN_ninja_98546187']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qgramm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgramm_dict = get_alph_dict(qgramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qgramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(qgramm, f, indent = 4)\n",
    "with open(\"qgramms_dict.json\" ,\"w\") as f:\n",
    "    json.dump(qgramm_dict, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK FOR SIMILAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_string (word_pos_list):\n",
    "    orig_string = ''\n",
    "    for el in word_pos_list:\n",
    "        word = el.split(\"_\")[0]\n",
    "        orig_string += word + ' '\n",
    "    orig_string = orig_string.strip()\n",
    "    return orig_string\n",
    "\n",
    "\n",
    "def get_lemmas_list (word_pos_list):\n",
    "    lemma_list = []\n",
    "    for el in word_pos_list:\n",
    "        lemma = el.split(\"_\")[2]\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list\n",
    "def get_pos_string (word_pos_list):\n",
    "    pos_string = ''\n",
    "    for el in word_pos_list:\n",
    "        pos = el.split(\"_\")[1]\n",
    "        pos_string += pos\n",
    "    return pos_string\n",
    "\n",
    "def copare_lemmas_lists(compared_lemmas, lemmas_from_list, search_range, debug = False):\n",
    "    for lemma_comp in compared_lemmas:\n",
    "        for lemma_list in lemmas_from_list:\n",
    "            if lemma_comp == lemma_list:\n",
    "                if debug:print(\"SAME WORD\")\n",
    "                return False\n",
    "            elif distance(lemma_comp, lemma_list) > 0 and distance(lemma_comp, lemma_list) <= search_range:\n",
    "                continue\n",
    "            else:\n",
    "                if debug:print(\"TOO MUCH lemmas DIFFERENCE\")\n",
    "                return False\n",
    "    return  True\n",
    "\n",
    "def compare_word_pos_lemma_list(compared_wpl, from_list_wpl, search_range, debug = False):\n",
    "    if debug: print(\"COMPARED\",compared_wpl, \"FROM LIST\",from_list_wpl, \"SEARCH RANGE\",search_range)\n",
    "    compared_lemma_list = get_lemmas_list(compared_wpl)\n",
    "    from_list_lemma_list = get_lemmas_list(from_list_wpl)\n",
    "    \n",
    "    compared_pos_string = get_pos_string(compared_wpl)\n",
    "    from_list_pos_string = get_pos_string(from_list_wpl)\n",
    "    if debug:print(compared_lemma_list, from_list_lemma_list, compared_pos_string,from_list_pos_string)\n",
    "    pos_check = False\n",
    "    if distance(compared_pos_string, from_list_pos_string) > 0 and distance(compared_pos_string, from_list_pos_string) <= search_range:\n",
    "        if debug: print(\"POS OK\")\n",
    "        pos_check = True\n",
    "    else:\n",
    "        if debug: print(\"TOO MUCH POS DIFFERENCE\")\n",
    "    lemmas_check = copare_lemmas_lists(compared_lemma_list, from_list_lemma_list, search_range)\n",
    "    \n",
    "    if pos_check == True and lemmas_check == True:\n",
    "        if debug: print(\"all checks OK\")\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT word_id, word_lemma, word_rating, jdesc->>'wd' AS word_value\n",
      "    FROM public.content_words WHERE(word_hash != 0) AND (((array_length(regexp_split_to_array(content_words.jdesc->>'wd', '[ ''-]'), 1) = 2)))\n",
      "    ORDER BY word_rating DESC LIMIT 10 OFFSET 0\n",
      "ex-wife\n",
      "['web-site', 'on-site', 'en-suite', 'fire engine', 'police office', 'outside line', 'live with', 'ice machine', 'lie awake', 'a-tick', 'white lie', 'lime juice', 'white tie', 'prime time', 'wide awake', 'x-rays', 'insider trading', 'i suppose', 'ripped off', 'media kit', 'scare off', 'time lapse', 'pissed off', 'news media', 'dim-witted', 'hi there', 'tumbling down', 'opposite words', 'walk in', 'which one']\n",
      "========\n",
      "half-sister\n",
      "['half-assed', 'half-baked', 'far-sighted', 'web-site', 'on-site', 'hard-wired', 'deep-seated', 'bare-footed', 'self-hatred', 'tight-fisted', 'hot-shot', 'salad-dish', 'small-scale', 'trend-setters', 'well-liked', 'high-minded', 'oval-shaped', 'hands-free', 'hard-boiled', 'dim-witted', 'self-respect', 'self-centered', 'self-obsessed', 'en-suite', 'rose-tinted', 'ill-fated', 'bare-chested', 'water heater', 'self-centred', 'ice-skates']\n",
      "========\n",
      "hot pink\n",
      "['pug in', 'hit man', 'no point', 'n-', 'put on', 'no look', 'tie pin', 'hang on', 'run on', 'join in', 'pop in', 'look in', 'dig in', 'ran into', 'hit on', 'work on', 'look on', 'run into', 'look into', 'no one', 'tipping point', 'hang on', 'fit in', 'hang in', 'look upon', 'run into', 'put into', 'fit into', 'put in', 'pay in']\n",
      "========\n",
      "light-pink\n",
      "['light-minded', 'night-lamp', 'light-brown', 'light-green', 'tight-lipped', 'light-headed', 'tight-fisted', 'mighty frightening', 'bright-eyed', 'high-minded', 'line discipline', 'light-hearted', 'first-born', 'think tank', 'high-pitched', 'things happen', 'teeny-tiny', 'like-minded', 'sing-song', 'line disciplines', 'check-ins', 'a-tick', 'shopping district', 'fish dish', 'Minimalist look', 'parking fine', 'thick hair', 'bank loan', 'improper fraction', 'financial statement']\n",
      "========\n",
      "cold colours\n",
      "['for good', 'counted out', 'folding chair', 'call out', 'co-ed', 'roll out', 'come about', 'booked solid', 'look for', 'called out', 'churn out', 'look over', 'hold-up', 'zoom out', 'roll over', 'look out', 'long for', 'looking for', 'pour out', 'blow out', 'hold out', 'coast along', 'Look out', 'come out', 'come along', \"c'mon\", 'hold over', \"o'er\", 'come clean', 'held out']\n",
      "========\n",
      "ex-husband\n",
      "['bald head', 'egg-shaped', 'red-handed', 'uh-huh', 'hang around', 'u-turn', 'hanging around', 'en-suite', 'x-rays', 'stumbled upon', 'Dominican Republic', 'un-break', 'self-assured', 'current account', 'easy-peasy', 'sea urchin', 'granulated sugar', 'head back', 'In-laws', 'night-lamp', 'wind band', 'self-hatred', 'sing-song', 'stale bread', 'push back', 'arc-boutant', 'evening edition', 'nothing else', 'Banda sea', 't-shirts']\n",
      "========\n",
      "light-orange\n",
      "['light-hearted', 'light-minded', 'light-brown', 'light-headed', 'light-green', 'night-lamp', 'tight-fisted', 'highly-trained', 'left-handed', 'tight-lipped', 'gift-wrapped', 'high-minded', 'first-born', 'fine-grained', 'wishy-washy', 'high-powered', 'fir-tree', 'lemon-tree', 'Chang Jiang', 'high-pitched', 'right lane', 'large-scale', 'bright-eyed', 'short-handed', 'store location', 'mighty frightening', 'sing-song', 'like-minded', 'high-heeled', 'large intestine']\n",
      "========\n",
      "great-grandfather\n",
      "[]\n",
      "========\n",
      "meat grinder\n",
      "['ran into', 'in time', 'get in', 'in front', 'running water', 'run into', 'never mind', 'in giving', 'good point', 'right lane', 'get round', 'in order', 'linear graph', 'in brief', 'blend in', 'in line', 'red hair', 'fair hair', 'order in', 'free ride', 'mineral water', 'hand in', 'get changed', 'in winter', 'and one', 'branding zone', 'white bread', 'ring road', 'run against', 'right hand']\n",
      "========\n",
      "bright green\n",
      "['brown bread', 'get rid', 'best friend', 'fresh bread', 'by Friday', 'big time', 'prime time', 'in earnest', 'kick in', 'put into', 'dropped out', 'get behind', 'drop out', 'side effects', 'watch out', 'wrapped in', 'white tie', 'i am', 'olive oil', 'wish out', 'high time', 'shut out', 'back pain', 'drawing out', 'going out', 'about time', 'current ratio', 'at bay', 'blot out', \"must n't\"]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "def look_for_similar (offset, interval, similar_ngramms_list, debug = False):\n",
    "    output_unigramm_json = []\n",
    "    request = \"\"\"SELECT word_id, word_lemma, word_rating, jdesc->>'wd' AS word_value\n",
    "    FROM public.content_words WHERE(word_hash != 0) AND (((array_length(regexp_split_to_array(content_words.jdesc->>'wd', '[ ''-]'), 1) = 2)))\n",
    "    ORDER BY word_rating DESC LIMIT \"\"\" + str(interval) + \" OFFSET \" + str(offset)\n",
    "    print(request)\n",
    "    conn.rollback()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(request)\n",
    "    for row in cursor:\n",
    "        curr_json = {}\n",
    "        word_id = row[0]\n",
    "        ngramm = row[3]\n",
    "        print(ngramm)\n",
    "        \n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation_small_set:\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        clean_ngramm = clean_ngramm.strip()\n",
    "        random.shuffle(similar_ngramms_list)\n",
    "        word_pos_lemma_list = get_word_pos_lemma_list(clean_ngramm)\n",
    "        collected_similar_ngramms = []\n",
    "        collected_similar_ngramms_count = 0\n",
    "        search_range = 2\n",
    "        handled_similar_indexes = []\n",
    "        while collected_similar_ngramms_count < 30 and search_range < 10:\n",
    "            for similar_ngramm_index in range(len(similar_ngramms_list)):\n",
    "                if (similar_ngramm_index not in handled_similar_indexes):\n",
    "                    similar_ngramm = similar_ngramms_list[similar_ngramm_index]\n",
    "                    comp_result = compare_word_pos_lemma_list(word_pos_lemma_list, similar_ngramm, search_range)\n",
    "                    if comp_result == True:\n",
    "                        handled_similar_indexes.append(similar_ngramm_index)\n",
    "                        orig_line = get_orig_string(similar_ngramm)\n",
    "                        collected_similar_ngramms.append(orig_line)\n",
    "                        collected_similar_ngramms_count += 1\n",
    "                        if debug: print(\"SIMILAR COLLECTED\",collected_similar_ngramms_count )\n",
    "                        if collected_similar_ngramms_count >= 30:\n",
    "                            break\n",
    "                else:\n",
    "                    if debug: print(\"INDEX ALREADY HANDLED\")\n",
    "            search_range += 1\n",
    "        print(collected_similar_ngramms)\n",
    "        print(\"========\")\n",
    "        #curr_json = {\"word_id\":word_id, \"word\":word, \"simlar_words\": sim_words_dict[word]}\n",
    "        #output_unigramm_json.append(curr_json)\n",
    "        \n",
    "    #write_response(output_unigramm_json,offset, offset + interval)\n",
    "\n",
    "look_for_similar(0, 10, bigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Get_NN_Get', 'lost_VBD_lose']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramm[126]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
