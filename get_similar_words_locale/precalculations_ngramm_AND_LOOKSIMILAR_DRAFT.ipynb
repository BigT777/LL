{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#from Levenshtein import distance\n",
    "import psycopg2\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation_small_set = punctuation.replace(\"-\",\"\")\n",
    "punctuation_small_set = punctuation_small_set.replace(\"'\",\"\")\n",
    "punctuation_small_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(\"sister-in-law  is here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'мама'\n",
    "p = morph.parse(w)[0]\n",
    "lemma = p.normal_form\n",
    "pos = p.tag.POS\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(\"sister-in-law  is here\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_list(clean_ngramm):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    word_pos_list = nltk.pos_tag(tkn)\n",
    "    for word_pos in word_pos_list:\n",
    "        wordnet_pos = get_wordnet_pos(word_pos[1]) \n",
    "        if (wordnet_pos):\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0], pos = wordnet_pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word_pos[0])\n",
    "        element = word_pos[0] + \"_\" + word_pos[1] + \"_\" + lemma\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_lemmatized_popular_ngramm(popular_ngramm, popular_ngramm_ids,popular_ngramm_ref_ids,popular_ngramm_set_ids, debug = False):\n",
    "    ngramm_list = []\n",
    "    for word_ind in tqdm(range(len(popular_ngramm))):\n",
    "        word_id = popular_ngramm_ids[word_ind]\n",
    "        ngramm = popular_ngramm[word_ind]\n",
    "        ref_id = popular_ngramm_ref_ids[word_ind]\n",
    "        set_id = popular_ngramm_set_ids[word_ind]\n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation: #punctuation_small_set\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        if debug: print(clean_ngramm)\n",
    "        final_ngramm = get_word_pos_lemma_id_list(clean_ngramm,word_id,ref_id,set_id)\n",
    "        ngramm_list.append(final_ngramm)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            final_ngramm = get_word_pos_lemma_id_list(clean_ngramm,word_id)\n",
    "            ngramm_list.append(final_ngramm)\n",
    "            if debug:print(final_ngramm)\n",
    "        except:\n",
    "            if debug:print(\"FAILED\",ngramm)\n",
    "            pass\n",
    "        \"\"\"\n",
    "    return ngramm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pos_lemma_id_list(clean_ngramm,word_id):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мама_NOUN_мама_10_1_1', 'мыла_NOUN_мыло_10_1_1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_pos_lemma_id_list(clean_ngramm,word_id, ref_id, set_id):\n",
    "    final_ngramm = []\n",
    "    tkn = nltk.word_tokenize(clean_ngramm)\n",
    "    #print(tkn)\n",
    "    for word in tkn:\n",
    "        p = morph.parse(word)[0]\n",
    "        lemma = p.normal_form\n",
    "        pos = p.tag.POS\n",
    "        #print(lemma,pos)\n",
    "        if not pos:\n",
    "            pos = \"nodetected\"\n",
    "        element = word + \"_\" + pos + \"_\" + lemma + '_' + str(word_id) + '_' + str(ref_id) + '_' + str(set_id)\n",
    "        final_ngramm.append(element)\n",
    "    return final_ngramm\n",
    "get_word_pos_lemma_id_list(\"мама мыла\", 10,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alph_dict(ngramms_pos_lemm_list):\n",
    "    alph_dict = {}\n",
    "    for ngramm in ngramms_pos_lemm_list:\n",
    "        for elemetn in ngramm:\n",
    "            word = elemetn.split(\"_\")[0]\n",
    "            first_two_el = min(2,len(word))\n",
    "            first_two_letters = word[:first_two_el]\n",
    "            if first_two_letters in alph_dict:\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "            else:\n",
    "                alph_dict[first_two_letters] = []\n",
    "                alph_dict[first_two_letters].append(ngramm)\n",
    "    return alph_dict\n",
    "#bigr_dict = get_alph_dict(bigramm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_db = pd.read_csv(\"top_10k_bigr.csv\")\n",
    "popular_bigramms = list(b_db['translate'])\n",
    "popular_bigramms_id = list(b_db['word_id'])\n",
    "popular_bigramms_ref_id = list(b_db['ref_id'])\n",
    "popular_bigramms_set_id = list(b_db['setting_id'])\n",
    "b_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm = det_lemmatized_popular_ngramm(popular_bigramms,popular_bigramms_id,\n",
    "                                        popular_bigramms_ref_id,popular_bigramms_set_id, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigr_dict = get_alph_dict(bigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bigramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(bigramm, f, indent = 4, ensure_ascii=False)\n",
    "with open(\"bigramms_alpha_dict.json\" ,\"w\") as f:\n",
    "    json.dump(bigr_dict, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_trigramms = pd.read_csv(\"top_10k_trigr.csv\")\n",
    "popular_trigramms_list = list(popular_trigramms['translate'])\n",
    "popular_trigramms_list_id = list(popular_trigramms['word_id'])\n",
    "popular_trigramms_ref_id = list(popular_trigramms['ref_id'])\n",
    "popular_trigramms_set_id = list(popular_trigramms['setting_id'])\n",
    "popular_trigramms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramm = det_lemmatized_popular_ngramm(popular_trigramms_list,popular_trigramms_list_id,\n",
    "                                        popular_trigramms_ref_id,\n",
    "                                        popular_trigramms_set_id)\n",
    "trigramm_dict = get_alph_dict(trigramm)\n",
    "with open(\"trigramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(trigramm, f, indent = 4, ensure_ascii=False)\n",
    "with open(\"trigramms_dict.json\" ,\"w\") as f:\n",
    "    json.dump(trigramm_dict, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prop(db):\n",
    "    popular_qgramms = pd.read_csv(db)\n",
    "    popular_qgramms_list = list(popular_qgramms['translate'])\n",
    "    ppopular_qgramms_list_id = list(popular_qgramms['word_id'])\n",
    "    popular_qgramms_ref_id = list(popular_qgramms['ref_id'])\n",
    "    popular_qgramms_set_id = list(popular_qgramms['setting_id'])\n",
    "    print(popular_qgramms.head())\n",
    "    return popular_qgramms_list, ppopular_qgramms_list_id, popular_qgramms_ref_id, popular_qgramms_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_id  ref_id  setting_id                                  translate\n",
      "0    37065       1           1                       родной брат и сестра\n",
      "1     3472       1           1                   рука (от кисти до плеча)\n",
      "2    12881       1           1  испытывающий / чувствующий головокружение\n",
      "3  5548373       1           1        с наибольшим количеством просмотров\n",
      "4     5453       1           1           блок, кубик, многоквартирный дом\n"
     ]
    }
   ],
   "source": [
    "popular_qgramms_list, ppopular_qgramms_list_id, popular_qgramms_ref_id, popular_qgramms_set_id = get_top_prop(\"top_10k_qgr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['родной брат и сестра',\n",
       " 'рука (от кисти до плеча)',\n",
       " 'испытывающий / чувствующий головокружение',\n",
       " 'с наибольшим количеством просмотров',\n",
       " 'блок, кубик, многоквартирный дом',\n",
       " 'перерыв, перемена (в школе), перелом',\n",
       " 'общий, всеобщий, публичный, распространённый',\n",
       " 'указывает на направление к, в, на',\n",
       " 'ещё один, другой, новый',\n",
       " 'останавливаться (на короткое время)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_qgramms_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reference(prefix, word_list, id_list, ref_id_list, set_id_list):\n",
    "    ngramm = det_lemmatized_popular_ngramm(word_list,id_list,ref_id_list,set_id_list)\n",
    "    ngramm_dict = get_alph_dict(ngramm)\n",
    "    with open(prefix +\"gramms_top.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm, f, indent = 4, ensure_ascii=False)\n",
    "    with open(prefix + \"gramms_dict.json\" ,\"w\") as f:\n",
    "        json.dump(ngramm_dict, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 551.89it/s]\n"
     ]
    }
   ],
   "source": [
    "save_reference(\"q\", popular_qgramms_list, ppopular_qgramms_list_id, popular_qgramms_ref_id, popular_qgramms_set_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_qgramms = pd.read_csv(\"top_rating_qgramms.csv\")\n",
    "popular_qgramms_list = list(popular_qgramms['word_value'])\n",
    "popular_qgramms_list_id = list(popular_qgramms['word_id'])\n",
    "popular_qgramms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgramm = det_lemmatized_popular_ngramm(popular_qgramms_list,popular_qgramms_list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgramm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgramm_dict = get_alph_dict(qgramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qgramms_top.json\" ,\"w\") as f:\n",
    "    json.dump(qgramm, f, indent = 4)\n",
    "with open(\"qgramms_dict.json\" ,\"w\") as f:\n",
    "    json.dump(qgramm_dict, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK FOR SIMILAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_string (word_pos_list):\n",
    "    orig_string = ''\n",
    "    for el in word_pos_list:\n",
    "        word = el.split(\"_\")[0]\n",
    "        orig_string += word + ' '\n",
    "    orig_string = orig_string.strip()\n",
    "    return orig_string\n",
    "\n",
    "\n",
    "def get_lemmas_list (word_pos_list):\n",
    "    lemma_list = []\n",
    "    for el in word_pos_list:\n",
    "        lemma = el.split(\"_\")[2]\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list\n",
    "def get_pos_string (word_pos_list):\n",
    "    pos_string = ''\n",
    "    for el in word_pos_list:\n",
    "        pos = el.split(\"_\")[1]\n",
    "        pos_string += pos\n",
    "    return pos_string\n",
    "\n",
    "def copare_lemmas_lists(compared_lemmas, lemmas_from_list, search_range, debug = False):\n",
    "    for lemma_comp in compared_lemmas:\n",
    "        for lemma_list in lemmas_from_list:\n",
    "            if lemma_comp == lemma_list:\n",
    "                if debug:print(\"SAME WORD\")\n",
    "                return False\n",
    "            elif distance(lemma_comp, lemma_list) > 0 and distance(lemma_comp, lemma_list) <= search_range:\n",
    "                continue\n",
    "            else:\n",
    "                if debug:print(\"TOO MUCH lemmas DIFFERENCE\")\n",
    "                return False\n",
    "    return  True\n",
    "\n",
    "def compare_word_pos_lemma_list(compared_wpl, from_list_wpl, search_range, debug = False):\n",
    "    if debug: print(\"COMPARED\",compared_wpl, \"FROM LIST\",from_list_wpl, \"SEARCH RANGE\",search_range)\n",
    "    compared_lemma_list = get_lemmas_list(compared_wpl)\n",
    "    from_list_lemma_list = get_lemmas_list(from_list_wpl)\n",
    "    \n",
    "    compared_pos_string = get_pos_string(compared_wpl)\n",
    "    from_list_pos_string = get_pos_string(from_list_wpl)\n",
    "    if debug:print(compared_lemma_list, from_list_lemma_list, compared_pos_string,from_list_pos_string)\n",
    "    pos_check = False\n",
    "    if distance(compared_pos_string, from_list_pos_string) > 0 and distance(compared_pos_string, from_list_pos_string) <= search_range:\n",
    "        if debug: print(\"POS OK\")\n",
    "        pos_check = True\n",
    "    else:\n",
    "        if debug: print(\"TOO MUCH POS DIFFERENCE\")\n",
    "    lemmas_check = copare_lemmas_lists(compared_lemma_list, from_list_lemma_list, search_range)\n",
    "    \n",
    "    if pos_check == True and lemmas_check == True:\n",
    "        if debug: print(\"all checks OK\")\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_similar (offset, interval, similar_ngramms_list, debug = False):\n",
    "    output_unigramm_json = []\n",
    "    request = \"\"\"SELECT word_id, word_lemma, word_rating, jdesc->>'wd' AS word_value\n",
    "    FROM public.content_words WHERE(word_hash != 0) AND (((array_length(regexp_split_to_array(content_words.jdesc->>'wd', '[ ''-]'), 1) = 2)))\n",
    "    ORDER BY word_rating DESC LIMIT \"\"\" + str(interval) + \" OFFSET \" + str(offset)\n",
    "    print(request)\n",
    "    conn.rollback()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(request)\n",
    "    for row in cursor:\n",
    "        curr_json = {}\n",
    "        word_id = row[0]\n",
    "        ngramm = row[3]\n",
    "        print(ngramm)\n",
    "        \n",
    "        clean_ngramm = ''\n",
    "        for char in ngramm:\n",
    "            if char not in punctuation_small_set:\n",
    "                clean_ngramm += char\n",
    "            else:\n",
    "                clean_ngramm += ' '\n",
    "        clean_ngramm = clean_ngramm.strip()\n",
    "        random.shuffle(similar_ngramms_list)\n",
    "        word_pos_lemma_list = get_word_pos_lemma_list(clean_ngramm)\n",
    "        collected_similar_ngramms = []\n",
    "        collected_similar_ngramms_count = 0\n",
    "        search_range = 2\n",
    "        handled_similar_indexes = []\n",
    "        while collected_similar_ngramms_count < 30 and search_range < 10:\n",
    "            for similar_ngramm_index in range(len(similar_ngramms_list)):\n",
    "                if (similar_ngramm_index not in handled_similar_indexes):\n",
    "                    similar_ngramm = similar_ngramms_list[similar_ngramm_index]\n",
    "                    comp_result = compare_word_pos_lemma_list(word_pos_lemma_list, similar_ngramm, search_range)\n",
    "                    if comp_result == True:\n",
    "                        handled_similar_indexes.append(similar_ngramm_index)\n",
    "                        orig_line = get_orig_string(similar_ngramm)\n",
    "                        collected_similar_ngramms.append(orig_line)\n",
    "                        collected_similar_ngramms_count += 1\n",
    "                        if debug: print(\"SIMILAR COLLECTED\",collected_similar_ngramms_count )\n",
    "                        if collected_similar_ngramms_count >= 30:\n",
    "                            break\n",
    "                else:\n",
    "                    if debug: print(\"INDEX ALREADY HANDLED\")\n",
    "            search_range += 1\n",
    "        print(collected_similar_ngramms)\n",
    "        print(\"========\")\n",
    "        #curr_json = {\"word_id\":word_id, \"word\":word, \"simlar_words\": sim_words_dict[word]}\n",
    "        #output_unigramm_json.append(curr_json)\n",
    "        \n",
    "    #write_response(output_unigramm_json,offset, offset + interval)\n",
    "\n",
    "look_for_similar(0, 10, bigramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm[126]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
