{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import psycopg2\n",
    "import copy\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "#fasttext = FastTextKeyedVectors.load(\"/Users/lilyakhoang/input/araneum_none_fasttextskipgram_300_5_2018/araneum_none_fasttextskipgram_300_5_2018.model\")\n",
    "fasttext = FastTextKeyedVectors.load('/Users/nigula/input/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "full_punctuation = punctuation + \"–\" + \",\" + \"»\" + \"«\" + \"…\" +'’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_def_info(word, word_id, from_lang, to_lang, multisence_dict, print_ouput = False):\n",
    "    multisence_dict[word] = {'word_id': word_id, 'lemma_branch':[], 'wordforms_branch':{}}\n",
    "    definitions = []\n",
    "    same_page = False\n",
    "    for index in range(1,4):\n",
    "        url = \"https://dictionary.cambridge.org/dictionary/\" + from_lang + \"-\" + to_lang + \"/\" + word + \"_\" + str(index)\n",
    "        if print_ouput:print(url)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8' \n",
    "        soup = bs(response.text, 'html.parser')\n",
    "        current_definition = {}\n",
    "        for link in soup.find_all([\"span\",\"b\"]):\n",
    "    \n",
    "            classes = link.get('class')\n",
    "            if classes and \"def\" in classes:\n",
    "                if link.text.strip() in definitions:\n",
    "                    same_page = True\n",
    "                    break\n",
    "                if print_ouput: print(\"DEFINITION\", link.text.strip())\n",
    "                current_definition['definition'] = link.text.strip()\n",
    "                definitions.append(link.text.strip())\n",
    "                \n",
    "            if classes and \"def-body\" in classes:\n",
    "                if print_ouput:print(\"RUS_DEFIN\", link.find(\"span\", attrs = {'class':\"trans\"}).text.strip())\n",
    "                current_definition['local_word'] = link.find(\"span\", attrs = {'class':\"trans\"}).text.strip()\n",
    "                current_definition['examples'] = []\n",
    "                examples_count = 0\n",
    "                for ex in link.find_all(\"span\", attrs = {'class':\"eg\"}):\n",
    "                    if examples_count > 3: break\n",
    "                    current_definition['examples'].append(ex.text.strip())\n",
    "                    examples_count += 1\n",
    "                    if print_ouput:print(\"ENG_DEFIN_EX\", ex.text.strip())\n",
    "                if print_ouput:print(current_definition)\n",
    "                multisence_dict[word]['lemma_branch'].append(current_definition)\n",
    "                current_definition = {}\n",
    "                if print_ouput:print(\"===\")\n",
    "\n",
    "        if same_page == True:\n",
    "            break\n",
    "        time.sleep(random.uniform(0.01,0.1))\n",
    "sence_dict = {}      \n",
    "get_def_info(\"book\", 10, \"english\",\"russian\",sence_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 10,\n",
       " 'lemma_branch': [{'definition': 'a set of pages fastened together in a cover for people to read',\n",
       "   'local_word': 'книга',\n",
       "   'examples': ['a book about animals']},\n",
       "  {'definition': 'a set of stamps, tickets, etc that are fastened together inside a cover',\n",
       "   'local_word': 'книжечка',\n",
       "   'examples': []},\n",
       "  {'definition': 'a set of pages fastened together in a cover and used for writing on',\n",
       "   'local_word': 'записная книжка',\n",
       "   'examples': ['an address book']},\n",
       "  {'definition': 'to arrange to use or do something at a particular time in the future',\n",
       "   'local_word': 'бронировать',\n",
       "   'examples': ['to book a ticket/hotel room',\n",
       "    \"We've booked a trip to Spain for next month.\",\n",
       "    'Sorry, the hotel is fully booked (= has no more rooms).']},\n",
       "  {'definition': 'to officially accuse someone of a crime',\n",
       "   'local_word': 'заводить дело',\n",
       "   'examples': ['Detectives booked him for resisting arrest.']},\n",
       "  {'definition': 'If a sports official books you, they write an official record of something you have done wrong.',\n",
       "   'local_word': 'штрафовать (в спорте)',\n",
       "   'examples': ['The referee booked two players for fighting during the game.']}],\n",
       " 'wordforms_branch': {}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sence_dict['book']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def nltk_lemmatize(word):\n",
    "    pos = pos_tag(word_tokenize(word))[0][1]\n",
    "    wordnet_pos = get_wordnet_pos(pos)\n",
    "    if wordnet_pos:\n",
    "        lemma = lemmatizer.lemmatize(word, pos = wordnet_pos)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma\n",
    "\n",
    "nltk_lemmatize(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наличие\n",
      "после\n",
      "имеющих\n",
      "имеющие\n",
      "чтобы\n",
      "имея\n",
      "весело\n",
      "необходимости\n",
      "того\n",
      "оказывает\n",
      "есть\n",
      "имеют\n",
      "имеет\n",
      "поскольку\n",
      "оказывают\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  наличие \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  после \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  имеющих \n",
      "\n",
      "Such policy completely ensures enrolment in the higher education system of students having such social status.\n",
      "Article 120 of the Labour Code fixes the duration of basic leave for employees recognized as having rendered special service to Azerbaijan.\n",
      "Such persons should be coded as having no fixed place of work.\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  имеющие \n",
      "\n",
      "The President was given the authority to issue decrees having the force of law.\n",
      "The Indian Supreme Court has interpreted such provisions as having real significance.\n",
      "Grant me, Divine Father, the delegation of having power over malevolent spirits.\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  чтобы \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  имея \n",
      "\n",
      "Not having a rope, you can easily lose each other.\n",
      "It's like I'm having some kind of a breakdown.\n",
      "They're just having a big meeting about next fall's chorus program.\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  весело \n",
      "\n",
      "You know, for a moment there, you were actually having fun, Detective.\n",
      "It looked like you were having so much fun together.\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  необходимости \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  того \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  оказывает \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  есть \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  имеют \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  имеет \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  поскольку \n",
      "\n",
      "========================================\n",
      "\n",
      "EXAMPLES FOR SENCE  оказывают \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sencewords_and_examples_reverso(eng_wordform, from_lang, to_lang, print_output = False):\n",
    "    wordform_localsencewords_examples_dict = {}\n",
    "    url = \"https://context.reverso.net/перевод/\" + from_lang + \"-\" + to_lang + \"/\" + eng_wordform \n",
    "    login = {'inUserName': 'n.babakov@lingualeo.com', 'inUserPass': '33vec33'}\n",
    "    header_main = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(url, headers=header_main, data = login)\n",
    "    response.encoding = 'utf-8' \n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    first_string_passed = False\n",
    "    translations_variants = []\n",
    "    for link in soup.find_all(\"a\", attrs={\"class\" : \"translation\"}):\n",
    "        if first_string_passed == True:\n",
    "            sence_word = link.text.strip()\n",
    "            if print_output:print(sence_word)\n",
    "            wordform_localsencewords_examples_dict[sence_word] = []\n",
    "            translations_variants.append(sence_word)\n",
    "        first_string_passed = True\n",
    "    time.sleep(random.uniform(0.01,0.2))\n",
    "    for transl_word in translations_variants:\n",
    "        time.sleep(random.uniform(0.01,0.2))\n",
    "        url_example = \"https://context.reverso.net/перевод/\" + to_lang + \"-\" + from_lang + \"/\" + transl_word\n",
    "        response_ex = requests.get(url_example, headers=header_main)\n",
    "        soup_sub = bs(response_ex.text, 'html.parser')\n",
    "        if print_output:\n",
    "            print(\"========================================\")\n",
    "            print(\"\\nEXAMPLES FOR SENCE \",transl_word,\"\\n\" )\n",
    "        examples_count = 0\n",
    "        for l in soup_sub.find_all('span', attrs = {'class':'text', 'lang':'en'}):\n",
    "            example = l.text.strip()\n",
    "            #print(example, word in example, type(l.text.strip()), word)\n",
    "            regex = \"\\W\" + eng_wordform + \"\\w{0,1}\\W\"\n",
    "            find = re.findall(regex, example)\n",
    "            if len(find) > 0 :\n",
    "                wordform_localsencewords_examples_dict[transl_word].append(example)\n",
    "                if print_output: print(example)\n",
    "                examples_count += 1\n",
    "                if examples_count >= 3: break\n",
    "    return wordform_localsencewords_examples_dict\n",
    "w_locsence_ex_dict = get_sencewords_and_examples_reverso('having',\"английский\",\"русский\", print_output = True )   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'наличие': [],\n",
       " 'после': [],\n",
       " 'имеющих': ['Such policy completely ensures enrolment in the higher education system of students having such social status.',\n",
       "  'Article 120 of the Labour Code fixes the duration of basic leave for employees recognized as having rendered special service to Azerbaijan.',\n",
       "  'Such persons should be coded as having no fixed place of work.'],\n",
       " 'имеющие': ['The President was given the authority to issue decrees having the force of law.',\n",
       "  'The Indian Supreme Court has interpreted such provisions as having real significance.',\n",
       "  'Grant me, Divine Father, the delegation of having power over malevolent spirits.'],\n",
       " 'чтобы': [],\n",
       " 'имея': ['Not having a rope, you can easily lose each other.',\n",
       "  \"It's like I'm having some kind of a breakdown.\",\n",
       "  \"They're just having a big meeting about next fall's chorus program.\"],\n",
       " 'весело': ['You know, for a moment there, you were actually having fun, Detective.',\n",
       "  'It looked like you were having so much fun together.'],\n",
       " 'необходимости': [],\n",
       " 'того': [],\n",
       " 'оказывает': [],\n",
       " 'есть': [],\n",
       " 'имеют': [],\n",
       " 'имеет': [],\n",
       " 'поскольку': [],\n",
       " 'оказывают': []}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_locsence_ex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~–,»«…’'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49991900191938105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l = ['книга', 'книжечка', 'записная книжка', 'бронировать', 'заводить дело', 'штрафовать (в спорте)']\n",
    "word = 'забронировать'\n",
    "\n",
    "def look_for_sublemma_word(wordform, lemmas_list, apply_w2v = False):\n",
    "    clean_wordform = ''\n",
    "    for ch in wordform:\n",
    "        if ch not in full_punctuation:\n",
    "            clean_wordform += ch\n",
    "    wordform = clean_wordform\n",
    "    \n",
    "    if len(wordform.split()) == 1 and apply_w2v == True:\n",
    "        p = morph.parse(wordform)[0]\n",
    "        lemma = p.normal_form\n",
    "        wordform = np.array(fasttext[lemma]).reshape(1, -1)\n",
    "    elif len(wordform.split()) > 1 and apply_w2v == True:\n",
    "        try:\n",
    "            words_vector = np.zeros(300).reshape(1, -1)\n",
    "            count = 0 \n",
    "            for word in wordform.split():\n",
    "                p = morph.parse(word)[0]\n",
    "                lemma = p.normal_form\n",
    "                words_vector += np.array(fasttext[lemma]).reshape(1, -1)\n",
    "                count += 1\n",
    "            wordform = words_vector / count\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    if isinstance(lemmas_list, list) or isinstance(lemmas_list, set):\n",
    "        for lemma in lemmas_list:\n",
    "            if len(lemma.split()) == 1:\n",
    "                if wordform in lemma or lemma in wordform:\n",
    "                    #print(wordform)\n",
    "                    return lemma\n",
    "        return None\n",
    "    elif isinstance(lemmas_list, str):\n",
    "        if apply_w2v == False:\n",
    "            if apply_w2v == False:\n",
    "                if wordform in lemmas_list or lemmas_list in wordform:\n",
    "                    return lemmas_list\n",
    "                else:\n",
    "                    return None\n",
    "        \n",
    "        elif apply_w2v == True:\n",
    "            words_vector = np.zeros(300).reshape(1, -1)\n",
    "            try:\n",
    "                count = 0 \n",
    "                for word in lemmas_list.split():\n",
    "                    p = morph.parse(word)[0]\n",
    "                    lemma = p.normal_form\n",
    "                    words_vector += np.array(fasttext[lemma]).reshape(1, -1)\n",
    "                    count += 1\n",
    "                words_vector = words_vector / count\n",
    "                return list(cosine_similarity(words_vector, wordform))[0][0]\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "    else:\n",
    "        return None #\"no type\" + str(type(lemmas_list))\n",
    "\n",
    "#look_for_sublemma_word(\"желтая пломба\", 'желтая пломба', apply_w2v = True) \n",
    "look_for_sublemma_word(\"занятый член\", 'занятый хер выавы', apply_w2v = True) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lemma_context_vs_oxford_examples(eng_lemma, local_wordsence_examples_dict, general_dict, print_output = False):\n",
    "    oxford_sensewords_list = []\n",
    "    oxford_lemmas_examples_pointers_dict = {}\n",
    "    for definition_element in general_dict[eng_lemma]['lemma_branch']:\n",
    "        #if print_output: print(definition_element)\n",
    "        oxford_sensewords_list.append(definition_element['local_word'])\n",
    "        oxford_lemmas_examples_pointers_dict[definition_element['local_word']] = definition_element['examples']\n",
    "    if print_output: print(\"WILL LOOK THROUGH LOCAL WORDFORMS\", oxford_sensewords_list)\n",
    "    oxford_sensewords_list = set(oxford_sensewords_list)\n",
    "    for context_local_lemma in local_wordsence_examples_dict.keys():\n",
    "        if print_output: print(\"\\nNOWPROCESSING\", context_local_lemma)\n",
    "        orig_intersection_lemma = look_for_sublemma_word(context_local_lemma, oxford_sensewords_list)\n",
    "        if not orig_intersection_lemma:\n",
    "            if print_output: print(context_local_lemma, \"\\nno such lemma in orig dict\")\n",
    "            pass\n",
    "        elif len(local_wordsence_examples_dict[context_local_lemma]) == 0:\n",
    "            if print_output: print(context_local_lemma, \"\\ndoes not have examples with english wodform\")\n",
    "            pass\n",
    "        else:\n",
    "            #general_dict[eng_lemma]['lemma_branch'][context_local_lemma]['examples'].extend(local_wordsence_examples_dict[context_local_lemma][:3])\n",
    "            if print_output:print(oxford_lemmas_examples_pointers_dict[orig_intersection_lemma])\n",
    "            examples_len = len(oxford_lemmas_examples_pointers_dict[orig_intersection_lemma])\n",
    "            extend_len = 4 - examples_len\n",
    "            if extend_len >0:\n",
    "                oxford_lemmas_examples_pointers_dict[orig_intersection_lemma].extend(local_wordsence_examples_dict[context_local_lemma][:extend_len])\n",
    "            if print_output:print(\"add\",local_wordsence_examples_dict[context_local_lemma][:3] )\n",
    "#dct = copy.deepcopy(sence_dict)\n",
    "#merge_lemma_context_vs_oxford_examples(\"book\",w_locsence_ex_dict, dct, print_output = True)        \n",
    "#dct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_locsence_ex_vs_lemma_structure(eng_wordform, eng_lemma,wordform_id, local_wordsence_examples_dict, general_dict, print_output = False):\n",
    "    general_dict[eng_lemma]['wordforms_branch'][eng_wordform] = {'word_id':wordform_id,'wordsenses':[]}\n",
    "    oxford_sensewords_list = []\n",
    "    for definition_element in general_dict[eng_lemma]['lemma_branch']:\n",
    "        #if print_output: print(definition_element)\n",
    "        oxford_sensewords_list.append(definition_element['local_word'])\n",
    "    if print_output: print(\"WILL LOOK THROUGH LOCAL WORDFORMS\", oxford_sensewords_list)\n",
    "    oxford_sensewords_list = set(oxford_sensewords_list)\n",
    "    successfull_merge_count = 0\n",
    "    for local_wordform in local_wordsence_examples_dict.keys():\n",
    "        p = morph.parse(local_wordform)[0]\n",
    "        local_lemma = p.normal_form\n",
    "        if print_output: print(\"\\nNOWPROCESSING\", local_wordform, local_lemma, look_for_sublemma_word (local_lemma, oxford_sensewords_list))\n",
    "        if look_for_sublemma_word (local_wordform, oxford_sensewords_list):\n",
    "            if print_output: print(local_wordform, \"\\nis original lemma itself\")\n",
    "            pass\n",
    "        elif len(local_wordsence_examples_dict[local_wordform]) == 0:\n",
    "            if print_output: print(local_wordform, \"\\ndoes not have examples with english wodform\")\n",
    "            pass\n",
    "        else:\n",
    "            if print_output: print(local_wordform, \"\\nno match but no limitations\")\n",
    "            local_wordform_dict = {\"local_word\":local_wordform,\"examples\":local_wordsence_examples_dict[local_wordform][:3]}\n",
    "            if print_output: print(\"LOCAL_DICT_OBJECT\",local_wordform_dict)\n",
    "            general_dict[eng_lemma]['wordforms_branch'][eng_wordform]['wordsenses'].append(local_wordform_dict)\n",
    "        \"\"\"\n",
    "        elif look_for_sublemma_word (local_lemma, oxford_sensewords_list) == True:\n",
    "            if print_output: print(local_wordform, \"\\nlocal_wordform to local lemma or viceversa match\")\n",
    "            successfull_merge_count += 1\n",
    "            local_wordform_dict = {\"sence_wordform_local\":local_wordform,\n",
    "                                   \"context\":[]}\n",
    "            for definition_element in general_dict[eng_lemma]['lemma_branch']:\n",
    "                \n",
    "                if look_for_sublemma_word (local_lemma, definition_element['local_word']):\n",
    "                    local_wordform_examples = copy.deepcopy(definition_element['examples'])\n",
    "                    local_wordform_dict['context'] = local_wordform_examples\n",
    "                    try:\n",
    "                        local_wordform_dict['context'].extend(local_wordsence_examples_dict[local_wordform][:3])\n",
    "                        definition_element['examples'].extend(local_wordsence_examples_dict[local_wordform][:3])\n",
    "                    except:\n",
    "                        local_wordform_dict['context'].extend(local_wordsence_examples_dict[definition_element['local_word']][:3])\n",
    "                        definition_element['examples'].extend(local_wordsence_examples_dict[definition_element['local_word']][:3])\n",
    "            general_dict[eng_lemma]['wordforms_branch'][eng_wordform].append(local_wordform_dict)\n",
    "            if print_output: print(\"LOCAL_DICT_OBJECT\",local_wordform_dict)\n",
    "        \"\"\"\n",
    "    if print_output:\n",
    "        if successfull_merge_count == 0: print(\"NO LOCAL INTERSECTIONS FOR WORDWORM\", eng_wordform)\n",
    "    if len(general_dict[eng_lemma]['wordforms_branch'][eng_wordform]['wordsenses']) == 0:\n",
    "        del general_dict[eng_lemma]['wordforms_branch'][eng_wordform]\n",
    "        \n",
    "        \n",
    "#merge_locsence_ex_vs_lemma_structure(\"booking\",\"book\",w_locsence_ex_dict, big_diCKt, print_output = True)        \n",
    "#big_diCKt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"one_word_transalte.json\", \"r\") as f:\n",
    "    one_word_transalte = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6401495"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cosine_similarity((np.array(fasttext[\"мама\"]).reshape(1, -1) + np.array(fasttext[\"папа\"]).reshape(1, -1) / 2), np.array(fasttext[\"мамка\"]).reshape(1, -1)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "a = {'a':1}\n",
    "del a['a']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мать'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ll_translate(text):\n",
    "    url_y = \"http://192.168.122.15:30556/getTranslates\"\n",
    "    h_y = {\"apiVersion\": \"1.0.0\", \"text\": text, \"langPair\": {\"source\": \"en\", \"target\": \"ru\"}}\n",
    "    #h_y = {\"apiVersion\": \"1.0.0\", \"text\": \"Landing to the mars\", \"langPair\": \"en-ru\"}\n",
    "    r = requests.post(url = url_y, json=h_y)\n",
    "    data = r.json()\n",
    "    return data['translate'][0]['translate_value']\n",
    "\n",
    "ll_translate(\"mother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salut, connard.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def yandex_translate(text, from_lang, to_lang):\n",
    "    url_y = \"http://192.168.122.13:31436/GetTranslate\"\n",
    "    h_y = {\"apiVersion\": \"1.0.0\", \"text\": text, \"langPair\": {\"source\": from_lang, \"target\": to_lang}}\n",
    "    r = requests.post(url = url_y, json=h_y)\n",
    "    data = r.json()\n",
    "    if 'translate' in data:\n",
    "        return data['translate']\n",
    "    else:\n",
    "        return None\n",
    "yandex_translate(\"привет уеба\", \"ru\", \"fr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_y = {\"apiVersion\": \"1.0.0\", \"text\": wordsenses['definition'], \"langPair\": {\"source\": \"en\", \"target\": \"ru\"}}\n",
    "wordsenses['definition_local'] = requests.post(url = url_y, json=h_y).json()['translate'][0]['translate_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_lldb_instance(new_senseword_json, lldb_instances_dict, used_words_from_lldb, debug = False):\n",
    "    similarity_dict = {}\n",
    "    \n",
    "    for sense_word_lldb in lldb_instances_dict.keys():\n",
    "        \n",
    "        if sense_word_lldb not in used_words_from_lldb:\n",
    "            if debug == True: print(new_senseword_json['local_word'], sense_word_lldb)\n",
    "            similarity_dict[sense_word_lldb] = look_for_sublemma_word(new_senseword_json['local_word'],sense_word_lldb, apply_w2v = True)\n",
    "\n",
    "    for sorted_lldb_element in sorted(similarity_dict.items(), key=lambda kv: kv[1], reverse = True):\n",
    "        if sorted_lldb_element[1] < 0.6:\n",
    "            if debug == True:print(\"run out of similar ex\")\n",
    "            new_senseword_json['word_rating'] = 0\n",
    "            return None\n",
    "        elif sorted_lldb_element[1] >= 0.6 and sorted_lldb_element[0] not in used_words_from_lldb:\n",
    "            if debug == True:print(\"word_rating similarity from\",sorted_lldb_element[0] )\n",
    "            new_senseword_json['word_rating'] = lldb_instances_dict[sorted_lldb_element[0]]['word_rating']\n",
    "            return sorted_lldb_element[0]\n",
    "    if debug == True: print(\"no similarities\")\n",
    "    new_senseword_json['word_rating'] = 0\n",
    "    return None\n",
    "    \n",
    "        \n",
    "def one_word_processing(wordsence_example_dict,word,  debug = False):\n",
    "    #word_id + wordsences  for wordform\n",
    "    #word_id + lemma_branch for lemma\n",
    "    if 'lemma_branch' in wordsence_example_dict:\n",
    "        if debug: print( \"\\n\\nLEMMA_EXAMPLES_ARE_BEING_PROCESSED\", word)\n",
    "        wordsenses_list = wordsence_example_dict['lemma_branch']\n",
    "    elif 'wordsenses' in wordsence_example_dict:\n",
    "        if debug: print(\"\\n\\nWORDFORM_EXAMPLES_ARE_BEING_PROCESSED\", word)\n",
    "        wordsenses_list = wordsence_example_dict['wordsenses']\n",
    "    request = \"\"\"SELECT \n",
    "    content_words_lang.word_rating,\n",
    "    content_words_lang.jdesc->>'tr' AS translate,\n",
    "    content_words_lang.jdesc->>'ctx' AS context\n",
    "\n",
    "    FROM content_words_lang\n",
    "    where content_words_lang.jdesc->>'ctx' is not null and content_words_lang.ref_id = 1 and \n",
    "    content_words_lang.word_id = \"\"\"+ str(wordsence_example_dict['word_id']) +\"\"\" and content_words_lang.word_rating > 0\n",
    "    LIMIT 100 \"\"\"\n",
    "    cursor.execute(request)\n",
    "    lldb_examples_dict = {}\n",
    "    for wordform in cursor:\n",
    "        lldb_examples_dict[wordform[1]] = {\"word_rating\":wordform[0],\"eng_example\":wordform[2]}\n",
    "    if debug == True: print(\"lldb_examples_dict\", lldb_examples_dict)\n",
    "    \n",
    "    used_lldb_words = []\n",
    "    for wordsenses in wordsenses_list:\n",
    "        #translate definition and examples\n",
    "        #print(wordsenses)\n",
    "        \"\"\"\n",
    "        if 'definition' in wordsenses:\n",
    "            try:\n",
    "                wordsenses['definition_local'] = ll_translate(wordsenses['definition'])\n",
    "            except:\n",
    "                wordsenses['definition_local'] = yandex_translate(wordsenses['definition'], \"en\", \"ru\")\n",
    "                if not wordsenses['definition_local']:\n",
    "                    print(\"FAILED TO TRANSLATE\",wordsenses['definition'])\n",
    "        \n",
    "        translated_examples_list = []\n",
    "        for example in wordsenses['examples']:\n",
    "            try:\n",
    "                translated_example = ll_translate(example)\n",
    "            except:\n",
    "                translated_example = yandex_translate(example, \"en\", \"ru\")\n",
    "            translated_examples_list.append(translated_example)\n",
    "        wordsenses['examples_local'] = translated_examples_list\n",
    "        \"\"\"\n",
    "        if len(list(lldb_examples_dict.keys())) == len(used_lldb_words):\n",
    "            if debug == True:print(\"run of of lldb words\")\n",
    "            wordsenses['word_rating'] = 0\n",
    "        if debug == True: \n",
    "            print(wordsence_example_dict['word_id'], wordsenses)\n",
    "            print()\n",
    "        used_word = get_most_similar_lldb_instance(wordsenses, lldb_examples_dict, used_lldb_words)\n",
    "        if used_word:\n",
    "            used_lldb_words.append(used_word)\n",
    "            if debug == True:print(\"used lldf words\", used_lldb_words)\n",
    "            \n",
    "def get_similar_merge_pictures_rating_and_translate(wodrs_dict, debug = False):\n",
    "    for eng_lemma in wodrs_dict.keys():\n",
    "        if debug == True:print(eng_lemma)\n",
    "        one_word_processing(wodrs_dict[eng_lemma],eng_lemma, debug = False )\n",
    "        for wordform in wodrs_dict[eng_lemma]['wordforms_branch'].keys():\n",
    "            one_word_processing(wodrs_dict[eng_lemma]['wordforms_branch'][wordform],wordform )\n",
    "        \n",
    "#copy_big_diCKt = copy.deepcopy(big_diCKt)      \n",
    "#get_similar_merge_pictures_rating(copy_big_diCKt)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy_big_diCKt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-80e1a9161bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy_big_diCKt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'copy_big_diCKt' is not defined"
     ]
    }
   ],
   "source": [
    "copy_big_diCKt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_lemma_and_forms(word, dictionary, show_words = False):\n",
    "    req = \"\"\"SELECT DISTINCT\n",
    "    content_words.word_id,content_words.word_lemma, content_words.word_value\n",
    "    FROM content_words\n",
    "    WHERE content_words.word_value !~ ('\\W') AND\n",
    "    (array_length(regexp_split_to_array(content_words.word_value, '[ ''-]'), 1) = 1) \n",
    "    and word_lemma != 0 and word_hash = calc_hash(' \"\"\" + word + \"\"\"')\"\"\"\n",
    "    cursor.execute(req)\n",
    "\n",
    "    \n",
    "    req_res = cursor.fetchone()\n",
    "    if not req_res:\n",
    "        if show_words == True: print(word, \" is not in db\")\n",
    "        return 0\n",
    "    else:\n",
    "        #print(a[0])\n",
    "        word_id = req_res[0]\n",
    "        if show_words == True: print(req_res[2])\n",
    "            \n",
    "    #moment_1 = time.time()\n",
    "    get_def_info(word, word_id, \"english\",\"russian\",dictionary)\n",
    "    #moment_2 = time.time()\n",
    "    #print(\"get_def_info\",moment_2 - moment_1)\n",
    "    \n",
    "    lemma_sence_ex_dict = get_sencewords_and_examples_reverso(word,\"английский\",\"русский\", print_output = False )\n",
    "    #moment_3 = time.time()\n",
    "    #print(\"get_sencewords_and_examples_reverso\",moment_3 - moment_2)\n",
    "    \n",
    "    merge_lemma_context_vs_oxford_examples(word,lemma_sence_ex_dict, dictionary, print_output = False) \n",
    "    #moment_4 = time.time()\n",
    "    #print(\"merge_lemma_context_vs_oxford_examples\",moment_4 - moment_3)\n",
    "    \n",
    "    word_forms_request = \"\"\"SELECT DISTINCT \n",
    "    content_words.word_value,\n",
    "    content_words.word_id\n",
    "    FROM content_words\n",
    "    where word_lemma = \"\"\" + str(word_id) + \"\"\"  and word_lemma != word_id \"\"\"\n",
    "    cursor.execute(word_forms_request)\n",
    "    #moment_6 = time.time()\n",
    "    #print(\"word forms request to lldb\",moment_6 - moment_5)\n",
    "    for wordform in cursor:\n",
    "        if show_words == True: print(wordform[0])\n",
    "        wordform_lemma = nltk_lemmatize(wordform[0])\n",
    "        if wordform_lemma == word:\n",
    "            if show_words == True: print(\"will process\", wordform[0])\n",
    "            local_wordforms_sence_examples_dict = get_sencewords_and_examples_reverso(wordform[0],\"английский\", \"русский\")  \n",
    "            merge_locsence_ex_vs_lemma_structure(wordform[0],word, wordform[1], local_wordforms_sence_examples_dict,dictionary ,\n",
    "                                                                                 print_output= False)\n",
    "            #если после этого узел englsih_wordform оказывается пустым списком значит reverso не нашел нормальной словесной единицы для этого\n",
    "            #то есть слово скорее всгео херовое как показала практика типа haveing haved\n",
    "        else:\n",
    "            if show_words == True: print(\"will not process\", wordform[0], \"lemma is\",wordform_lemma )\n",
    "        #get_def_info(wordform[0], \"english\",\"russian\",dictionary,is_word_form = True, original_word = word )\n",
    "    get_similar_merge_pictures_rating_and_translate(dictionary)\n",
    "        \n",
    "big_diCKt = {}        \n",
    "process_lemma_and_forms(\"book\",big_diCKt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_branch >> arrange to use or do something at a particu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': {'word_id': 5756,\n",
       "  'lemma_branch': [{'definition': 'a set of pages fastened together in a cover for people to read',\n",
       "    'local_word': 'книга',\n",
       "    'examples': ['a book about animals',\n",
       "     'Recently has published as a co-author with Ra-Ma the book entitled Internet protocols.',\n",
       "     'The book aimed to sensitize children to the different aspects of poverty.',\n",
       "     'The book is available on the OHCHR web site.'],\n",
       "    'definition_local': 'набор страниц, скрепленных вместе в обложке для людей, чтобы читать',\n",
       "    'examples_local': ['книга о животных',\n",
       "     'Недавно опубликовал в соавторстве с РА-Ма книгу под названием интернет-протоколы.',\n",
       "     'Цель этой книги-привлечь внимание детей к различным аспектам нищеты.',\n",
       "     'С этой книгой можно ознакомиться на вебсайте УВКПЧ.'],\n",
       "    'word_rating': 163702},\n",
       "   {'definition': 'a set of stamps, tickets, etc that are fastened together inside a cover',\n",
       "    'local_word': 'книжечка',\n",
       "    'examples': ['Have you got an autograph book?',\n",
       "     'Where is your book?',\n",
       "     'You taking Little League book like you did last year?'],\n",
       "    'definition_local': 'набор марок, билетов и т. д., которые скрепляются между собой внутри крышки',\n",
       "    'examples_local': ['У тебя есть книга для автографов?',\n",
       "     'Где твоя книга?',\n",
       "     'Ты берешь книжку из Малой лиги, как в прошлом году?'],\n",
       "    'word_rating': 3},\n",
       "   {'definition': 'a set of pages fastened together in a cover and used for writing on',\n",
       "    'local_word': 'записная книжка',\n",
       "    'examples': ['an address book'],\n",
       "    'definition_local': 'набор страниц, скрепленных вместе в обложке и используемых для записи на',\n",
       "    'examples_local': ['адресная книга'],\n",
       "    'word_rating': 0},\n",
       "   {'definition': 'to arrange to use or do something at a particular time in the future',\n",
       "    'local_word': 'бронировать',\n",
       "    'examples': ['to book a ticket/hotel room',\n",
       "     \"We've booked a trip to Spain for next month.\",\n",
       "     'Sorry, the hotel is fully booked (= has no more rooms).',\n",
       "     \"I'm sorry, it won't let me book it without the correct ranking.\"],\n",
       "    'definition_local': 'чтобы организовать использование или сделать что-то в определенное время в будущем',\n",
       "    'examples_local': ['забронировать авиабилет / номер в отеле',\n",
       "     'Мы забронировали поездку в Испанию на следующий месяц.',\n",
       "     'Извините, отель полностью забронирован (=нет больше номеров).',\n",
       "     'Извините, это не позволит мне забронировать его без правильного рейтинга.'],\n",
       "    'word_rating': 53027},\n",
       "   {'definition': 'to officially accuse someone of a crime',\n",
       "    'local_word': 'заводить дело',\n",
       "    'examples': ['Detectives booked him for resisting arrest.'],\n",
       "    'definition_local': 'официально обвинить кого-то в преступлении',\n",
       "    'examples_local': ['Детективы арестовали его за сопротивление при аресте.'],\n",
       "    'word_rating': 0},\n",
       "   {'definition': 'If a sports official books you, they write an official record of something you have done wrong.',\n",
       "    'local_word': 'штрафовать (в спорте)',\n",
       "    'examples': ['The referee booked two players for fighting during the game.'],\n",
       "    'definition_local': 'Если спортивный чиновник записывает вас, они пишут официальную запись о том, что вы сделали неправильно.',\n",
       "    'examples_local': ['Судья назначил двух игроков за драку во время игры.'],\n",
       "    'word_rating': 0}],\n",
       "  'wordforms_branch': {'books': {'word_id': 53613,\n",
       "    'wordsenses': [{'local_word': 'книги',\n",
       "      'examples': [\"It's not easy to get books in English around here.\",\n",
       "       'She sent the companion into Reading to exchange some library books, and took some sleeping pills.',\n",
       "       'So there must be a reason why they used two books.'],\n",
       "      'examples_local': ['Здесь нелегко достать книги на английском языке.',\n",
       "       'Она послала своего спутника в Ридинг, чтобы тот обменялся библиотечными книгами, и приняла снотворное.',\n",
       "       'Так что должна быть причина, почему они использовали две книги.'],\n",
       "      'word_rating': 7123},\n",
       "     {'local_word': 'учебники',\n",
       "      'examples': [\"Educational history books have been rewritten to legitimize Russia's actions.\",\n",
       "       'Some of the books are unsuitable and in need of comprehensive overhaul and updating.',\n",
       "       'In primary education, the adoption of school books, interactive games and online tools has proved to be particularly useful.'],\n",
       "      'examples_local': ['Учебники истории были переписаны, чтобы узаконить действия России.',\n",
       "       'Некоторые книги непригодны и нуждаются в всестороннем пересмотре и обновлении.',\n",
       "       'В области начального образования особенно полезным оказалось внедрение школьных учебников, интерактивных игр и онлайновых инструментов.'],\n",
       "      'word_rating': 1370},\n",
       "     {'local_word': 'книжек',\n",
       "      'examples': ['I read one of your books, by the way.',\n",
       "       \"Apparently he read a couple books in the '80s.\",\n",
       "       'I was so nervous with my mom, think I put like ten books in there.'],\n",
       "      'examples_local': ['Кстати, я читал одну из ваших книг.',\n",
       "       'Видимо, он прочитал пару книг в 80-х.',\n",
       "       'Я так нервничала с мамой, что, кажется, положила туда десять книг.'],\n",
       "      'word_rating': 0},\n",
       "     {'local_word': 'книжках',\n",
       "      'examples': [\"That's what you get in children's books.\",\n",
       "       'Do you want to be in the good books or the bad books?',\n",
       "       'Well, she sleeps with her both hands tucked under her cheek like in those kid books.'],\n",
       "      'examples_local': ['Это то, что вы получаете в детских книгах.',\n",
       "       'Вы хотите быть в хороших книгах или в плохих книгах?',\n",
       "       'Ну, она спит, подложив обе руки под щеку, как в тех детских книжках.'],\n",
       "      'word_rating': 0},\n",
       "     {'local_word': 'бухгалтерию',\n",
       "      'examples': ['And that would open up our books in an alarming way.',\n",
       "       'Now, no businessman wants to open up his books, especially one with $43 billion.',\n",
       "       'Took down the whole setup, books, cash, everything.'],\n",
       "      'examples_local': ['И это открыло бы наши книги тревожным образом.',\n",
       "       'Теперь ни один бизнесмен не хочет открывать свои книги, особенно с 43 миллиардами долларов.',\n",
       "       'Снял всю установку, книги, наличные, все.'],\n",
       "      'word_rating': 0}]},\n",
       "   'booking': {'word_id': 46395,\n",
       "    'wordsenses': [{'local_word': 'бронирование',\n",
       "      'examples': ['Because you can make the booking any time you want.',\n",
       "       'Consultations and booking of tickets for all flights of the Airline.',\n",
       "       'If the customer cancels the booking, the invoice will be cancelled automatically.'],\n",
       "      'examples_local': ['Потому что вы можете сделать заказ в любое время вы хотите.',\n",
       "       'Консультации и бронирование билетов на все рейсы авиакомпании.',\n",
       "       'Если клиент отменит бронирование, счет будет аннулирован автоматически.'],\n",
       "      'word_rating': 10714},\n",
       "     {'local_word': 'резервирование',\n",
       "      'examples': ['The organizers are responsible for providing premises for the workshop and booking hotels for participants.',\n",
       "       'In this case, Pilgram Apartments performs the confirmation of the booking itself.'],\n",
       "      'examples_local': ['Организаторы несут ответственность за предоставление помещений для проведения семинара и бронирование гостиниц для участников.',\n",
       "       'В этом случае Pilgram Apartments выполняет подтверждение бронирования самостоятельно.'],\n",
       "      'word_rating': 3}]},\n",
       "   'booked': {'word_id': 5769,\n",
       "    'wordsenses': [{'local_word': 'заказал',\n",
       "      'examples': [\"Calvin's booked the best DJ on campus for the after-party.\",\n",
       "       'So I booked an O.R. for the next five hours.',\n",
       "       'I booked the O.R. I read up on the procedure.'],\n",
       "      'examples_local': ['Кэлвин заказал лучшего ди-джея в кампусе для вечеринки.',\n",
       "       'Поэтому я заказал операционную на следующие пять часов.',\n",
       "       'Я заказал операционную, я прочитал о процедуре.'],\n",
       "      'word_rating': 187},\n",
       "     {'local_word': 'забронировал',\n",
       "      'examples': ['The Spanish lawyer booked a flight to Istanbul but never made it.',\n",
       "       'I booked a nice cozy room for you at the Don Jail.',\n",
       "       'I booked a trip to Europe in the fall.'],\n",
       "      'examples_local': ['Испанский адвокат забронировал билет на самолет до Стамбула, но так и не прилетел.',\n",
       "       'Я забронировал для тебя хороший уютный номер в донской тюрьме.',\n",
       "       'Я забронировал поездку в Европу осенью.'],\n",
       "      'word_rating': 3583},\n",
       "     {'local_word': 'забронировала',\n",
       "      'examples': ['I mean, I just booked my friends a gig.',\n",
       "       'I booked you a room in the hotel.',\n",
       "       'I booked a room for us in our favourite hotel at Lake Chiemsee.'],\n",
       "      'examples_local': ['Я только что заказал своим друзьям концерт.',\n",
       "       'Я забронировал тебе номер в отеле.',\n",
       "       'Я забронировал номер для нас в нашем любимом отеле на озере Кимзее.'],\n",
       "      'word_rating': 0},\n",
       "     {'local_word': 'забронировали',\n",
       "      'examples': ['We booked you a family room at the hotel on the roundabout.',\n",
       "       \"No, I don't think so, we only booked two.\",\n",
       "       'When we were together, we booked a cruise.'],\n",
       "      'examples_local': ['Мы забронировали вам семейный номер в отеле на кольцевой развязке.',\n",
       "       'Нет, я так не думаю, мы заказали только два.',\n",
       "       'Когда мы были вместе, мы заказали круиз.'],\n",
       "      'word_rating': 0},\n",
       "     {'local_word': 'заказали',\n",
       "      'examples': [\"We booked a table at Papa Del's.\",\n",
       "       'We booked it, we sent out invitations...'],\n",
       "      'examples_local': ['Мы заказали столик у папы Дель.',\n",
       "       'Мы забронировали его, разослали приглашения...'],\n",
       "      'word_rating': 3},\n",
       "     {'local_word': 'занят',\n",
       "      'examples': [\"Well, I'm pretty much booked, tripp.\"],\n",
       "      'examples_local': ['Ну, у меня почти все занято, Трипп.'],\n",
       "      'word_rating': 4115},\n",
       "     {'local_word': 'забронированы',\n",
       "      'examples': ['I have three suites and two rooms booked in my name.',\n",
       "       'The conference venue had been chosen, the meeting rooms booked and a transport system organized.',\n",
       "       'The rooms can be booked with half-board (at an additional cost).'],\n",
       "      'examples_local': ['У меня есть три люкса и два номера, забронированные на мое имя.',\n",
       "       'Было выбрано место проведения конференции, забронированы залы заседаний и организована транспортная система.',\n",
       "       'Номера можно забронировать с полупансионом (за дополнительную плату).'],\n",
       "      'word_rating': 0},\n",
       "     {'local_word': 'заказан',\n",
       "      'examples': [\"You'd be booked all season.\",\n",
       "       \"I've booked passage on an Andorian transport.\",\n",
       "       'This conversation was booked in English.'],\n",
       "      'examples_local': ['Вы будете заняты весь сезон.',\n",
       "       'Я заказал билет на Андорианский транспорт.',\n",
       "       'Этот разговор был заказан на английском языке.'],\n",
       "      'word_rating': 7}]}}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_diCKt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"one_word_transalte.json\", \"w\") as f:\n",
    "    json.dump(big_diCKt, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_diCKt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words_db = pd.read_csv(\"test_words.csv\")\n",
    "test_words = list(test_words_db['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  0%|          | 0/200 [00:00<?, ?it/s]\n",
    "\n",
    "\n",
    "  0%|          | 1/200 [00:08<28:14,  8.52s/it]\n",
    "\n",
    "\n",
    "  1%|          | 2/200 [00:22<33:04, 10.02s/it]\n",
    "\n",
    "\n",
    "  2%|▏         | 3/200 [01:02<1:02:31, 19.04s/it]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/123 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 1/123 [01:18<2:39:29, 78.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 2/123 [02:02<2:17:32, 68.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 3/123 [02:18<1:45:07, 52.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 4/123 [02:38<1:24:48, 42.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 5/123 [02:59<1:11:12, 36.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 6/123 [03:15<58:48, 30.16s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 7/123 [03:39<54:40, 28.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 8/123 [04:08<54:39, 28.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 9/123 [04:28<49:18, 25.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 10/123 [04:50<46:43, 24.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 11/123 [05:13<45:20, 24.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 12/123 [05:41<47:01, 25.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█         | 13/123 [06:07<46:48, 25.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|█▏        | 14/123 [06:42<51:22, 28.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 15/123 [07:02<46:24, 25.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|█▎        | 16/123 [07:26<45:15, 25.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|█▍        | 17/123 [07:56<46:57, 26.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 18/123 [08:48<59:52, 34.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 19/123 [09:24<1:00:17, 34.78s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▋        | 20/123 [09:52<56:05, 32.68s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 21/123 [10:21<53:50, 31.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 22/123 [10:22<37:52, 22.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▊        | 23/123 [10:51<40:42, 24.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|█▉        | 24/123 [11:15<40:08, 24.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 25/123 [11:41<40:44, 24.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 26/123 [12:04<39:22, 24.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██▏       | 27/123 [12:54<51:17, 32.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 28/123 [13:32<53:16, 33.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▎       | 29/123 [14:05<52:38, 33.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 30/123 [15:32<1:16:37, 49.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 31/123 [16:13<1:12:14, 47.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▌       | 32/123 [16:59<1:11:02, 46.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 33/123 [18:15<1:23:18, 55.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 34/123 [19:03<1:18:52, 53.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 28%|██▊       | 35/123 [19:49<1:14:58, 51.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 36/123 [20:44<1:15:33, 52.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 30%|███       | 37/123 [21:41<1:16:56, 53.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 38/123 [22:30<1:13:58, 52.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 39/123 [23:15<1:09:59, 50.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 40/123 [24:13<1:12:48, 52.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 41/123 [25:21<1:18:13, 57.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 34%|███▍      | 42/123 [26:12<1:14:42, 55.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 43/123 [27:04<1:12:17, 54.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|███▌      | 44/123 [28:05<1:14:08, 56.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 45/123 [28:52<1:09:25, 53.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 46/123 [29:44<1:07:59, 52.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 47/123 [30:35<1:06:16, 52.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███▉      | 48/123 [31:27<1:05:23, 52.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 40%|███▉      | 49/123 [32:18<1:03:54, 51.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 50/123 [33:12<1:03:51, 52.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████▏     | 51/123 [34:10<1:05:16, 54.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 52/123 [35:18<1:09:01, 58.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 43%|████▎     | 53/123 [36:14<1:07:16, 57.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 54/123 [37:08<1:05:07, 56.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 55/123 [38:12<1:06:34, 58.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 56/123 [39:18<1:07:56, 60.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▋     | 57/123 [40:40<1:13:52, 67.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 58/123 [41:45<1:12:12, 66.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|████▊     | 59/123 [43:13<1:17:55, 73.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▉     | 60/123 [44:40<1:21:01, 77.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|████▉     | 61/123 [45:59<1:20:23, 77.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 62/123 [47:24<1:21:23, 80.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|█████     | 63/123 [48:35<1:17:14, 77.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 64/123 [50:06<1:20:00, 81.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 53%|█████▎    | 65/123 [51:49<1:24:49, 87.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▎    | 66/123 [53:07<1:20:44, 84.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 67/123 [54:33<1:19:24, 85.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 68/123 [55:51<1:16:08, 83.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████▌    | 69/123 [57:06<1:12:29, 80.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|█████▋    | 70/123 [58:19<1:09:21, 78.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a140fb285957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprocess_lemma_and_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"big_dict_with_almost_all_features_v4.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-a228f082e067>\u001b[0m in \u001b[0;36mprocess_lemma_and_forms\u001b[0;34m(word, dictionary, show_words)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow_words\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"will not process\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lemma is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordform_lemma\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#get_def_info(wordform[0], \"english\",\"russian\",dictionary,is_word_form = True, original_word = word )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mget_similar_merge_pictures_rating_and_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mbig_diCKt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-70265652c6b5>\u001b[0m in \u001b[0;36mget_similar_merge_pictures_rating_and_translate\u001b[0;34m(wodrs_dict, debug)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meng_lemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwodrs_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_lemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mone_word_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwodrs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meng_lemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meng_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwordform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwodrs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meng_lemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wordforms_branch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mone_word_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwodrs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meng_lemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wordforms_branch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwordform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordform\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-70265652c6b5>\u001b[0m in \u001b[0;36mone_word_processing\u001b[0;34m(wordsence_example_dict, word, debug)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mcontent_words_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"+ str(wordsence_example_dict['word_id']) +\"\"\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontent_words_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_rating\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     LIMIT 100 \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mlldb_examples_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwordform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/encodings/utf_8.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_dict = {}\n",
    "count = 0\n",
    "for word in tqdm(test_words[77:]):\n",
    "    count += 1\n",
    "    process_lemma_and_forms(word.lower(), global_dict)\n",
    "    if count % 5 == 0:\n",
    "        with open(\"big_dict_with_almost_all_features_v4.json\", \"w\") as f:\n",
    "            json.dump(global_dict, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
