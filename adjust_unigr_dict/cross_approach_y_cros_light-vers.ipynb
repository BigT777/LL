{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from gensim.models.wrappers import FastText\n",
    "model_fr = FastText.load_fasttext_format('/Users/nigula/input/fr/fr.bin')\n",
    "\n",
    "%%time\n",
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "model_ru= FastTextKeyedVectors.load('/Users/nigula/input/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request_func(from_lang,to_lang, word, header_main, login, en_from_lang, en_to_lang):\n",
    "    start = time.time()\n",
    "    url = \"https://context.reverso.net/перевод/\" + from_lang + \"-\" + to_lang + \"/\" + word \n",
    "    print(\"going to find word in\", url)\n",
    "    response = requests.get(url, headers=header_main, data = login)\n",
    "    response.encoding = 'utf-8' \n",
    "    #\n",
    "    save_dir = os.path.join(\"/Users/nigula/LL/adjust_unigr_dict/lookup/reverso\" + \"_\" + en_from_lang + \"_\" + en_to_lang,word + \".xls\")\n",
    "    file = open(save_dir, \"wb\")\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "    print(\"saved to directory\", save_dir)\n",
    "    soup = bs(response.text, 'html.parser') \n",
    "    time.sleep(0.01)\n",
    "    print(\"request_time\",time.time() - start) \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_word_in /Users/nigula/LL/adjust_unigr_dict/lookup/reverso_fr_ru/Extra.xls\n",
      "definiotns_time 0.11478376388549805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sence_word': 'высшего сорта', 'pos': ''},\n",
       " {'sence_word': 'экстра', 'pos': 'adv'},\n",
       " {'sence_word': 'сверху', 'pos': 'adv'},\n",
       " {'sence_word': 'бонус', 'pos': 'n'},\n",
       " {'sence_word': 'крутой', 'pos': 'adj'},\n",
       " {'sence_word': 'пространства', 'pos': 'adj'},\n",
       " {'sence_word': 'первого сортов', 'pos': 'adj'},\n",
       " {'sence_word': 'классно', 'pos': 'adj'},\n",
       " {'sence_word': 'дополнительно', 'pos': 'adj'},\n",
       " {'sence_word': 'замечательная', 'pos': 'adj'},\n",
       " {'sence_word': 'дополнительный', 'pos': 'adj'},\n",
       " {'sence_word': 'отличный парень', 'pos': 'adj'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_definitions_reverso(word, from_lang, to_lang, print_output = False):\n",
    "    start = time.time()\n",
    "    login = {'inUserName': 'n.babakov@lingualeo.com', 'inUserPass': '33vec33'}\n",
    "    header_main = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    senseword_list = []\n",
    "    land_dict = {\"русский\":\"ru\",\"французский\":\"fr\", \"английский\":\"en\"}\n",
    "    try:\n",
    "        word_dir = os.path.join(\"/Users/nigula/LL/adjust_unigr_dict/lookup/reverso\" + \"_\" + land_dict[from_lang] + \"_\" + land_dict[to_lang],word + \".xls\")\n",
    "        infile = open(word_dir,\"r\")\n",
    "        print(\"found_word_in\", word_dir)\n",
    "        contents = infile.read()\n",
    "        soup = bs(contents,'html.parser')\n",
    "    except:\n",
    "        soup = send_request_func(from_lang,to_lang, word, header_main, login, land_dict[from_lang], land_dict[to_lang])\n",
    "    #print(soup.prettify())\n",
    "    first_string_passed = False\n",
    "    pos = ''\n",
    "    for link in soup.find_all(\"a\", attrs={\"class\" : \"translation\"}):\n",
    "        #print(link.prettify())\n",
    "        try:\n",
    "            #print(\"||\",link['data-pos'])\n",
    "            pos = link['data-pos'][1:-1]\n",
    "        except:\n",
    "            pass\n",
    "        if first_string_passed == True:\n",
    "            sence_word = link.text.strip()\n",
    "            if len(sence_word) >0:\n",
    "                senseword_list.append({\"sence_word\":sence_word.lower(), \"pos\":pos})\n",
    "            #print(sence_word)\n",
    "        first_string_passed = True\n",
    "        \n",
    "    for link in soup.find_all(\"div\", attrs={\"class\" : \"translation\"}):\n",
    "        #print(link.prettify())\n",
    "        try:\n",
    "            #print(\"||\",link['data-pos'])\n",
    "            pos = link['data-pos'][1:-1]\n",
    "        except:\n",
    "            pass\n",
    "        if first_string_passed == True:\n",
    "            sence_word = link.text.strip()\n",
    "            if len(sence_word) >0:\n",
    "                senseword_list.append({\"sence_word\":sence_word.lower(), \"pos\":pos})\n",
    "            #print(sence_word)\n",
    "        first_string_passed = True\n",
    "    if len (senseword_list) == 0:\n",
    "        print(\"turn to alternative marks\")\n",
    "        for link in soup.find_all(\"div\", attrs={\"lang\" : \"fr\"}):\n",
    "            try:\n",
    "                #print(\"||\",link['data-pos'])\n",
    "                pos = link['data-pos'][1:-1]\n",
    "            except:\n",
    "                pass\n",
    "            sence_word = link.text.strip()\n",
    "            senseword_list.append({\"sence_word\":sence_word.lower(), \"pos\":pos})\n",
    "            #print(link.prettify())\n",
    "    print(\"definiotns_time\", time.time() - start)\n",
    "    return senseword_list\n",
    "\n",
    "\n",
    "get_definitions_reverso(\"Extra\", \"французский\", \"русский\", print_output = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEMMATIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "def fr_lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    return lemma\n",
    "def preprocess_words(words_list, lemmatizer_func):\n",
    "    start = time.time()\n",
    "    preprocessed_words = []\n",
    "    for word in words_list:\n",
    "        lemma = lemmatizer_func(word['sence_word'].lower())\n",
    "        word['sence_word'] = lemma\n",
    "    print(\"lemmat_time\", time.time() - start)\n",
    "        #print(\"lemma\",lemma)\n",
    "        #preprocessed_words.append(lemma)\n",
    "    #return preprocessed_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_nonstopwords_vectors_sum(lemm_list):\n",
    "    orig_vector = np.zeros(shape = 100)\n",
    "    for lemma in lemm_list:\n",
    "        if lemma not in stopWords:\n",
    "            #print (lemma)\n",
    "            try:\n",
    "                orig_vector += model[lemma]\n",
    "            except:\n",
    "                pass\n",
    "    return orig_vector\n",
    "\n",
    "\n",
    "def en_lemmatize(line):\n",
    "    pos_tagged_ngramm = pos_tag(line.split())\n",
    "    lemmatized_line_list = []\n",
    "    for word_el in pos_tagged_ngramm:\n",
    "        pos = get_wordnet_pos(word_el[1])\n",
    "        if pos:\n",
    "            lemma = lemmatizer.lemmatize(word_el[0], pos =pos)\n",
    "        else:\n",
    "            lemma = word_el[0]\n",
    "        lemmatized_line_list.append(lemma)\n",
    "    return ' '.join(lemmatized_line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый мама красиво мыть рама\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "text = \"Красивая мама красиво мыла раму\"\n",
    "mystem = Mystem()\n",
    "lemmas = mystem.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'красивый мама красиво мыло рам'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def ru_lemmatize(line):\n",
    "    line_lemm = ''\n",
    "    for word in line.split():\n",
    "        lemma = morph.parse(word)[0]\n",
    "        lemma = lemma.normal_form\n",
    "        line_lemm += lemma + ' '\n",
    "    line_lemm = line_lemm.strip()\n",
    "    return line_lemm\n",
    "\n",
    "ru_lemmatize(\"Красивая мама красиво мыла раму\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_fr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f8f945e944a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfind_lines_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"французский\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#find_lines_similarity(l1, l2, \"русский\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f8f945e944a9>\u001b[0m in \u001b[0;36mfind_lines_similarity\u001b[0;34m(line_1, line_2, lang)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_lines_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0ml1_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0ml2_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f8f945e944a9>\u001b[0m in \u001b[0;36mget_vectors_sum\u001b[0;34m(lemm_line, lang)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_vectors_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemm_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"французский\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"русский\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMOD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fr' is not defined"
     ]
    }
   ],
   "source": [
    "l1 = \"maman\"\n",
    "l2 = 'maman famille'\n",
    "\n",
    "def get_vectors_sum(lemm_line, lang):\n",
    "    if lang == \"французский\": model = model_fr\n",
    "    elif lang == \"русский\": model = MOD\n",
    "    else:\n",
    "        print(\"NO_MODEL_FOUND\")\n",
    "        return np.zeros(shape = 300)\n",
    "    orig_vector = np.zeros(shape = 300)\n",
    "    for lemma in lemm_line.split():\n",
    "        #print(lemma)\n",
    "        try:\n",
    "            orig_vector += model[lemma]\n",
    "        except:\n",
    "            pass\n",
    "    return orig_vector\n",
    "\n",
    "def find_lines_similarity(line_1, line_2, lang):\n",
    "    l1_vect = get_vectors_sum(line_1, lang)\n",
    "    l2_vect = get_vectors_sum(line_2, lang)\n",
    "    cos_sim = cosine_similarity(l1_vect.reshape(1, -1), l2_vect.reshape(1, -1))[0][0]\n",
    "    #print(cos_sim)\n",
    "    return cos_sim\n",
    "\n",
    "find_lines_similarity(l1, l2, \"французский\")\n",
    "#find_lines_similarity(l1, l2, \"русский\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pos(pos1, pos2):\n",
    "    if pos1 == pos2:\n",
    "        #print(\"abs eq\", pos1, pos2)\n",
    "        return True\n",
    "    elif pos1 in pos2 or pos2 in pos1:\n",
    "        #print(\"inclusive eq\", pos1, pos2)\n",
    "        return True\n",
    "    #print(\"no eq\", pos1, pos2)\n",
    "    return False\n",
    "def equate_empty_pos(pos1, pos2):\n",
    "    if pos1 == '': \n",
    "        pos1 = pos2\n",
    "        #print(\"pos1 eqauted\")\n",
    "    elif pos2 == '': \n",
    "        pos2 = pos1\n",
    "        #print(\"pos2 eqauted\")\n",
    "    return pos1, pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_lines_inclusive_similarity(line_1,line_2):\n",
    "    if line_1 in line_2: return True\n",
    "    elif line_2 in line_1: return True\n",
    "    else: return False\n",
    "    \n",
    "    \n",
    "find_lines_inclusive_similarity(\"chauffer\",\"réchauffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle indirect similarity\n",
    "handled_words = []\n",
    "for word_1 in list_1:\n",
    "    for word_2 in list_2:\n",
    "        word_1['pos'], word_2['pos'] = equate_empty_pos(word_1['pos'], word_2['pos'])\n",
    "        pos_eq = compare_pos(word_1['pos'],word_2['pos'])\n",
    "        if pos_eq == True and word_1['sence_word'] != word_2['sence_word'] and word_1['sence_word'] not in handled_words and word_2['sence_word'] not in handled_words:\n",
    "            sim = find_lines_similarity(word_1['sence_word'], word_2['sence_word'], lists_lang)\n",
    "            if sim > 0.9:\n",
    "                print('COS_SIM_WORKED', word_1['sence_word'], word_2['sence_word'])\n",
    "                if word_1['sence_word'] in cos_sim_dict:\n",
    "                    handled_words.append(word_2['sence_word'])\n",
    "                    cos_sim_dict[word_1['sence_word']].append(word_2['sence_word'])\n",
    "                   # overall_intersection[word_1_index] += '| ' + word_2\n",
    "\n",
    "                elif word_2['sence_word'] in cos_sim_dict:\n",
    "                     handled_words.append(word_1['sence_word'])\n",
    "                     cos_sim_dict[word_2['sence_word']].append(word_1['sence_word'])\n",
    "                else:\n",
    "                    cos_sim_dict[word_1['sence_word']] = [word_2['sence_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9087066650390625e-05 passed get_two_target_lang_lists_intersection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['réchauffer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_two_target_lang_lists_intersection(list_1, list_2, lists_lang):\n",
    "    start = time.time()\n",
    "    overall_intersection = []\n",
    "    cos_sim_dict ={}\n",
    "    #overall_intersection\n",
    "    for word_1 in list_1:\n",
    "        for word_2 in list_2:\n",
    "            #print(word_1, word_2)\n",
    "            word_1['pos'], word_2['pos'] = equate_empty_pos(word_1['pos'], word_2['pos'])\n",
    "            pos_eq = compare_pos(word_1['pos'],word_2['pos'])  \n",
    "            if pos_eq == True and word_1['sence_word'] == word_2['sence_word']:\n",
    "                #overall_intersection.append(word_1)\n",
    "                cos_sim_dict[word_1['sence_word']] = []\n",
    "                    \n",
    "    #GETET INTERSECTION LIST\n",
    "    for first_word in cos_sim_dict.keys():\n",
    "        intersect_element = first_word\n",
    "        for similar_word in cos_sim_dict[first_word]:\n",
    "            intersect_element += \" | \" + similar_word\n",
    "        overall_intersection.append(intersect_element)\n",
    "    print(time.time() - start, \"passed get_two_target_lang_lists_intersection\")\n",
    "    return overall_intersection\n",
    "\n",
    "fr_proc_1 = [{'sence_word': 'réchauffer', 'pos': 'v'}, {'sence_word': 'chaud', 'pos': 'v'}]\n",
    "\n",
    "fr_proc_2 = [{'sence_word': 'chaud', 'pos': 'adj'},\n",
    " {'sence_word': 'chaleureux', 'pos': 'adj'},\n",
    " {'sence_word': 'tiède', 'pos': 'adj'},\n",
    " {'sence_word': 'doux', 'pos': 'adj'},\n",
    " {'sence_word': 'bon', 'pos': 'adj/nm'},\n",
    " {'sence_word': 'cordial', 'pos': 'adj'},\n",
    " {'sence_word': 'réchauffer', 'pos': 'v'},\n",
    " {'sence_word': 'chauffer', 'pos': 'v'},\n",
    " {'sence_word': 'échauffer', 'pos': 'v'},\n",
    " {'sence_word': 'se réchauffer', 'pos': 'v'},\n",
    " {'sence_word': 'tiédir', 'pos': 'v'},\n",
    " {'sence_word': 'chaleur', 'pos': 'nf'},\n",
    " {'sence_word': 'accueil', 'pos': 'nm'},\n",
    " {'sence_word': 'chaleureusement', 'pos': 'adv'},\n",
    " {'sence_word': 'warm', 'pos': 'adv'},\n",
    " {'sence_word': 'accueillante', 'pos': 'adv'},\n",
    " {'sence_word': 'conviviale', 'pos': 'adv'}]\n",
    "get_two_target_lang_lists_intersection(fr_proc_1, fr_proc_2, \"fr\")      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_word_in /Users/nigula/LL/adjust_unigr_dict/lookup/reverso_en_fr/warm.xls\n",
      "definiotns_time 0.11221694946289062\n",
      "lemmat_time 0.0005366802215576172\n",
      "found_word_in /Users/nigula/LL/adjust_unigr_dict/lookup/reverso_ru_fr/согревать.xls\n",
      "definiotns_time 0.2977890968322754\n",
      "lemmat_time 6.198883056640625e-05\n",
      "lang_1_to_target_words_dict [{'sence_word': 'chaud', 'pos': 'adj'}, {'sence_word': 'chaleureux', 'pos': 'adj'}, {'sence_word': 'tiède', 'pos': 'adj'}, {'sence_word': 'doux', 'pos': 'adj'}, {'sence_word': 'bon', 'pos': 'adj/nm'}, {'sence_word': 'cordial', 'pos': 'adj'}, {'sence_word': 'réchauffer', 'pos': 'v'}, {'sence_word': 'chauffer', 'pos': 'v'}, {'sence_word': 'échauffer', 'pos': 'v'}, {'sence_word': 'se réchauffer', 'pos': 'v'}, {'sence_word': 'tiédir', 'pos': 'v'}, {'sence_word': 'chaleur', 'pos': 'nf'}, {'sence_word': 'accueil', 'pos': 'nm'}, {'sence_word': 'chaleureusement', 'pos': 'adv'}, {'sence_word': 'warm', 'pos': 'adv'}, {'sence_word': 'accueillante', 'pos': 'adv'}, {'sence_word': 'conviviale', 'pos': 'adv'}, {'sence_word': 'bien au chaud', 'pos': 'adj'}, {'sence_word': 'enflammée', 'pos': 'adj'}] \n",
      "\n",
      "lang_2_to_target_words_dict [{'sence_word': 'réchauffer', 'pos': 'v'}, {'sence_word': 'chaud', 'pos': 'v'}]\n",
      "4.1961669921875e-05 passed get_two_target_lang_lists_intersection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['réchauffer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cross_translate_reverso(word_lang_1, lang_1, word_lang_2, lang_2, target_lang, lemmatizer_func):\n",
    "    lang_1_to_target_words_dict = get_definitions_reverso(word_lang_1, lang_1, target_lang, print_output=True)\n",
    "    preprocess_words(lang_1_to_target_words_dict, lemmatizer_func)\n",
    "    \n",
    "    lang_2_to_target_words_dict = get_definitions_reverso(word_lang_2, lang_2, target_lang, print_output=True)\n",
    "    preprocess_words(lang_2_to_target_words_dict, lemmatizer_func)\n",
    "    \n",
    "    print(\"lang_1_to_target_words_dict\", lang_1_to_target_words_dict,\"\\n\")\n",
    "    print(\"lang_2_to_target_words_dict\", lang_2_to_target_words_dict)\n",
    "    \n",
    "    intersected_elements = get_two_target_lang_lists_intersection(lang_1_to_target_words_dict, lang_2_to_target_words_dict,\n",
    "                                                                 target_lang)\n",
    "    \n",
    "    if len(intersected_elements) == 0:intersected_elements.append(\"no_equality\")\n",
    "    return intersected_elements\n",
    "\n",
    "get_cross_translate_reverso(\"warm\", \"английский\", \"согревать\",\"русский\", \"французский\", fr_lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilang_from_df(df, lookup_from, lookup_to, target_lang, target_lang_lemmatizer_func):\n",
    "    land_dict = {\"русский\":\"ru\",\"французский\":\"fr\", \"английский\":\"en\"}\n",
    "    eng_word_list = []\n",
    "    lang_1_word_list = []\n",
    "    lang_2_word_list = []\n",
    "    eng_word = 'first_word'\n",
    "    save_triple_path = land_dict[lookup_from] + '_' + land_dict[lookup_to] + '_' + land_dict[target_lang]\n",
    "    \n",
    "    start = False\n",
    "    \n",
    "    #for df_ind in tqdm(range(len(df))):\n",
    "    count = 0 \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if eng_word != row[\"word\"]: print(\"\\n************************************\\n\")\n",
    "        eng_word = row[\"word\"]\n",
    "        #if eng_word == \"trajet\": start = True\n",
    "        #if start == True:\n",
    "        loc_1_word = row[\"local_word\"]\n",
    "        print(eng_word, loc_1_word)\n",
    "        sense_intersect = get_cross_translate_reverso(eng_word, lookup_from, loc_1_word, lookup_to, target_lang, target_lang_lemmatizer_func)\n",
    "        #sense_intersect = get_cross_translate_yand(eng_word, \"en\", loc_1_word,\"ru\", \"fr\", fr_lemmatize)\n",
    "        print(\"\\n SENSE_INTERSECTION \", sense_intersect)\n",
    "        for sense_intersect_element in sense_intersect:\n",
    "            eng_word_list.append(eng_word)\n",
    "            lang_1_word_list.append(loc_1_word)\n",
    "            lang_2_word_list.append(sense_intersect_element)\n",
    "\n",
    "\n",
    "        if count % 10 == 0 and count != 0:\n",
    "\n",
    "            save_path = os.path.join(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/cross_barbos\",\n",
    "                         save_triple_path, str(count) + \".csv\")\n",
    "            print(save_path)\n",
    "            data = pd.DataFrame(list(zip(eng_word_list, lang_1_word_list,lang_2_word_list)),\n",
    "                        columns =[land_dict[lookup_from], land_dict[lookup_to], land_dict[target_lang]])\n",
    "            data.to_csv(save_path)\n",
    "        count += 1\n",
    "            \n",
    "    data = pd.DataFrame(list(zip(eng_word_list, lang_1_word_list,lang_2_word_list)),\n",
    "                        columns =[land_dict[lookup_from], land_dict[lookup_to], land_dict[target_lang]])\n",
    "    \n",
    "    return data\n",
    "\n",
    "#df_din = get_multilang_from_df(df_en_ru_yandex, \"английский\", \"русский\", \"французский\", fr_lemmatize)\n",
    "#df_din.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en-ru >> fr ready_res\n",
    "ru-en >> fr\n",
    "\n",
    "en-fr >>ru ready_res\n",
    "fr-en >>ru ready_res\n",
    "\n",
    "ru-fr>>en ready_res\n",
    "fr-ru>>en ready_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
