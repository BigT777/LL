{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime le vin\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(text,from_lang, to_lang):\n",
    "    tr = None\n",
    "    attempts = 0\n",
    "    while not tr:\n",
    "        try:\n",
    "            tr = translator.translate(text, src=from_lang, dest = to_lang).text\n",
    "            return tr\n",
    "        except:\n",
    "            tr = None\n",
    "            time.sleep(0.01)\n",
    "            attempts +=1\n",
    "            print(\"attempt\", attempts)\n",
    "translate ('I like wine', 'en','fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempt 1\n",
      "attempt 2\n",
      "attempt 3\n",
      "attempt 4\n",
      "attempt 5\n",
      "attempt 6\n",
      "attempt 7\n",
      "attempt 8\n",
      "attempt 9\n",
      "attempt 10\n",
      "attempt 11\n",
      "attempt 12\n",
      "attempt 13\n",
      "attempt 14\n",
      "attempt 15\n",
      "attempt 16\n",
      "attempt 17\n",
      "attempt 18\n",
      "attempt 19\n",
      "attempt 20\n",
      "attempt 21\n",
      "attempt 22\n",
      "attempt 23\n",
      "attempt 24\n",
      "attempt 25\n",
      "attempt 26\n",
      "attempt 27\n",
      "attempt 28\n",
      "attempt 29\n",
      "attempt 30\n",
      "attempt 31\n",
      "attempt 32\n",
      "attempt 33\n",
      "attempt 34\n",
      "attempt 35\n",
      "attempt 36\n",
      "attempt 37\n",
      "attempt 38\n",
      "attempt 39\n",
      "attempt 40\n",
      "attempt 41\n",
      "attempt 42\n",
      "attempt 43\n",
      "attempt 44\n",
      "attempt 45\n",
      "attempt 46\n",
      "attempt 47\n",
      "attempt 48\n",
      "attempt 49\n",
      "attempt 50\n",
      "attempt 51\n",
      "attempt 52\n",
      "attempt 53\n",
      "attempt 54\n",
      "attempt 55\n",
      "attempt 56\n",
      "attempt 57\n",
      "attempt 58\n",
      "attempt 59\n",
      "attempt 60\n",
      "attempt 61\n",
      "attempt 62\n",
      "attempt 63\n",
      "attempt 64\n",
      "attempt 65\n",
      "attempt 66\n",
      "attempt 67\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "sence_id_list = []\n",
    "definition = []\n",
    "examples = []\n",
    "def_trans = []\n",
    "ex_trans = []\n",
    "def sence_translation( from_lang, to_lang, word,words, sence_ids, defs, ex, definitions_trans, examples_trans, debug = False, show_sentence = False,show_all = False):\n",
    "    sence_id = 0 \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        words.append(word)\n",
    "        sence_ids.append(sence_id)\n",
    "        if (debug): print(i, synset)\n",
    "        comparison_data = []    \n",
    "        current_definition = synset.definition()\n",
    "        defs.append(current_definition)   \n",
    "        def_tr = translate (current_definition, from_lang,to_lang)\n",
    "        definitions_trans.append(def_tr)\n",
    "        examples =  synset.examples()\n",
    "        comparison_data = [ex_snetence for ex_snetence in examples]\n",
    "\n",
    "        #comparison_data.append(current_definition)#теперь сюда входят и примеры и само значение\n",
    "        \n",
    "        examples = []\n",
    "        translated_examples = []\n",
    "        for comp_sentence in comparison_data:\n",
    "            examples.append(comp_sentence)\n",
    "            ex_tr = translate (comp_sentence, from_lang,to_lang)\n",
    "            translated_examples.append(ex_tr)\n",
    "            if (debug):print(comp_sentence)\n",
    "        ex.append(examples)\n",
    "        examples_trans.append(translated_examples)\n",
    "        if (debug):print()\n",
    "        \n",
    "        sence_id += 1\n",
    "            \n",
    "sence_translation(\"en\", \"ru\", \"home\",word_list,sence_id_list,definition,examples,def_trans, ex_trans, debug = False, show_sentence = True)#True False\n",
    "df = pd.DataFrame(list(zip(word_list,sence_id_list,definition,examples,def_trans,ex_trans)), \n",
    "                                              columns =['word','sence_id','definition','example','def_trans','ex_trans']) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"break_wordnet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_for_eval = ['go','report','drive','look','feel','people','learn','table','bokk']\n",
    "def get_multisence_trans(words_list,export_name):\n",
    "    word_list = []\n",
    "    sence_id_list = []\n",
    "    definition = []\n",
    "    examples = []\n",
    "    def_trans = []\n",
    "    ex_trans = []\n",
    "    for word in words_list:\n",
    "        sence_translation(\"home\",word_list,sence_id_list,definition,examples,def_trans, ex_trans, debug = False, show_sentence = True)#True False\n",
    "    df = pd.DataFrame(list(zip(word_list,sence_id_list,definition,examples,def_trans,ex_trans)), \n",
    "                                              columns =['word','sence_id','definition','example','def_trans','ex_trans']) \n",
    "    df.to_csv(export_name + '.csv')\n",
    "\n",
    "get_multisence_trans(words_for_eval,'en_fr_wordnet')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk( word, sentence, debug = False, show_sentence = False,show_all = False):\n",
    "    \n",
    "    \n",
    "    \"\"\"Ваш код тут\"\"\"\n",
    "    sentence =  sentence.split(\" \")\n",
    "    sentence = [w for w in sentence if not w in stop_words] \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sentence = [wordnet_lemmatizer.lemmatize(w) for w in sentence]\n",
    "    \n",
    "    bestsense = 0\n",
    "    #maxoverlap = 0\n",
    "    min_mean_overlap_dist = 100#начальная точка новой метрики\n",
    "    final_definition = ''\n",
    "    \n",
    "    word_index = sentence.index(word)#индекс исследуемого слова в предложении\n",
    "    \n",
    "    #получаем часть речи для дальнейшего отграничения ранжира поиска\n",
    "    #pos_sentence = nltk.pos_tag(sentence)\n",
    "    #word_pos = get_pos(pos_sentence, word)\n",
    "    #if (debug): print(\"word_pos\", word_pos)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        #if (debug): print(i, synset)\n",
    "        comparison_data = []    \n",
    "        current_definition = synset.definition()\n",
    "        \n",
    "        definition = tokenize(current_definition)\n",
    "        #print(definition)\n",
    "        #if (debug):print(\"DEFINITION FULL\",current_definition)\n",
    "        \n",
    "        #if (debug):print(\"DEFINITION PROCESSED\",definition)\n",
    "        \n",
    "        \n",
    "        examples =  synset.examples()\n",
    "        comparison_data = [tokenize(ex_snetence) for ex_snetence in examples]\n",
    "        sentence_set = set(sentence)\n",
    "        #if (show_sentence): print(\"SETENCE\",  sentence)\n",
    "        \n",
    "        #if (debug): print(definition & sentence_set)\n",
    "        mean_overlap_distance = 0\n",
    "        comparison_data.append(definition)#теперь сюда входят и примеры и само значение\n",
    "        #print(comparison_data) \n",
    "        comparison_data_set = [set(el) for el in comparison_data]\n",
    "        if (show_sentence): print(\"SETENCE\",  sentence)\n",
    "        if (debug): print(\"START ITERATING OVER AL DATA\")\n",
    "        \n",
    "        word_distance = []\n",
    "        for comp_sentence_set,comp_sentence in zip(comparison_data_set, comparison_data):\n",
    "            if (debug):print(\"comp_sentence:\",' '.join(comp_sentence))\n",
    "            overlap_words_set = sentence_set & comp_sentence_set\n",
    "            if(word in overlap_words_set):\n",
    "                overlap_words_set.remove(word)\n",
    "\n",
    "            if (overlap_words_set and debug):print(\"overlap_words_set\",overlap_words_set)\n",
    "            for overlapped_word in overlap_words_set:\n",
    "                if (debug): print(\"disambig_word_index = \", word_index)\n",
    "                ind = sentence.index(overlapped_word)\n",
    "                if (debug):print(\"overlapped_word = \", overlapped_word)\n",
    "                if (debug): print(\"overlapped_word_ind\", ind)\n",
    "                abs_distance = abs(ind - word_index)\n",
    "                if (debug): print(\"abs_distance\", abs_distance)\n",
    "                word_distance.append(abs_distance)  \n",
    "        \n",
    "        \n",
    "        #overlap_len = len(overlap)\n",
    "        if (debug): print(\"word_distance\" , word_distance)\n",
    "        #применяем новую метрику\n",
    "        mean_overlap_distance = np.mean(word_distance) / len(word_distance)\n",
    "        if (debug):\n",
    "            if ( mean_overlap_distance == 0 or math.isnan(mean_overlap_distance) ): \n",
    "                print(\"ITERATING FINISHED.NO OVERLAP FOUND\",'\\n')\n",
    "        if(mean_overlap_distance > 0):\n",
    "            if (debug):print(\"OVERLAP_FOUND\")\n",
    "            if (debug):print(\"EXAMPLES_ORIGINAL = \", examples)\n",
    "            if (debug):print(\"ALL_COMPAR_DATA = \", comparison_data)\n",
    "            if (debug): print(i, synset)\n",
    "            if (debug):print(\"DEFINITION FULL\",current_definition)\n",
    "            if (debug):print(\"DEFINITION PROCESSED\",definition)\n",
    "            if (show_sentence): print(\"SETENCE\",  sentence)\n",
    "            if (debug): print(\"mean_overlap_distance\", mean_overlap_distance)\n",
    "                \n",
    "            if (debug): print(\"\\n\")\n",
    "        \n",
    "        #обновляем посчитанное значение если оно меньше предыдущего\n",
    "        if (mean_overlap_distance < min_mean_overlap_dist):\n",
    "            min_mean_overlap_dist = mean_overlap_distance\n",
    "            bestsense = i\n",
    "            final_definition = current_definition\n",
    "    \n",
    "    return bestsense, final_definition\n",
    "lesk(\"break\", \"They break the rules in order to convince the rule-makers\",debug = True, show_sentence = True)#True False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
