{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)  получаем lookup от исходного языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_token_dict = {\n",
    " \"vertolet_token\" : \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\",\n",
    "   \"bbk_token\" : \"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\", \n",
    "\"vasya_2019\" : \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\",\n",
    "\"vasya_2011\" : \"dict.1.1.20191105T083743Z.8b9cdebb9755fa57.e42afa31c184e2ced2305b5452809e17bc19c920\",\n",
    "\"fanyi0\" : \"dict.1.1.20191105T095108Z.82f72da803eeedc2.ada6293a0591bed0222d3a8045bd722915f025bb\",\n",
    "\"fanyi00\" : \"dict.1.1.20191105T103102Z.3e0f76aef01d786a.92fea3781473b7c432aa49ee1f3879cb95181a63\",\n",
    "\"prvd0\" : \"dict.1.1.20191105T103348Z.960e0892b02b628b.5746d7d95af92a16d15941b24410707bbb7568c4\",\n",
    "\"prvd00\" : \"dict.1.1.20191105T105231Z.520f147163f275cd.f329913de7b0a719762e89ef84914cbe8c3f3d25\",\n",
    "\"prvdfin0\": \"dict.1.1.20191105T105704Z.a6f94da915862b61.76003fa911b16bbad05aa655d5be9ea1f78b3e1c\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_lookup_json(from_lang_words_list, from_lang, to_lang, token_dict, directory_with_handled_files):\n",
    "    tokens_names_list = []\n",
    "    tokens_list = []\n",
    "    for token_name, token in token_dict.items():\n",
    "        tokens_names_list.append(token_name)\n",
    "        tokens_list.append(token)\n",
    "    token_index = 0 \n",
    "    not_handled_words_list = []\n",
    "    \n",
    "    save_folder = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2\"\n",
    "   \n",
    "    handled_words_from_folder = get_handled_words(directory_with_handled_files)\n",
    "    handled_words_from_folder = set(handled_words_from_folder)\n",
    "    handled_words_from_folder_2 = get_handled_words(save_folder)\n",
    "    handled_words_from_folder_2 = set(handled_words_from_folder_2)\n",
    "    handled_words_from_folder = handled_words_from_folder.union(handled_words_from_folder_2)\n",
    "    print(\"handled_all_words\", len(handled_words_from_folder))\n",
    "    \n",
    "    \n",
    "\n",
    "    count = 0 \n",
    "    print(\"going_to_use token\", tokens_names_list[token_index])\n",
    "    skipped_words_count = 0\n",
    "    handled_words_count = 0\n",
    "    for fr_word in tqdm(from_lang_words_list):\n",
    "        #print(fr_word)\n",
    "        isnan = False\n",
    "        #print(count, start_from_count)\n",
    "        try:\n",
    "            isnan = math.isnan(fr_word)\n",
    "        except:\n",
    "            pass\n",
    "        if fr_word != \"no_equality\" and isnan == False and fr_word not in handled_words_from_folder:\n",
    "            lang_pair = from_lang + \"-\" + to_lang\n",
    "            headers = {\"key\":tokens_list[token_index],\n",
    "                  \"lang\":lang_pair,\"text\":fr_word}\n",
    "            ddd = requests.get(url, headers).json()\n",
    "            if 'message' in ddd and ddd['message'] == \"Limit of daily requests exceeded\":\n",
    "                token_index += 1\n",
    "                print(\"LIMIT REACHED, switch to token\",tokens_names_list[token_index] )\n",
    "                not_handled_words_list.append(fr_word)\n",
    "            try:\n",
    "                save_loc = os.path.join(save_folder, fr_word + \".json\")\n",
    "            except:\n",
    "                print(fr_word, \"is ambigious float or smth\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(save_loc, 'w') as f:\n",
    "                    json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "                    #print(\"saved at \", save_loc)\n",
    "            except Exception as E:\n",
    "                print(fr_word, E)\n",
    "            #time.sleep(0.0001)\n",
    "            handled_words_count += 1\n",
    "        else:\n",
    "            skipped_words_count += 1\n",
    "            #print(fr_word, \"===>>>>skipped\")\n",
    "            pass\n",
    "            \n",
    "        count += 1\n",
    "        if count %500 == 0:\n",
    "            handled_words_from_folder = get_handled_words(directory_with_handled_files)\n",
    "            handled_words_from_folder = set(handled_words_from_folder)\n",
    "            handled_words_from_folder_2 = get_handled_words(save_folder)\n",
    "            handled_words_from_folder_2 = set(handled_words_from_folder_2)\n",
    "            handled_words_from_folder = handled_words_from_folder.union(handled_words_from_folder_2)\n",
    "            print(\"handled_words_from_folder\", len(handled_words_from_folder))\n",
    "            print(handled_words_count, \"words handled\", skipped_words_count, \"words skipped\\n\",)     \n",
    "    return not_handled_words_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en\"\n",
    "#from_ind = 1480+382+133+1858+15\n",
    "#to_ind = 25000\n",
    "from_ind = 0\n",
    "to_ind = len(words_to_be_handled)\n",
    "not_handled_words = get_y_lookup_json(words_to_be_handled[from_ind:to_ind], \"fr\", \"en\", small_token_dict,\n",
    "                  directory_with_handled_files = han dled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) объединяем сохраненные файлы в csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_yandex_lookup(directory, print_output = False):\n",
    "    word = []\n",
    "    local_word = []\n",
    "    pos_list = []\n",
    "    examples = []\n",
    "    examples_local = []\n",
    "    words_from_not_handled_count = 0 \n",
    "    empty = 0\n",
    "    for file in tqdm(os.listdir(directory)):\n",
    "        if print_output == True: print(file)\n",
    "        current_word = file.split(\".\")[0]\n",
    "        \n",
    "        if \"DS_Store\" not in file:\n",
    "            try:\n",
    "                f = open(os.path.join(directory,file), \"r\", encoding='utf-8')\n",
    "            except:\n",
    "                f = open(os.path.join(directory,file), \"r\")\n",
    "            try:\n",
    "                data = json.load(f, encoding = 'utf-8')\n",
    "            except:\n",
    "                data = json.load(f)\n",
    "            if \"def\" in data:\n",
    "                for definition in data['def']:\n",
    "                    word_current =  definition['text']\n",
    "                    if 'pos' in definition:\n",
    "                        global_pos = definition['pos']\n",
    "                    else:\n",
    "                        global_pos = 'no_pos_available'\n",
    "                    for translation in definition['tr']: \n",
    "                        if 'pos' in translation:\n",
    "                            pos = translation['pos']\n",
    "                        else:\n",
    "                            pos = global_pos\n",
    "                        pos_list.append(pos)\n",
    "                        word.append(word_current)\n",
    "                        local_word.append(translation['text'])\n",
    "                        ex_en = []\n",
    "                        ex_rus = []\n",
    "                        if 'ex' in translation:\n",
    "                            for exmpl in translation['ex']:\n",
    "                                #print(exmpl)\n",
    "                                ex_en.append(exmpl['text'])\n",
    "                                ex_rus.append(exmpl['tr'][0]['text'])\n",
    "                        examples.append(ex_en)\n",
    "                        examples_local.append(ex_rus)\n",
    "                if len(data['def']) == 0:\n",
    "                    empty += 1\n",
    "            words_from_not_handled_count += 1\n",
    "    print(\"empty json\", empty)\n",
    "    print(\"non_handled_basic_language__words_added_to_dataframe\", words_from_not_handled_count)\n",
    "    data = pd.DataFrame(list(zip(word,local_word,pos_list, examples, examples_local)),columns =['word', 'local_word', 'pos', 'examples','local_examples'])\n",
    "    return  data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Запускаем скрипт cross_approach_script к полученному csv. Меняем параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) объединяем с прошлым concat_medium_results_and_detect_missing_stuff "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
