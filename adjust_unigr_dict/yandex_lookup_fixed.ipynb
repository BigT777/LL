{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "url = \"https://dictionary.yandex.net/api/v1/dicservice.json/lookup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK HANDLED triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(handled_triplets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_triplets_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_triplets_list = []\n",
    "all_folders_path = \"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/cross_barbos/ress\"\n",
    "lang_1_list = []\n",
    "lang_2_list = []\n",
    "lang_3_list = []\n",
    "\n",
    "for folder in os.listdir(all_folders_path):\n",
    "    if \"DS_Store\" not in folder:\n",
    "        print(folder)\n",
    "        prev_df_len = 0\n",
    "        count = 0 \n",
    "        for file in tqdm(os.listdir(os.path.join(all_folders_path, folder))):\n",
    "            if \"DS_Store\" not in file:\n",
    "                file_csv = pd.read_csv(os.path.join(all_folders_path, folder, file))\n",
    "                for index, row in file_csv.iterrows():\n",
    "                    handled_triplet = row['en'] + \"_\" + row['ru'] + \"_\" + row['fr']\n",
    "                    if handled_triplet not in handled_triplets_list:\n",
    "                        handled_triplets_list.append(handled_triplet)\n",
    "                        lang_1_list.append(row['en'])\n",
    "                        lang_2_list.append(row['ru'])\n",
    "                        lang_3_list.append(row['fr'])\n",
    "                count += 1\n",
    "                        \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_en_ru_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr = pd.DataFrame()\n",
    "current_en_ru_fr['en'] = lang_1_list\n",
    "current_en_ru_fr['ru'] = lang_2_list\n",
    "current_en_ru_fr['fr'] = lang_3_list\n",
    "current_en_ru_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr = current_en_ru_fr.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_en_ru_fr.to_csv(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/cross_barbos/current_en_ru_fr_no_dub.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get handled eng_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_eng_word = pd.DataFrame()\n",
    "handled_eng_word['word'] = current_en_ru_fr['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_eng_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_eng_word_unique = handled_eng_word.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_eng_word_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(handled_eng_word_unique['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDLED_ENG_WORDS = list(handled_eng_word_unique['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET_ALL_NECECARY_VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word_list_unique = pd.read_csv(\"/Users/nigula/LL/adjust_unigr_dict/BASIC_ENG_VOCAB/eng_vocab_all_lemmas.csv\")\n",
    "final_word_list_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_necessary_eng_words = list(final_word_list_unique['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET WORDS not handled in triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_necessary_eng_words_set = set(all_necessary_eng_words)\n",
    "HANDLED_ENG_WORDS_set = set(HANDLED_ENG_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_necessary_eng_words_set),len(HANDLED_ENG_WORDS_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_in_triplet_eng_word = all_necessary_eng_words_set - HANDLED_ENG_WORDS_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_handled_in_triplet_eng_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_in_triplet_eng_word_csv = pd.DataFrame()\n",
    "not_handled_in_triplet_eng_word_csv['words_no_handled'] = list(not_handled_in_triplet_eng_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_in_triplet_eng_word_csv = not_handled_in_triplet_eng_word_csv.dropna()\n",
    "not_handled_in_triplet_eng_word_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_in_triplet_eng_word_csv.to_csv(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/cross_barbos/not_handled_in_triplet_eng_word.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET YANDEX LOOKUP ENG RUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_token_dict = {\n",
    " \"vertolet_token\" : \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\",\n",
    "   \"bbk_token\" : \"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\", \n",
    "\"vasya_2019\" : \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\",\n",
    "\"vasya_2011\" : \"dict.1.1.20191105T083743Z.8b9cdebb9755fa57.e42afa31c184e2ced2305b5452809e17bc19c920\",\n",
    "\"fanyi0\" : \"dict.1.1.20191105T095108Z.82f72da803eeedc2.ada6293a0591bed0222d3a8045bd722915f025bb\",\n",
    "\"fanyi00\" : \"dict.1.1.20191105T103102Z.3e0f76aef01d786a.92fea3781473b7c432aa49ee1f3879cb95181a63\",\n",
    "\"prvd0\" : \"dict.1.1.20191105T103348Z.960e0892b02b628b.5746d7d95af92a16d15941b24410707bbb7568c4\",\n",
    "\"prvd00\" : \"dict.1.1.20191105T105231Z.520f147163f275cd.f329913de7b0a719762e89ef84914cbe8c3f3d25\",\n",
    "\"prvdfin0\": \"dict.1.1.20191105T105704Z.a6f94da915862b61.76003fa911b16bbad05aa655d5be9ea1f78b3e1c\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_token_dict = {\n",
    " \"vertolet_token\" : \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\",\n",
    "   \"bbk_token\" : \"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\", \n",
    "\"fanyi0\" : \"dict.1.1.20191105T095108Z.82f72da803eeedc2.ada6293a0591bed0222d3a8045bd722915f025bb\",\n",
    "\"fanyi00\" : \"dict.1.1.20191105T103102Z.3e0f76aef01d786a.92fea3781473b7c432aa49ee1f3879cb95181a63\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(token_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertolet_token = \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\"\n",
    "bbk_token = \"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\"\n",
    "vasya_2019 = \"dict.1.1.20191029T212638Z.e3a6524ff15de8ee.53dbf57d908343531709c5f0686045121f0fc6ae\"\n",
    "vasya_2011 = \"dict.1.1.20191105T083743Z.8b9cdebb9755fa57.e42afa31c184e2ced2305b5452809e17bc19c920\"\n",
    "fanyi0 = \"dict.1.1.20191105T095108Z.82f72da803eeedc2.ada6293a0591bed0222d3a8045bd722915f025bb\"\n",
    "fanyi00 = \"dict.1.1.20191105T103102Z.3e0f76aef01d786a.92fea3781473b7c432aa49ee1f3879cb95181a63\"\n",
    "prvd0 = \"dict.1.1.20191105T103348Z.960e0892b02b628b.5746d7d95af92a16d15941b24410707bbb7568c4\"\n",
    "prvd00 = \"dict.1.1.20191105T105231Z.520f147163f275cd.f329913de7b0a719762e89ef84914cbe8c3f3d25\"\n",
    "prvdfin0 = \"dict.1.1.20191105T105704Z.a6f94da915862b61.76003fa911b16bbad05aa655d5be9ea1f78b3e1c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_handled_words(\"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_lookup_json(from_lang_words_list, from_lang, to_lang, token_dict, directory_with_handled_files):\n",
    "    tokens_names_list = []\n",
    "    tokens_list = []\n",
    "    for token_name, token in token_dict.items():\n",
    "        tokens_names_list.append(token_name)\n",
    "        tokens_list.append(token)\n",
    "    token_index = 0 \n",
    "    not_handled_words_list = []\n",
    "    \n",
    "    save_folder = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2\"\n",
    "   \n",
    "    handled_words_from_folder = get_handled_words(directory_with_handled_files)\n",
    "    handled_words_from_folder = set(handled_words_from_folder)\n",
    "    handled_words_from_folder_2 = get_handled_words(save_folder)\n",
    "    handled_words_from_folder_2 = set(handled_words_from_folder_2)\n",
    "    handled_words_from_folder = handled_words_from_folder.union(handled_words_from_folder_2)\n",
    "    print(\"handled_all_words\", len(handled_words_from_folder))\n",
    "    \n",
    "    \n",
    "\n",
    "    count = 0 \n",
    "    print(\"going_to_use token\", tokens_names_list[token_index])\n",
    "    skipped_words_count = 0\n",
    "    handled_words_count = 0\n",
    "    for fr_word in tqdm(from_lang_words_list):\n",
    "        #print(fr_word)\n",
    "        isnan = False\n",
    "        #print(count, start_from_count)\n",
    "        try:\n",
    "            isnan = math.isnan(fr_word)\n",
    "        except:\n",
    "            pass\n",
    "        if fr_word != \"no_equality\" and isnan == False and fr_word not in handled_words_from_folder:\n",
    "            lang_pair = from_lang + \"-\" + to_lang\n",
    "            headers = {\"key\":tokens_list[token_index],\n",
    "                  \"lang\":lang_pair,\"text\":fr_word}\n",
    "            ddd = requests.get(url, headers).json()\n",
    "            if 'message' in ddd and ddd['message'] == \"Limit of daily requests exceeded\":\n",
    "                token_index += 1\n",
    "                print(\"LIMIT REACHED, switch to token\",tokens_names_list[token_index] )\n",
    "                not_handled_words_list.append(fr_word)\n",
    "            try:\n",
    "                save_loc = os.path.join(save_folder, fr_word + \".json\")\n",
    "            except:\n",
    "                print(fr_word, \"is ambigious float or smth\")\n",
    "                continue\n",
    "            try:\n",
    "                with open(save_loc, 'w') as f:\n",
    "                    json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "                    #print(\"saved at \", save_loc)\n",
    "            except Exception as E:\n",
    "                print(fr_word, E)\n",
    "            #time.sleep(0.0001)\n",
    "            handled_words_count += 1\n",
    "        else:\n",
    "            skipped_words_count += 1\n",
    "            #print(fr_word, \"===>>>>skipped\")\n",
    "            pass\n",
    "            \n",
    "        count += 1\n",
    "        if count %500 == 0:\n",
    "            handled_words_from_folder = get_handled_words(directory_with_handled_files)\n",
    "            handled_words_from_folder = set(handled_words_from_folder)\n",
    "            handled_words_from_folder_2 = get_handled_words(save_folder)\n",
    "            handled_words_from_folder_2 = set(handled_words_from_folder_2)\n",
    "            handled_words_from_folder = handled_words_from_folder.union(handled_words_from_folder_2)\n",
    "            print(\"handled_words_from_folder\", len(handled_words_from_folder))\n",
    "            print(handled_words_count, \"words handled\", skipped_words_count, \"words skipped\\n\",)     \n",
    "    return not_handled_words_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get fr ru lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handled_words(directory_with_handled_files):\n",
    "    handled_words_from_folder = []\n",
    "    for word in os.listdir(directory_with_handled_files):\n",
    "        #print(word.split('.')[0])\n",
    "        handled_words_from_folder.append(word.split('.')[0])\n",
    "    handled_words_from_folder = set(handled_words_from_folder)\n",
    "    #print(check_word, \"word_in\", check_word in handled_words_from_folder)\n",
    "    #print(\"akready_handled_from_directory\", len(handled_words_from_folder))\n",
    "    return handled_words_from_folder\n",
    "\n",
    "a = get_handled_words(\"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54398"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handled_words_from_folder = get_handled_words(\"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en\")\n",
    "handled_words_from_folder = set(handled_words_from_folder)\n",
    "handled_words_from_folder_2 = get_handled_words(\"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2\")\n",
    "handled_words_from_folder_2 = set(handled_words_from_folder_2)\n",
    "all_handled = handled_words_from_folder.union(handled_words_from_folder_2)\n",
    "len(handled_words_from_folder.union(handled_words_from_folder_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_handled - set(v2_dict['fr']))#!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= set(['1','2'])\n",
    "b = set(['1','2'])\n",
    "len(a.union(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45541"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(handled_words_from_folder.union(handled_words_from_folder_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45443"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(handled_words_from_folder.union(handled_words_from_folder_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['culminant',\n",
       " 'jeux olympiques',\n",
       " 'astrakan',\n",
       " 'télépathe',\n",
       " 'pluralisme',\n",
       " 'du tout au tout',\n",
       " 'sans récompense',\n",
       " 'me marie',\n",
       " 'limbo',\n",
       " 'fièvre cérébrale']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_dict = pd.read_csv(\"./reverse_connected_dicts/to_release/merge_reverse_en_ru_fr_vs_ru_fr_en.csv\")\n",
    "words_to_be_handled = list(set(v2_dict['fr']))\n",
    "words_to_be_handled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57206"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(v2_dict['fr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57206"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_be_handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_token_dict = {\n",
    "\"fanyi0\" : \"dict.1.1.20191105T095108Z.82f72da803eeedc2.ada6293a0591bed0222d3a8045bd722915f025bb\",\n",
    "\"fanyi00\" : \"dict.1.1.20191105T103102Z.3e0f76aef01d786a.92fea3781473b7c432aa49ee1f3879cb95181a63\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_handled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/coupole.json\n",
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/détecteur de mensonges.json\n",
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/infliction.json\n",
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/intrants-extrants.json\n",
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/groupe dirigeant.json\n",
    "saved at  /Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/faire le maître.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handled_all_words 45554\n",
      "going_to_use token fanyi0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7887db965e4f3ab0e93b8b02af2015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57206), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT REACHED, switch to token fanyi00\n",
      "handled_words_from_folder 45561\n",
      "7 words handled 493 words skipped\n",
      "\n",
      "handled_words_from_folder 45667\n",
      "113 words handled 887 words skipped\n",
      "\n",
      "handled_words_from_folder 45767\n",
      "214 words handled 1286 words skipped\n",
      "\n",
      "handled_words_from_folder 45868\n",
      "316 words handled 1684 words skipped\n",
      "\n",
      "handled_words_from_folder 45958\n",
      "406 words handled 2094 words skipped\n",
      "\n",
      "handled_words_from_folder 46056\n",
      "505 words handled 2495 words skipped\n",
      "\n",
      "handled_words_from_folder 46167\n",
      "618 words handled 2882 words skipped\n",
      "\n",
      "handled_words_from_folder 46281\n",
      "732 words handled 3268 words skipped\n",
      "\n",
      "handled_words_from_folder 46382\n",
      "833 words handled 3667 words skipped\n",
      "\n",
      "handled_words_from_folder 46482\n",
      "935 words handled 4065 words skipped\n",
      "\n",
      "handled_words_from_folder 46599\n",
      "1052 words handled 4448 words skipped\n",
      "\n",
      "handled_words_from_folder 46710\n",
      "1164 words handled 4836 words skipped\n",
      "\n",
      "handled_words_from_folder 46807\n",
      "1261 words handled 5239 words skipped\n",
      "\n",
      "handled_words_from_folder 46898\n",
      "1353 words handled 5647 words skipped\n",
      "\n",
      "handled_words_from_folder 46998\n",
      "1453 words handled 6047 words skipped\n",
      "\n",
      "handled_words_from_folder 47122\n",
      "1571 words handled 6429 words skipped\n",
      "\n",
      "handled_words_from_folder 47296\n",
      "1671 words handled 6829 words skipped\n",
      "\n",
      "handled_words_from_folder 47504\n",
      "1784 words handled 7216 words skipped\n",
      "\n",
      "handled_words_from_folder 47709\n",
      "1895 words handled 7605 words skipped\n",
      "\n",
      "handled_words_from_folder 47900\n",
      "1997 words handled 8003 words skipped\n",
      "\n",
      "handled_words_from_folder 48104\n",
      "2103 words handled 8397 words skipped\n",
      "\n",
      "handled_words_from_folder 48315\n",
      "2207 words handled 8793 words skipped\n",
      "\n",
      "handled_words_from_folder 48509\n",
      "2307 words handled 9193 words skipped\n",
      "\n",
      "handled_words_from_folder 48704\n",
      "2403 words handled 9597 words skipped\n",
      "\n",
      "handled_words_from_folder 48886\n",
      "2495 words handled 10005 words skipped\n",
      "\n",
      "handled_words_from_folder 49033\n",
      "2573 words handled 10427 words skipped\n",
      "\n",
      "handled_words_from_folder 49205\n",
      "2663 words handled 10837 words skipped\n",
      "\n",
      "handled_words_from_folder 49395\n",
      "2760 words handled 11240 words skipped\n",
      "\n",
      "handled_words_from_folder 49564\n",
      "2845 words handled 11655 words skipped\n",
      "\n",
      "handled_words_from_folder 49752\n",
      "2944 words handled 12056 words skipped\n",
      "\n",
      "handled_words_from_folder 49909\n",
      "3027 words handled 12473 words skipped\n",
      "\n",
      "handled_words_from_folder 50087\n",
      "3123 words handled 12877 words skipped\n",
      "\n",
      "handled_words_from_folder 50270\n",
      "3217 words handled 13283 words skipped\n",
      "\n",
      "1/2 heure [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/1/2 heure.json'\n",
      "handled_words_from_folder 50438\n",
      "3301 words handled 13699 words skipped\n",
      "\n",
      "handled_words_from_folder 50618\n",
      "3399 words handled 14101 words skipped\n",
      "\n",
      "handled_words_from_folder 50765\n",
      "3470 words handled 14530 words skipped\n",
      "\n",
      "handled_words_from_folder 50921\n",
      "3550 words handled 14950 words skipped\n",
      "\n",
      "francs/mois [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/francs/mois.json'\n",
      "handled_words_from_folder 51095\n",
      "3642 words handled 15358 words skipped\n",
      "\n",
      "handled_words_from_folder 51229\n",
      "3715 words handled 15785 words skipped\n",
      "\n",
      "handled_words_from_folder 51408\n",
      "3806 words handled 16194 words skipped\n",
      "\n",
      "handled_words_from_folder 51536\n",
      "3875 words handled 16625 words skipped\n",
      "\n",
      "handled_words_from_folder 51676\n",
      "3948 words handled 17052 words skipped\n",
      "\n",
      "handled_words_from_folder 51814\n",
      "4022 words handled 17478 words skipped\n",
      "\n",
      "handled_words_from_folder 51904\n",
      "4113 words handled 17887 words skipped\n",
      "\n",
      "handled_words_from_folder 51993\n",
      "4203 words handled 18297 words skipped\n",
      "\n",
      "1/2h [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/1/2h.json'\n",
      "handled_words_from_folder 52080\n",
      "4293 words handled 18707 words skipped\n",
      "\n",
      "handled_words_from_folder 52150\n",
      "4365 words handled 19135 words skipped\n",
      "\n",
      "handled_words_from_folder 52225\n",
      "4442 words handled 19558 words skipped\n",
      "\n",
      "handled_words_from_folder 52303\n",
      "4521 words handled 19979 words skipped\n",
      "\n",
      "handled_words_from_folder 52373\n",
      "4592 words handled 20408 words skipped\n",
      "\n",
      "24h/24 [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/24h/24.json'\n",
      "handled_words_from_folder 52440\n",
      "4661 words handled 20839 words skipped\n",
      "\n",
      "24 h/24 [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/24 h/24.json'\n",
      "handled_words_from_folder 52507\n",
      "4730 words handled 21270 words skipped\n",
      "\n",
      "handled_words_from_folder 52577\n",
      "4801 words handled 21699 words skipped\n",
      "\n",
      "handled_words_from_folder 52639\n",
      "4863 words handled 22137 words skipped\n",
      "\n",
      "handled_words_from_folder 52704\n",
      "4928 words handled 22572 words skipped\n",
      "\n",
      "handled_words_from_folder 52772\n",
      "4997 words handled 23003 words skipped\n",
      "\n",
      "handled_words_from_folder 52852\n",
      "5079 words handled 23421 words skipped\n",
      "\n",
      "handled_words_from_folder 52930\n",
      "5157 words handled 23843 words skipped\n",
      "\n",
      "handled_words_from_folder 53010\n",
      "5238 words handled 24262 words skipped\n",
      "\n",
      "handled_words_from_folder 53089\n",
      "5319 words handled 24681 words skipped\n",
      "\n",
      "handled_words_from_folder 53164\n",
      "5395 words handled 25105 words skipped\n",
      "\n",
      "handled_words_from_folder 53245\n",
      "5477 words handled 25523 words skipped\n",
      "\n",
      "handled_words_from_folder 53321\n",
      "5553 words handled 25947 words skipped\n",
      "\n",
      "24/24h [Errno 2] No such file or directory: '/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en_2/24/24h.json'\n",
      "handled_words_from_folder 53398\n",
      "5632 words handled 26368 words skipped\n",
      "\n",
      "handled_words_from_folder 53471\n",
      "5708 words handled 26792 words skipped\n",
      "\n",
      "handled_words_from_folder 53543\n",
      "5782 words handled 27218 words skipped\n",
      "\n",
      "handled_words_from_folder 53617\n",
      "5856 words handled 27644 words skipped\n",
      "\n",
      "handled_words_from_folder 53689\n",
      "5931 words handled 28069 words skipped\n",
      "\n",
      "handled_words_from_folder 53771\n",
      "6013 words handled 28487 words skipped\n",
      "\n",
      "handled_words_from_folder 53845\n",
      "6089 words handled 28911 words skipped\n",
      "\n",
      "handled_words_from_folder 53915\n",
      "6161 words handled 29339 words skipped\n",
      "\n",
      "handled_words_from_folder 54002\n",
      "6248 words handled 29752 words skipped\n",
      "\n",
      "handled_words_from_folder 54084\n",
      "6332 words handled 30168 words skipped\n",
      "\n",
      "handled_words_from_folder 54152\n",
      "6400 words handled 30600 words skipped\n",
      "\n",
      "handled_words_from_folder 54227\n",
      "6476 words handled 31024 words skipped\n",
      "\n",
      "handled_words_from_folder 54290\n",
      "6544 words handled 31456 words skipped\n",
      "\n",
      "handled_words_from_folder 54376\n",
      "6631 words handled 31869 words skipped\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9a2850da56ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mto_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_to_be_handled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m not_handled_words = get_y_lookup_json(words_to_be_handled[from_ind:to_ind], \"fr\", \"en\", small_token_dict,\n\u001b[0;32m----> 7\u001b[0;31m                   directory_with_handled_files = handled)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c8f3b216bb76>\u001b[0m in \u001b[0;36mget_y_lookup_json\u001b[0;34m(from_lang_words_list, from_lang, to_lang, token_dict, directory_with_handled_files)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'message'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mddd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Limit of daily requests exceeded\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mtoken_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LIMIT REACHED, switch to token\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens_names_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mnot_handled_words_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "handled = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en\"\n",
    "#from_ind = 1480+382+133+1858+15\n",
    "#to_ind = 25000\n",
    "from_ind = 0\n",
    "to_ind = len(words_to_be_handled)\n",
    "not_handled_words = get_y_lookup_json(words_to_be_handled[from_ind:to_ind], \"fr\", \"en\", small_token_dict,\n",
    "                  directory_with_handled_files = han dled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = {\"a\":\"b\"}\n",
    "with open(\"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_fr_en/Rachel.json\", \"w\") as f:\n",
    "    json.dump(test_word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coran\n",
      "serbie\n",
      "David\n",
      "anglais\n",
      "ab\n",
      "j\n",
      "halal\n",
      "Norvégien\n",
      "Ambre\n",
      "Apocalypse\n",
      "Renoir\n",
      "azerbaïdjanais\n",
      "morphée\n",
      "Dustin\n",
      "Sicile\n",
      "birmingham\n",
      "rubinstein\n",
      "arctique\n",
      "inox\n",
      "Afrique australe\n",
      "Ext\n",
      "Athénien\n",
      "Syrie\n",
      "Sherman\n",
      "nuremberg\n",
      "Labo\n",
      "Écosse\n",
      "Kelvin\n",
      "site internet\n",
      "Managua\n",
      "Suzanne\n",
      "prussien\n",
      "scone\n",
      "Nairobi\n",
      "susanna\n",
      "Madame\n",
      "mahomet\n",
      "L\n",
      "Argentin\n",
      "zimmerman\n",
      "russe\n",
      "libye\n",
      "Santiago\n",
      "osborn\n",
      "Milan\n",
      "birmanie\n",
      "Carolyn\n",
      "maya\n",
      "Ézéchiel\n",
      "Rébecca\n",
      "Spartan\n",
      "Angleterre\n",
      "Rachel\n",
      "Cleveland\n",
      "Frédéric\n",
      "Hor\n",
      "Goya\n",
      "kevin\n",
      "ford\n",
      "Stéphanie\n",
      "comb\n",
      "istanbul\n",
      "GRETA\n",
      "rouget\n",
      "haïtien\n",
      "Khodorkovski\n",
      "ARN\n",
      "fonds\n",
      "Stuart\n",
      "Robin\n",
      "République Arabe syrienne\n",
      "scandinave\n",
      "Costaricien\n",
      "debra\n",
      "Compt\n",
      "victoria\n",
      "dist\n",
      "Seigneur\n",
      "Morgan\n",
      "ka\n",
      "PA\n",
      "Bert\n",
      "nouveau testament\n",
      "Haut\n",
      "aryen\n",
      "osc\n",
      "kepler\n",
      "dwight\n",
      "hassan\n",
      "barcelone\n",
      "MacLeod\n",
      "Apollo\n",
      "alexandrie\n",
      "Welch\n",
      "Ted\n",
      "galion\n",
      "hermès\n",
      "stewart\n",
      "état\n",
      "Bridget\n",
      "Hank\n",
      "Prague\n",
      "Gershwin\n",
      "aral\n",
      "Manille\n",
      "alexandre\n",
      "Acc\n",
      "faulkner\n",
      "Chick\n",
      "A\n",
      "bombay\n",
      "athènes\n",
      "Art\n",
      "Libby\n",
      "autrichien\n",
      "Giselle\n",
      "hubert\n",
      "Benjamin\n",
      "b\n",
      "Elliott\n",
      "Stradivarius\n",
      "cedric\n",
      "noir\n",
      "Delhi\n",
      "britt\n",
      "Juda\n",
      "Barry\n",
      "zagreb\n",
      "Caire\n",
      "somalien\n",
      "indien\n",
      "iran\n",
      "espagnol\n",
      "Oka\n",
      "Fact\n",
      "sabine\n",
      "Bacchus\n",
      "michel\n",
      "éd\n",
      "Turc\n",
      "ouzbékistan\n",
      "Amérique centrale\n",
      "Fidji\n",
      "Rés\n",
      "lisbonne\n",
      "Kate\n",
      "thésée\n",
      "o\n",
      "richard\n",
      "Sisyphe\n",
      "Emma\n",
      "bonn\n",
      "rayons x\n",
      "John\n",
      "afro-américain\n",
      "Dniestr\n",
      "Marlin\n",
      "devon\n",
      "Staline\n",
      "Eq\n",
      "Pérou\n",
      "clotilde\n",
      "Av\n",
      "highland\n",
      "Hymen\n",
      "firefox\n",
      "Pot\n",
      "horace\n",
      "mes\n",
      "excellence\n",
      "Larg\n",
      "chypre\n",
      "or\n",
      "Koweït\n",
      "volga\n",
      "empire\n",
      "Papou\n",
      "q\n",
      "de crimée\n",
      "beach\n",
      "joe\n",
      "int\n",
      "Malais\n",
      "Rotterdam\n",
      "Katar\n",
      "Conseil\n",
      "saxon\n",
      "B\n",
      "Nadine\n",
      "Paul\n",
      "pollock\n",
      "Manager\n",
      "Dimitri\n",
      "gary\n",
      "eugène\n",
      "Tchèque\n",
      "caracas\n",
      "Albanie\n",
      "tadjikistan\n",
      "mer du Nord\n",
      "yémen\n",
      "libérien\n",
      "venezuela\n",
      "Olympe\n",
      "kelly\n",
      "Montague\n",
      "Bornéo\n",
      "mer des caraïbes\n",
      "france\n",
      "pme\n",
      "Rhin\n",
      "hongrois\n",
      "Cincinnati\n",
      "rédempteur\n",
      "argentine\n",
      "PDA\n",
      "Mer\n",
      "Munich\n",
      "Manchester\n",
      "adn\n",
      "Adriatique\n",
      "trump\n",
      "Cyanide\n",
      "euphrate\n",
      "Johannesburg\n",
      "primus\n",
      "berlinois\n",
      "flandre\n",
      "akready_handled_from_directory 228\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rédempteur\" in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ouzbékistan\" in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get en ru lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_en_ru\"\n",
    "\n",
    "not_handled_words = get_y_lookup_json(not_handled_in_triplet_eng_word, \"en\", \"ru\", token = bbk_token,\n",
    "                  directory_with_handled_files = handled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ru fr lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ru_fr_dict = pd.read_csv(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/to_release/en_ru_fr_V1.csv\")\n",
    "russian_words = list(set(en_ru_fr_dict['ru']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ru_fr_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(russian_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_ru_fr\"\n",
    "get_y_lookup_json(russian_words[11345+11421+10592+11501+425+10132+11229:], \"ru\", \"fr\", token = prvdfin0,\n",
    "                  directory_with_handled_files = handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET EN FR LOOKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_word_list_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final_word_list_unique['word'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled = \"/Users/nigula/LL/adjust_unigr_dict/lookup/yandex_lookup_en_fr\"\n",
    "\n",
    "get_y_lookup_json(all_necessary_eng_words, \"en\", \"fr\", token = vertolet_token,\n",
    "                  directory_with_handled_files = handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "         \"lang\":\"en-ru\",\"text\":\"book\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_links = pd.read_csv(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/y_crossrevers_vs_pos_cycle/four_links_dict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = True\n",
    "handled = []\n",
    "for fr_word in four_links['ru']:\n",
    "    print(fr_word)\n",
    "    isnan = False\n",
    "    if start == True:\n",
    "        try:\n",
    "            isnan = math.isnan(fr_word)\n",
    "        except:\n",
    "            pass\n",
    "        if fr_word != \"no_equality\" and isnan == False and fr_word not in handled:\n",
    "            print(\"proceed\")\n",
    "            headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "                  \"lang\":\"ru-fr\",\"text\":fr_word}\n",
    "            ddd = requests.get(url, headers).json()\n",
    "            save_loc = \"./lookup/yandex_lookup_ru_fr/\" + fr_word + \".json\"\n",
    "            with open(save_loc, 'w') as f:\n",
    "                json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "            handled.append(fr_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ru-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_links = pd.read_csv(\"/Users/nigula/LL/adjust_unigr_dict/reverse_connected_dicts/y_crossrevers_vs_pos_cycle/glob_dict_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = True\n",
    "handled = []\n",
    "for fr_word in five_links['ru']:\n",
    "    print(fr_word)\n",
    "    isnan = False\n",
    "    if start == True:\n",
    "        try:\n",
    "            isnan = math.isnan(fr_word)\n",
    "        except:\n",
    "            pass\n",
    "        if fr_word != \"no_equality\" and isnan == False and fr_word not in handled and not in directory_handled_words:\n",
    "            print(\"proceed\")\n",
    "            headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "                  \"lang\":\"ru-en\",\"text\":fr_word}\n",
    "            ddd = requests.get(url, headers).json()\n",
    "            save_loc = \"./lookup/yandex_lookup_ru_en/\" + fr_word + \".json\"\n",
    "            with open(save_loc, 'w') as f:\n",
    "                json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "            handled.append(fr_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"url = \\\"https://dictionary.yandex.net/api/v1/dicservice.json/getLangs\\\"\\n\",\n",
    "    \"h = {\\\"key\\\":\\\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\\\"}\\n\",\n",
    "    \"d = requests.get(url, h).json()\"\n",
    "   ]\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"def get_word_forms(word):\\n\",\n",
    "    \"    req = \\\"\\\"\\\"SELECT DISTINCT\\n\",\n",
    "    \"    content_words.word_id,content_words.word_lemma, content_words.word_value\\n\",\n",
    "    \"    FROM content_words\\n\",\n",
    "    \"    WHERE content_words.word_value !~ ('\\\\W') AND\\n\",\n",
    "    \"    (array_length(regexp_split_to_array(content_words.word_value, '[ ''-]'), 1) = 1) \\n\",\n",
    "    \"    and word_lemma != 0 and word_hash = calc_hash(' \\\"\\\"\\\" + word + \\\"\\\"\\\"')\\\"\\\"\\\"\\n\",\n",
    "    \"    cursor.execute(req)\\n\",\n",
    "    \"    req_res = cursor.fetchone()\\n\",\n",
    "    \"    if not req_res:\\n\",\n",
    "    \"        print(word, \\\" is not in db\\\")\\n\",\n",
    "    \"        return []\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        #print(a[0])\\n\",\n",
    "    \"        word_id = req_res[0]\\n\",\n",
    "    \"    word_forms_request = \\\"\\\"\\\"SELECT DISTINCT \\n\",\n",
    "    \"    content_words.word_value\\n\",\n",
    "    \"    FROM content_words\\n\",\n",
    "    \"    where word_lemma = \\\"\\\"\\\" + str(word_id) + \\\"\\\"\\\"  and word_lemma != word_id \\\"\\\"\\\"\\n\",\n",
    "    \"    cursor.execute(word_forms_request)\\n\",\n",
    "    \"    word_forms = []\\n\",\n",
    "    \"    for word in cursor:\\n\",\n",
    "    \"        #print(word[0])\\n\",\n",
    "    \"        word_forms.append(word[0])\\n\",\n",
    "    \"    return word_forms\\n\",\n",
    "    \"get_word_forms(\\\"book\\\")\\n\"\n",
    "   ]\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"['be-be',\\n\",\n",
    "    \" 'be-ru',\\n\",\n",
    "    \" 'bg-ru',\\n\",\n",
    "    \" 'cs-cs',\\n\",\n",
    "    \" 'cs-en',\\n\",\n",
    "    \" 'cs-ru',\\n\",\n",
    "    \" 'da-en',\\n\",\n",
    "    \" 'da-ru',\\n\",\n",
    "    \" 'de-de',\\n\",\n",
    "    \" 'de-en',\\n\",\n",
    "    \" 'de-ru',\\n\",\n",
    "    \" 'de-tr',\\n\",\n",
    "    \" 'el-en',\\n\",\n",
    "    \" 'el-ru',\\n\",\n",
    "    \" 'en-cs',\\n\",\n",
    "    \" 'en-da',\\n\",\n",
    "    \" 'en-de',\\n\",\n",
    "    \" 'en-el',\\n\",\n",
    "    \" 'en-en',\\n\",\n",
    "    \" 'en-es',\\n\",\n",
    "    \" 'en-et',\\n\",\n",
    "    \" 'en-fi',\\n\",\n",
    "    \" 'en-fr',\\n\",\n",
    "    \" 'en-it',\\n\",\n",
    "    \" 'en-lt',\\n\",\n",
    "    \" 'en-lv',\\n\",\n",
    "    \" 'en-nl',\\n\",\n",
    "    \" 'en-no',\\n\",\n",
    "    \" 'en-pt',\\n\",\n",
    "    \" 'en-ru',\\n\",\n",
    "    \" 'en-sk',\\n\",\n",
    "    \" 'en-sv',\\n\",\n",
    "    \" 'en-tr',\\n\",\n",
    "    \" 'en-uk',\\n\",\n",
    "    \" 'es-en',\\n\",\n",
    "    \" 'es-es',\\n\",\n",
    "    \" 'es-ru',\\n\",\n",
    "    \" 'et-en',\\n\",\n",
    "    \" 'et-ru',\\n\",\n",
    "    \" 'fi-en',\\n\",\n",
    "    \" 'fi-ru',\\n\",\n",
    "    \" 'fi-fi',\\n\",\n",
    "    \" 'fr-fr',\\n\",\n",
    "    \" 'fr-en',\\n\",\n",
    "    \" 'fr-ru',\\n\",\n",
    "    \" 'hu-hu',\\n\",\n",
    "    \" 'hu-ru',\\n\",\n",
    "    \" 'it-en',\\n\",\n",
    "    \" 'it-it',\\n\",\n",
    "    \" 'it-ru',\\n\",\n",
    "    \" 'lt-en',\\n\",\n",
    "    \" 'lt-lt',\\n\",\n",
    "    \" 'lt-ru',\\n\",\n",
    "    \" 'lv-en',\\n\",\n",
    "    \" 'lv-ru',\\n\",\n",
    "    \" 'mhr-ru',\\n\",\n",
    "    \" 'mrj-ru',\\n\",\n",
    "    \" 'nl-en',\\n\",\n",
    "    \" 'nl-ru',\\n\",\n",
    "    \" 'no-en',\\n\",\n",
    "    \" 'no-ru',\\n\",\n",
    "    \" 'pl-ru',\\n\",\n",
    "    \" 'pt-en',\\n\",\n",
    "    \" 'pt-ru',\\n\",\n",
    "    \" 'ru-be',\\n\",\n",
    "    \" 'ru-bg',\\n\",\n",
    "    \" 'ru-cs',\\n\",\n",
    "    \" 'ru-da',\\n\",\n",
    "    \" 'ru-de',\\n\",\n",
    "    \" 'ru-el',\\n\",\n",
    "    \" 'ru-en',\\n\",\n",
    "    \" 'ru-es',\\n\",\n",
    "    \" 'ru-et',\\n\",\n",
    "    \" 'ru-fi',\\n\",\n",
    "    \" 'ru-fr',\\n\",\n",
    "    \" 'ru-hu',\\n\",\n",
    "    \" 'ru-it',\\n\",\n",
    "    \" 'ru-lt',\\n\",\n",
    "    \" 'ru-lv',\\n\",\n",
    "    \" 'ru-mhr',\\n\",\n",
    "    \" 'ru-mrj',\\n\",\n",
    "    \" 'ru-nl',\\n\",\n",
    "    \" 'ru-no',\\n\",\n",
    "    \" 'ru-pl',\\n\",\n",
    "    \" 'ru-pt',\\n\",\n",
    "    \" 'ru-ru',\\n\",\n",
    "    \" 'ru-sk',\\n\",\n",
    "    \" 'ru-sv',\\n\",\n",
    "    \" 'ru-tr',\\n\",\n",
    "    \" 'ru-tt',\\n\",\n",
    "    \" 'ru-uk',\\n\",\n",
    "    \" 'sk-en',\\n\",\n",
    "    \" 'sk-ru',\\n\",\n",
    "    \" 'sv-en',\\n\",\n",
    "    \" 'sv-ru',\\n\",\n",
    "    \" 'tr-de',\\n\",\n",
    "    \" 'tr-en',\\n\",\n",
    "    \" 'tr-ru',\\n\",\n",
    "    \" 'tt-ru',\\n\",\n",
    "    \" 'uk-en',\\n\",\n",
    "    \" 'uk-ru',\\n\",\n",
    "    \" 'uk-uk',\\n\",\n",
    "    \" 'zh-ru']\"\n",
    "   ]\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
