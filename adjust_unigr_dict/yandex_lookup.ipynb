{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://dictionary.yandex.net/api/v1/dicservice.json/lookup\"\n",
    "headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "          \"lang\":\"ru-fr\",\"text\":\"book\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': {},\n",
       " 'def': [{'text': 'книга',\n",
       "   'pos': 'noun',\n",
       "   'gen': 'ж',\n",
       "   'anm': 'неодуш',\n",
       "   'tr': [{'text': 'livre',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'm',\n",
       "     'syn': [{'text': 'carnet', 'pos': 'noun', 'gen': 'm'},\n",
       "      {'text': 'livret', 'pos': 'noun', 'gen': 'm'},\n",
       "      {'text': 'imprimé', 'pos': 'noun', 'gen': 'm'},\n",
       "      {'text': 'bouquin', 'pos': 'noun', 'gen': 'm'},\n",
       "      {'text': 'journal', 'pos': 'noun', 'gen': 'm'},\n",
       "      {'text': 'registre', 'pos': 'noun', 'gen': 'm'}],\n",
       "     'mean': [{'text': 'книжка'}, {'text': 'брошюра'}, {'text': 'журнал'}],\n",
       "     'ex': [{'text': 'священные книги', 'tr': [{'text': 'livres sacrés'}]},\n",
       "      {'text': 'адресная книга', 'tr': [{'text': 'adresse carnet'}]}]},\n",
       "    {'text': 'book', 'pos': 'noun', 'gen': 'm'},\n",
       "    {'text': 'ouvrage',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'm',\n",
       "     'mean': [{'text': 'учебник'}],\n",
       "     'ex': [{'text': 'книга под названием',\n",
       "       'tr': [{'text': 'ouvrage intitulé'}]}]},\n",
       "    {'text': 'cahier',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'm',\n",
       "     'mean': [{'text': 'тетрадь'}]},\n",
       "    {'text': 'livre de prix', 'pos': 'noun'}]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://dictionary.yandex.net/api/v1/dicservice.json/lookup\"\n",
    "headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "              \"lang\":\"ru-fr\",\"text\":\"книга\"}\n",
    "ddd = requests.get(url, headers).json()\n",
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname='pgstage', user='linguist', password='eDQGK0GCStlYlHNV', host='192.168.122.183')\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['booker', 'booked', 'booking', 'books']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_forms(word):\n",
    "    req = \"\"\"SELECT DISTINCT\n",
    "    content_words.word_id,content_words.word_lemma, content_words.word_value\n",
    "    FROM content_words\n",
    "    WHERE content_words.word_value !~ ('\\W') AND\n",
    "    (array_length(regexp_split_to_array(content_words.word_value, '[ ''-]'), 1) = 1) \n",
    "    and word_lemma != 0 and word_hash = calc_hash(' \"\"\" + word + \"\"\"')\"\"\"\n",
    "    cursor.execute(req)\n",
    "    req_res = cursor.fetchone()\n",
    "    if not req_res:\n",
    "        print(word, \" is not in db\")\n",
    "        return []\n",
    "    else:\n",
    "        #print(a[0])\n",
    "        word_id = req_res[0]\n",
    "    word_forms_request = \"\"\"SELECT DISTINCT \n",
    "    content_words.word_value\n",
    "    FROM content_words\n",
    "    where word_lemma = \"\"\" + str(word_id) + \"\"\"  and word_lemma != word_id \"\"\"\n",
    "    cursor.execute(word_forms_request)\n",
    "    word_forms = []\n",
    "    for word in cursor:\n",
    "        #print(word[0])\n",
    "        word_forms.append(word[0])\n",
    "    return word_forms\n",
    "get_word_forms(\"book\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = requests.get(url, headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'rezervovat',\n",
       " 'pos': 'verb',\n",
       " 'syn': [{'text': 'objednat', 'pos': 'verb'}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd['def'][1]['tr'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  word\n",
       "1   the\n",
       "2   and\n",
       "3  that\n",
       "4   for"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv(\"test_words.csv\", header = None)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./y_lookup/en_ru/word.json\", 'w') as f:\n",
    "    json.dump(ddd, f, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'word')\n",
      "(1, 'the')\n",
      "(2, 'and')\n",
      "(3, 'that')\n",
      "(4, 'for')\n",
      "(5, 'with')\n",
      "(6, 'but')\n",
      "(7, 'you')\n",
      "(8, 'this')\n",
      "(9, 'not')\n",
      "(10, 'have')\n",
      "(11, 'letter')\n",
      "(12, 'plate')\n",
      "(13, 'treat')\n",
      "(14, 'dinner')\n",
      "(15, 'necessary')\n",
      "(16, 'go')\n",
      "(17, 'secret')\n",
      "(18, 'table')\n",
      "(19, 'create')\n",
      "(20, 'warm')\n",
      "(21, 'America')\n",
      "(22, 'nobody')\n",
      "(23, 'entire')\n",
      "(24, 'thousand')\n",
      "(25, 'Belgium')\n",
      "(26, 'anything')\n",
      "(27, 'background')\n",
      "(28, 'child')\n",
      "(29, 'childhood')\n",
      "(30, 'dependable')\n",
      "(31, 'everything')\n",
      "(32, 'grandchild')\n",
      "(33, 'ketchup')\n",
      "(34, 'point')\n",
      "(35, 'strength')\n",
      "(36, 'star')\n",
      "(37, 'thing')\n",
      "(38, 'coincide')\n",
      "(39, 'coincidental')\n",
      "(40, 'dance')\n",
      "(41, 'countryside')\n",
      "(42, 'break')\n",
      "(43, 'mother')\n",
      "(44, 'start')\n",
      "(45, 'tip')\n",
      "(46, 'apologise')\n",
      "(47, 'come')\n",
      "(48, 'curriculum')\n",
      "(49, 'expertise')\n",
      "(50, 'freeze')\n",
      "(51, 'issue')\n",
      "(52, 'peremptory')\n",
      "(53, 'parent')\n",
      "(54, 'member')\n",
      "(55, 'band')\n",
      "(56, 'order')\n",
      "(57, 'hot')\n",
      "(58, 'fly')\n",
      "(59, 'lie')\n",
      "(60, 'select')\n",
      "(61, 'school')\n",
      "(62, 'cafe')\n",
      "(63, 'buy')\n",
      "(64, 'tell')\n",
      "(65, 'international')\n",
      "(66, 'stand')\n",
      "(67, 'fish')\n",
      "(68, 'benefit')\n",
      "(69, 'intelligent')\n",
      "(70, 'follow')\n",
      "(71, 'journey')\n",
      "(72, 'news')\n",
      "(73, 'write')\n",
      "(74, 'approbation')\n",
      "(75, 'read')\n",
      "(76, 'hear')\n",
      "(77, 'feel')\n",
      "(78, 'be')\n",
      "(79, 'right')\n",
      "(80, 'guru')\n",
      "(81, 'beginner')\n",
      "(82, 'collide')\n",
      "(83, 'genetically')\n",
      "(84, 'precedent')\n",
      "(85, 'refugee')\n",
      "(86, 'secular')\n",
      "(87, 'pasta')\n",
      "(88, 'beaver')\n",
      "(89, 'tiger')\n",
      "(90, 'lion')\n",
      "(91, 'pen')\n",
      "(92, 'coconut')\n",
      "(93, 'dynamite')\n",
      "(94, 'spoon')\n",
      "(95, 'ocean')\n",
      "(96, 'mug')\n",
      "(97, 'shampoo')\n",
      "(98, 'buzzer')\n",
      "(99, 'exhaustively')\n",
      "(100, 'apoplexy')\n",
      "(101, 'submissively')\n",
      "(102, 'typhus')\n",
      "(103, 'ravioli')\n",
      "(104, 'meeting')\n",
      "(105, 'actually')\n",
      "(106, 'earring')\n",
      "(107, 'add')\n",
      "(108, 'adjective')\n",
      "(109, 'drawing')\n",
      "(110, 'dream')\n",
      "(111, 'dressed')\n",
      "(112, 'driving')\n",
      "(113, 'drum')\n",
      "(114, 'dry')\n",
      "(115, 'duck')\n",
      "(116, 'advanced')\n",
      "(117, 'during')\n",
      "(118, 'earn')\n",
      "(119, 'singular')\n",
      "(120, 'easily')\n",
      "(121, 'east')\n",
      "(122, 'eighth')\n",
      "(123, 'adventure')\n",
      "(124, 'eighty')\n",
      "(125, 'adverb')\n",
      "(126, 'electric')\n",
      "(127, 'advertisement')\n",
      "(128, 'century')\n",
      "(129, 'size')\n",
      "(130, 'electricity')\n",
      "(131, 'elephant')\n",
      "(132, 'else')\n",
      "(133, 'empty')\n",
      "(134, 'engine')\n",
      "(135, 'engineer')\n",
      "(136, 'enough')\n",
      "(137, 'enter')\n",
      "(138, 'entrance')\n",
      "(139, 'advice')\n",
      "(140, 'aeroplane')\n",
      "(141, 'envelope')\n",
      "(142, 'especially')\n",
      "(143, 'euro')\n",
      "(144, 'even')\n",
      "(145, 'afraid')\n",
      "(146, 'afterwards')\n",
      "(147, 'everyone')\n",
      "(148, 'against')\n",
      "(149, 'everything')\n",
      "(150, 'everywhere')\n",
      "(151, 'aged')\n",
      "(152, 'ago')\n",
      "(153, 'exactly')\n",
      "(154, 'exam')\n",
      "(155, 'examination')\n",
      "(156, 'excellent')\n",
      "(157, 'except')\n",
      "(158, 'exercise')\n",
      "(159, 'exit')\n",
      "(160, 'explain')\n",
      "(161, 'extra')\n",
      "(162, 'fact')\n",
      "(163, 'fail')\n",
      "(164, 'fair')\n",
      "(165, 'fall')\n",
      "(166, 'agree')\n",
      "(167, 'air')\n",
      "(168, 'airport')\n",
      "(169, 'fan')\n",
      "(170, 'fantastic')\n",
      "(171, 'far')\n",
      "(172, 'farmer')\n",
      "(173, 'fashion')\n",
      "(174, 'quite')\n",
      "(175, 'alarm')\n",
      "(176, 'album')\n",
      "(177, 'alcohol')\n",
      "(178, 'almost')\n",
      "(179, 'alone')\n",
      "(180, 'field')\n",
      "(181, 'fifth')\n",
      "(182, 'fifty')\n",
      "(183, 'file')\n",
      "(184, 'fill')\n",
      "(185, 'final')\n",
      "(186, 'finally')\n",
      "(187, 'finger')\n",
      "(188, 'fire')\n",
      "(189, 'fishing')\n",
      "(190, 'fit')\n",
      "(191, 'flight')\n",
      "(192, 'along')\n",
      "(193, 'bedroom')\n",
      "(194, 'beer')\n",
      "(195, 'before')\n",
      "(196, 'begin')\n",
      "(197, 'behind')\n",
      "(198, 'below')\n",
      "(199, 'best')\n",
      "(200, 'synchrophasotron')\n"
     ]
    }
   ],
   "source": [
    "words = pd.read_csv(\"test_words.csv\", header = None)\n",
    "words.head()\n",
    "wds = []\n",
    "for word in words[0].items():\n",
    "    print(word)\n",
    "    wds.append(word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wds.index('exhaustively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "the\n",
      "and\n",
      "that\n",
      "for\n",
      "with\n",
      "but\n",
      "you\n",
      "this\n",
      "not\n",
      "have\n",
      "letter\n",
      "plate\n",
      "treat\n",
      "dinner\n",
      "necessary\n",
      "go\n",
      "secret\n",
      "table\n",
      "create\n",
      "warm\n",
      "America\n",
      "nobody\n",
      "entire\n",
      "thousand\n",
      "Belgium\n",
      "anything\n",
      "background\n",
      "child\n",
      "childhood\n",
      "dependable\n",
      "everything\n",
      "grandchild\n",
      "ketchup\n",
      "point\n",
      "strength\n",
      "star\n",
      "thing\n",
      "coincide\n",
      "coincidental\n",
      "dance\n",
      "countryside\n",
      "break\n",
      "mother\n",
      "start\n",
      "tip\n",
      "apologise\n",
      "come\n",
      "curriculum\n",
      "expertise\n",
      "freeze\n",
      "issue\n",
      "peremptory\n",
      "parent\n",
      "member\n",
      "band\n",
      "order\n",
      "hot\n",
      "fly\n",
      "lie\n",
      "select\n",
      "school\n",
      "cafe\n",
      "buy\n",
      "tell\n",
      "international\n",
      "stand\n",
      "fish\n",
      "benefit\n",
      "intelligent\n",
      "follow\n",
      "journey\n",
      "news\n",
      "write\n",
      "approbation\n",
      "read\n",
      "hear\n",
      "feel\n",
      "be\n",
      "right\n",
      "guru\n",
      "beginner\n",
      "collide\n",
      "genetically\n",
      "precedent\n",
      "refugee\n",
      "secular\n",
      "pasta\n",
      "beaver\n",
      "tiger\n",
      "lion\n",
      "pen\n",
      "coconut\n",
      "dynamite\n",
      "spoon\n",
      "ocean\n",
      "mug\n",
      "shampoo\n",
      "buzzer\n",
      "exhaustively\n",
      "apoplexy\n",
      "submissively\n",
      "typhus\n",
      "ravioli\n",
      "meeting\n",
      "actually\n",
      "earring\n",
      "add\n",
      "adjective\n",
      "drawing\n",
      "dream\n",
      "dressed\n",
      "driving\n",
      "drum\n",
      "dry\n",
      "duck\n",
      "advanced\n",
      "during\n",
      "earn\n",
      "singular\n",
      "easily\n",
      "east\n",
      "eighth\n",
      "adventure\n",
      "eighty\n",
      "adverb\n",
      "electric\n",
      "advertisement\n",
      "century\n",
      "size\n",
      "electricity\n",
      "elephant\n",
      "else\n",
      "empty\n",
      "engine\n",
      "engineer\n",
      "enough\n",
      "enter\n",
      "entrance\n",
      "advice\n",
      "aeroplane\n",
      "envelope\n",
      "especially\n",
      "euro\n",
      "even\n",
      "afraid\n",
      "afterwards\n",
      "everyone\n",
      "against\n",
      "everything\n",
      "everywhere\n",
      "aged\n",
      "ago\n",
      "exactly\n",
      "exam\n",
      "examination\n",
      "excellent\n",
      "except\n",
      "exercise\n",
      "exit\n",
      "explain\n",
      "extra\n",
      "fact\n",
      "fail\n",
      "fair\n",
      "fall\n",
      "agree\n",
      "air\n",
      "airport\n",
      "fan\n",
      "fantastic\n",
      "far\n",
      "farmer\n",
      "fashion\n",
      "quite\n",
      "alarm\n",
      "album\n",
      "alcohol\n",
      "almost\n",
      "alone\n",
      "field\n",
      "fifth\n",
      "fifty\n",
      "file\n",
      "fill\n",
      "final\n",
      "finally\n",
      "finger\n",
      "fire\n",
      "fishing\n",
      "fit\n",
      "flight\n",
      "along\n",
      "bedroom\n",
      "beer\n",
      "before\n",
      "begin\n",
      "behind\n",
      "below\n",
      "best\n",
      "synchrophasotron\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for word in words[0].items():\n",
    "    print (word[1])\n",
    "    headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "          \"lang\":\"en-ru\",\"text\":word[1]}\n",
    "    ddd = requests.get(url, headers).json()\n",
    "    save_loc = \"./y_lookup/yandex_lookup_big_en_ru/\" + word[1] + \".json\"\n",
    "    with open(save_loc, 'w') as f:\n",
    "        json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "    count += 1\n",
    "    #if count > 1:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      ">>> wording\n",
      ">>> worded\n",
      ">>> words\n",
      "the\n",
      ">>> thebed\n",
      ">>> thest\n",
      ">>> the bed\n",
      ">>> thebest\n",
      ">>> thes\n",
      "and\n",
      ">>> anding\n",
      ">>> ands\n",
      ">>> ood\n",
      ">>> andren\n",
      ">>> anded\n",
      ">>> ander\n",
      "that\n",
      ">>> that led\n",
      ">>> thats\n",
      ">>> those\n",
      "for\n",
      ">>> forring\n",
      ">>> fored\n",
      ">>> fors\n",
      "with\n",
      ">>> withim\n",
      ">>> withing\n",
      ">>> withs\n",
      "but\n",
      ">>> buts\n",
      "you\n",
      ">>> youed\n",
      ">>> yous\n",
      ">>> youta\n",
      ">>> youx\n",
      ">>> youing\n",
      ">>> youger\n",
      "this\n",
      ">>> thises\n",
      ">>> this led\n",
      ">>> these\n",
      "not\n",
      ">>> not bed\n",
      ">>> notest\n",
      ">>> notting\n",
      ">>> nots\n",
      "have\n",
      ">>> have led\n",
      ">>> haveta\n",
      ">>> haveing\n",
      ">>> had\n",
      ">>> haved\n",
      ">>> haves\n",
      ">>> having\n",
      ">>> havest\n",
      "letter\n",
      ">>> lettered\n",
      ">>> letterd\n",
      ">>> lettering\n",
      ">>> letters\n",
      ">>> lettera\n",
      "plate\n",
      ">>> plating\n",
      ">>> plated\n",
      ">>> plates\n",
      "treat\n",
      ">>> treats\n",
      ">>> treated\n",
      ">>> treaten\n",
      ">>> treating\n",
      ">>> treatest\n",
      "dinner\n",
      ">>> dinners\n",
      "necessary\n",
      ">>> necessarie\n",
      "go\n",
      ">>> gota\n",
      ">>> gos\n",
      ">>> gone\n",
      ">>> went\n",
      ">>> goed\n",
      ">>> goes\n",
      ">>> goren\n",
      ">>> going\n",
      "secret\n",
      ">>> secrets\n",
      ">>> secret\n",
      ">>> secreter\n",
      "table\n",
      ">>> tables\n",
      ">>> tabled\n",
      ">>> tabling\n",
      ">>> tabler\n",
      "create\n",
      ">>> creating\n",
      ">>> creates\n",
      ">>> createing\n",
      ">>> created\n",
      "warm\n",
      ">>> warmest\n",
      ">>> warmed\n",
      ">>> warms\n",
      ">>> warmer\n",
      ">>> warming\n",
      "America\n",
      ">>> americae\n",
      ">>> americas\n",
      ">>> america\n",
      "nobody\n",
      ">>> nobodies\n",
      ">>> nobodys\n",
      "entire\n",
      ">>> entire\n",
      ">>> entired\n",
      "thousand\n",
      ">>> thousands\n",
      "Belgium\n",
      ">>> belgiums\n",
      "anything\n",
      ">>> anythings\n",
      "background\n",
      ">>> background\n",
      "child\n",
      ">>> children\n",
      ">>> childer\n",
      ">>> childs\n",
      ">>> chilt\n",
      "childhood\n",
      ">>> childhoods\n",
      ">>> childhood\n",
      "dependable\n",
      ">>> dependable\n",
      "everything\n",
      ">>> everythung\n",
      ">>> everything\n",
      "grandchild\n",
      ">>> grandchild\n",
      "ketchup\n",
      ">>> ketchups\n",
      ">>> ketchuped\n",
      "point\n",
      ">>> pointing\n",
      ">>> pointest\n",
      ">>> points\n",
      ">>> pointed\n",
      ">>> point\n",
      ">>> pointtest\n",
      "strength\n",
      ">>> strengths\n",
      "star\n",
      ">>> star\n",
      ">>> starred\n",
      ">>> starring\n",
      ">>> starrer\n",
      ">>> stars\n",
      ">>> starer\n",
      "thing\n",
      ">>> thung\n",
      ">>> things\n",
      "coincide\n",
      ">>> coincided\n",
      ">>> coinciding\n",
      ">>> coincides\n",
      "coincidental\n",
      "dance\n",
      ">>> danceing\n",
      ">>> dancing\n",
      ">>> dances\n",
      ">>> danced\n",
      "countryside\n",
      ">>> countrysid\n",
      "break\n",
      ">>> breaks\n",
      ">>> broke\n",
      ">>> breakest\n",
      ">>> breaking\n",
      ">>> broken\n",
      ">>> breaked\n",
      "mother\n",
      ">>> mothers\n",
      ">>> mothering\n",
      ">>> motherin\n",
      ">>> mothered\n",
      "start\n",
      ">>> starttest\n",
      ">>> startd\n",
      ">>> started\n",
      ">>> starts\n",
      ">>> startted\n",
      ">>> starting\n",
      "tip\n",
      ">>> tipping\n",
      ">>> tipped\n",
      ">>> tips\n",
      ">>> tiped\n",
      "apologise\n",
      ">>> apologised\n",
      ">>> apologisin\n",
      ">>> apologises\n",
      "come\n",
      ">>> came\n",
      ">>> comeing\n",
      ">>> comest\n",
      ">>> coming\n",
      ">>> comes\n",
      "curriculum\n",
      ">>> curricula\n",
      ">>> curriculum\n",
      "expertise\n",
      ">>> expertised\n",
      ">>> expertise\n",
      ">>> expertisin\n",
      ">>> expertises\n",
      "freeze\n",
      ">>> freezing\n",
      ">>> froze\n",
      ">>> frozen\n",
      ">>> freezes\n",
      ">>> freezed\n",
      "issue\n",
      ">>> issue\n",
      ">>> issueing\n",
      ">>> issued\n",
      ">>> issuing\n",
      ">>> issues\n",
      "peremptory\n",
      ">>> peremptori\n",
      "parent\n",
      ">>> parented\n",
      ">>> parenting\n",
      ">>> parent\n",
      ">>> parents\n",
      "member\n",
      ">>> members\n",
      "band\n",
      ">>> banding\n",
      ">>> bands\n",
      ">>> bood\n",
      ">>> banded\n",
      "order\n",
      ">>> orders\n",
      ">>> orderi\n",
      ">>> ordering\n",
      ">>> ordered\n",
      ">>> orderest\n",
      "hot\n",
      ">>> hots\n",
      ">>> hoting\n",
      ">>> hotted\n",
      ">>> hotest\n",
      ">>> hotter\n",
      ">>> hotting\n",
      ">>> hoted\n",
      ">>> hottest\n",
      "fly\n",
      ">>> flid\n",
      ">>> flyest\n",
      ">>> flew\n",
      ">>> flying\n",
      ">>> flyed\n",
      ">>> fliest\n",
      ">>> flies\n",
      ">>> flied\n",
      ">>> flys\n",
      "lie\n",
      ">>> lies\n",
      ">>> lying\n",
      ">>> lieing\n",
      ">>> lied\n",
      ">>> lain\n",
      ">>> liebing\n",
      "select\n",
      ">>> selects\n",
      ">>> selected\n",
      ">>> selectin\n",
      ">>> selectest\n",
      ">>> selectted\n",
      ">>> selecting\n",
      "school\n",
      ">>> schools\n",
      ">>> schooling\n",
      ">>> schooled\n",
      "cafe\n",
      ">>> cafes\n",
      "buy\n",
      ">>> buys\n",
      ">>> buying\n",
      ">>> buyest\n",
      ">>> buies\n",
      ">>> buyed\n",
      "tell\n",
      ">>> tellest\n",
      ">>> tollen\n",
      ">>> telling\n",
      ">>> tells\n",
      ">>> told\n",
      "international\n",
      ">>> internatio\n",
      "stand\n",
      ">>> standed\n",
      ">>> stands\n",
      ">>> standest\n",
      ">>> stander\n",
      ">>> stood\n",
      ">>> standing\n",
      "fish\n",
      ">>> fishd\n",
      ">>> fishing\n",
      ">>> fishes\n",
      ">>> fishs\n",
      ">>> fished\n",
      ">>> fishbed\n",
      "benefit\n",
      ">>> benefits\n",
      ">>> benefittin\n",
      ">>> benefitted\n",
      ">>> benefited\n",
      ">>> benefiting\n",
      "intelligent\n",
      ">>> intelligen\n",
      "follow\n",
      ">>> follows\n",
      ">>> followed\n",
      ">>> following\n",
      "journey\n",
      ">>> journeying\n",
      ">>> journeys\n",
      ">>> journeyd\n",
      ">>> journeyin\n",
      ">>> journeyed\n",
      "news\n",
      ">>> newses\n",
      "write\n",
      ">>> writes\n",
      ">>> wrote\n",
      ">>> writing\n",
      ">>> written\n",
      ">>> writeing\n",
      "approbation\n",
      ">>> approbatio\n",
      "read\n",
      ">>> readest\n",
      ">>> reads\n",
      ">>> reader\n",
      ">>> reading\n",
      ">>> readed\n",
      "hear\n",
      ">>> hearred\n",
      ">>> hearing\n",
      ">>> hearther\n",
      ">>> heared\n",
      ">>> horne\n",
      ">>> heard\n",
      ">>> hears\n",
      "feel\n",
      ">>> feeling\n",
      ">>> feelest\n",
      ">>> felt\n",
      ">>> feelling\n",
      ">>> feels\n",
      ">>> feeled\n",
      "be\n",
      ">>> beger\n",
      ">>> bes\n",
      ">>> is\n",
      ">>> am\n",
      ">>> will\n",
      ">>> being\n",
      ">>> were\n",
      ">>> been\n",
      ">>> beren\n",
      ">>> was\n",
      "right\n",
      ">>> rights\n",
      ">>> rightest\n",
      ">>> righter\n",
      ">>> righted\n",
      ">>> righting\n",
      "guru\n",
      ">>> guru\n",
      ">>> gurus\n",
      "beginner\n",
      ">>> beginners\n",
      "collide\n",
      ">>> collode\n",
      ">>> collided\n",
      ">>> collides\n",
      ">>> colliding\n",
      "genetically\n",
      "precedent\n",
      ">>> precedents\n",
      "refugee\n",
      ">>> refugees\n",
      ">>> refugeeing\n",
      "secular\n",
      ">>> seculars\n",
      "pasta\n",
      ">>> pastas\n",
      "beaver\n",
      ">>> beavered\n",
      ">>> beavers\n",
      ">>> beavering\n",
      "tiger\n",
      ">>> tigers\n",
      ">>> tigering\n",
      ">>> tigered\n",
      "lion\n",
      ">>> lions\n",
      "pen\n",
      ">>> penning\n",
      ">>> pened\n",
      ">>> pens\n",
      ">>> penned\n",
      "coconut\n",
      ">>> coconuts\n",
      "dynamite\n",
      ">>> dynamites\n",
      ">>> dynamited\n",
      ">>> dynamiting\n",
      "spoon\n",
      ">>> spoons\n",
      ">>> spoonin\n",
      ">>> spooned\n",
      ">>> spooning\n",
      "ocean\n",
      ">>> ocean\n",
      ">>> oceaner\n",
      ">>> oceans\n",
      "mug\n",
      ">>> mugging\n",
      ">>> mugged\n",
      ">>> mugs\n",
      "shampoo\n",
      ">>> shampoos\n",
      ">>> shampooing\n",
      ">>> shampooed\n",
      "buzzer\n",
      ">>> buzzers\n",
      "exhaustively\n",
      "exhaustively  is not in db\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-04c08563a20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwordforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for word in words[0].items():\n",
    "    print (word[1])\n",
    "    wordforms = get_word_forms(word[1].lower())\n",
    "    for wf in wordforms:\n",
    "        print(\">>>\", wf)\n",
    "        headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "              \"lang\":\"en-ru\",\"text\":wf}\n",
    "        ddd = requests.get(url, headers).json()\n",
    "        save_loc = \"./y_lookup/en_ru/\" + wf + \".json\"\n",
    "        with open(save_loc, 'w') as f:\n",
    "            json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "    count += 1\n",
    "    if count > 100:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      ">>> wording\n",
      ">>> worded\n",
      ">>> words\n",
      "the\n",
      ">>> thebed\n",
      ">>> thest\n",
      ">>> the bed\n",
      ">>> thebest\n",
      ">>> thes\n",
      "and\n",
      ">>> anding\n",
      ">>> ands\n",
      ">>> ood\n",
      ">>> andren\n",
      ">>> anded\n",
      ">>> ander\n",
      "that\n",
      ">>> that led\n",
      ">>> thats\n",
      ">>> those\n",
      "for\n",
      ">>> forring\n",
      ">>> fored\n",
      ">>> fors\n",
      "with\n",
      ">>> withim\n",
      ">>> withing\n",
      ">>> withs\n",
      "but\n",
      ">>> buts\n",
      "you\n",
      ">>> youed\n",
      ">>> yous\n",
      ">>> youta\n",
      ">>> youx\n",
      ">>> youing\n",
      ">>> youger\n",
      "this\n",
      ">>> thises\n",
      ">>> this led\n",
      ">>> these\n",
      "not\n",
      ">>> not bed\n",
      ">>> notest\n",
      ">>> notting\n",
      ">>> nots\n",
      "have\n",
      ">>> have led\n",
      ">>> haveta\n",
      ">>> haveing\n",
      ">>> had\n",
      ">>> haved\n",
      ">>> haves\n",
      ">>> having\n",
      ">>> havest\n",
      "letter\n",
      ">>> lettered\n",
      ">>> letterd\n",
      ">>> lettering\n",
      ">>> letters\n",
      ">>> lettera\n",
      "plate\n",
      ">>> plating\n",
      ">>> plated\n",
      ">>> plates\n",
      "treat\n",
      ">>> treats\n",
      ">>> treated\n",
      ">>> treaten\n",
      ">>> treating\n",
      ">>> treatest\n",
      "dinner\n",
      ">>> dinners\n",
      "necessary\n",
      ">>> necessarie\n",
      "go\n",
      ">>> gota\n",
      ">>> gos\n",
      ">>> gone\n",
      ">>> went\n",
      ">>> goed\n",
      ">>> goes\n",
      ">>> goren\n",
      ">>> going\n",
      "secret\n",
      ">>> secrets\n",
      ">>> secret\n",
      ">>> secreter\n",
      "table\n",
      ">>> tables\n",
      ">>> tabled\n",
      ">>> tabling\n",
      ">>> tabler\n",
      "create\n",
      ">>> creating\n",
      ">>> creates\n",
      ">>> createing\n",
      ">>> created\n",
      "warm\n",
      ">>> warmest\n",
      ">>> warmed\n",
      ">>> warms\n",
      ">>> warmer\n",
      ">>> warming\n",
      "America\n",
      ">>> americae\n",
      ">>> americas\n",
      ">>> america\n",
      "nobody\n",
      ">>> nobodies\n",
      ">>> nobodys\n",
      "entire\n",
      ">>> entire\n",
      ">>> entired\n",
      "thousand\n",
      ">>> thousands\n",
      "Belgium\n",
      ">>> belgiums\n",
      "anything\n",
      ">>> anythings\n",
      "background\n",
      ">>> background\n",
      "child\n",
      ">>> children\n",
      ">>> childer\n",
      ">>> childs\n",
      ">>> chilt\n",
      "childhood\n",
      ">>> childhoods\n",
      ">>> childhood\n",
      "dependable\n",
      ">>> dependable\n",
      "everything\n",
      ">>> everythung\n",
      ">>> everything\n",
      "grandchild\n",
      ">>> grandchild\n",
      "ketchup\n",
      ">>> ketchups\n",
      ">>> ketchuped\n",
      "point\n",
      ">>> pointing\n",
      ">>> pointest\n",
      ">>> points\n",
      ">>> pointed\n",
      ">>> point\n",
      ">>> pointtest\n",
      "strength\n",
      ">>> strengths\n",
      "star\n",
      ">>> star\n",
      ">>> starred\n",
      ">>> starring\n",
      ">>> starrer\n",
      ">>> stars\n",
      ">>> starer\n",
      "thing\n",
      ">>> thung\n",
      ">>> things\n",
      "coincide\n",
      ">>> coincided\n",
      ">>> coinciding\n",
      ">>> coincides\n",
      "coincidental\n",
      "dance\n",
      ">>> danceing\n",
      ">>> dancing\n",
      ">>> dances\n",
      ">>> danced\n",
      "countryside\n",
      ">>> countrysid\n",
      "break\n",
      ">>> breaks\n",
      ">>> broke\n",
      ">>> breakest\n",
      ">>> breaking\n",
      ">>> broken\n",
      ">>> breaked\n",
      "mother\n",
      ">>> mothers\n",
      ">>> mothering\n",
      ">>> motherin\n",
      ">>> mothered\n",
      "start\n",
      ">>> starttest\n",
      ">>> startd\n",
      ">>> started\n",
      ">>> starts\n",
      ">>> startted\n",
      ">>> starting\n",
      "tip\n",
      ">>> tipping\n",
      ">>> tipped\n",
      ">>> tips\n",
      ">>> tiped\n",
      "apologise\n",
      ">>> apologised\n",
      ">>> apologisin\n",
      ">>> apologises\n",
      "come\n",
      ">>> came\n",
      ">>> comeing\n",
      ">>> comest\n",
      ">>> coming\n",
      ">>> comes\n",
      "curriculum\n",
      ">>> curricula\n",
      ">>> curriculum\n",
      "expertise\n",
      ">>> expertised\n",
      ">>> expertise\n",
      ">>> expertisin\n",
      ">>> expertises\n",
      "freeze\n",
      ">>> freezing\n",
      ">>> froze\n",
      ">>> frozen\n",
      ">>> freezes\n",
      ">>> freezed\n",
      "issue\n",
      ">>> issue\n",
      ">>> issueing\n",
      ">>> issued\n",
      ">>> issuing\n",
      ">>> issues\n",
      "peremptory\n",
      ">>> peremptori\n",
      "parent\n",
      ">>> parented\n",
      ">>> parenting\n",
      ">>> parent\n",
      ">>> parents\n",
      "member\n",
      ">>> members\n",
      "band\n",
      ">>> banding\n",
      ">>> bands\n",
      ">>> bood\n",
      ">>> banded\n",
      "order\n",
      ">>> orders\n",
      ">>> orderi\n",
      ">>> ordering\n",
      ">>> ordered\n",
      ">>> orderest\n",
      "hot\n",
      ">>> hots\n",
      ">>> hoting\n",
      ">>> hotted\n",
      ">>> hotest\n",
      ">>> hotter\n",
      ">>> hotting\n",
      ">>> hoted\n",
      ">>> hottest\n",
      "fly\n",
      ">>> flid\n",
      ">>> flyest\n",
      ">>> flew\n",
      ">>> flying\n",
      ">>> flyed\n",
      ">>> fliest\n",
      ">>> flies\n",
      ">>> flied\n",
      ">>> flys\n",
      "lie\n",
      ">>> lies\n",
      ">>> lying\n",
      ">>> lieing\n",
      ">>> lied\n",
      ">>> lain\n",
      ">>> liebing\n",
      "select\n",
      ">>> selects\n",
      ">>> selected\n",
      ">>> selectin\n",
      ">>> selectest\n",
      ">>> selectted\n",
      ">>> selecting\n",
      "school\n",
      ">>> schools\n",
      ">>> schooling\n",
      ">>> schooled\n",
      "cafe\n",
      ">>> cafes\n",
      "buy\n",
      ">>> buys\n",
      ">>> buying\n",
      ">>> buyest\n",
      ">>> buies\n",
      ">>> buyed\n",
      "tell\n",
      ">>> tellest\n",
      ">>> tollen\n",
      ">>> telling\n",
      ">>> tells\n",
      ">>> told\n",
      "international\n",
      ">>> internatio\n",
      "stand\n",
      ">>> standed\n",
      ">>> stands\n",
      ">>> standest\n",
      ">>> stander\n",
      ">>> stood\n",
      ">>> standing\n",
      "fish\n",
      ">>> fishd\n",
      ">>> fishing\n",
      ">>> fishes\n",
      ">>> fishs\n",
      ">>> fished\n",
      ">>> fishbed\n",
      "benefit\n",
      ">>> benefits\n",
      ">>> benefittin\n",
      ">>> benefitted\n",
      ">>> benefited\n",
      ">>> benefiting\n",
      "intelligent\n",
      ">>> intelligen\n",
      "follow\n",
      ">>> follows\n",
      ">>> followed\n",
      ">>> following\n",
      "journey\n",
      ">>> journeying\n",
      ">>> journeys\n",
      ">>> journeyd\n",
      ">>> journeyin\n",
      ">>> journeyed\n",
      "news\n",
      ">>> newses\n",
      "write\n",
      ">>> writes\n",
      ">>> wrote\n",
      ">>> writing\n",
      ">>> written\n",
      ">>> writeing\n",
      "approbation\n",
      ">>> approbatio\n",
      "read\n",
      ">>> readest\n",
      ">>> reads\n",
      ">>> reader\n",
      ">>> reading\n",
      ">>> readed\n",
      "hear\n",
      ">>> hearred\n",
      ">>> hearing\n",
      ">>> hearther\n",
      ">>> heared\n",
      ">>> horne\n",
      ">>> heard\n",
      ">>> hears\n",
      "feel\n",
      ">>> feeling\n",
      ">>> feelest\n",
      ">>> felt\n",
      ">>> feelling\n",
      ">>> feels\n",
      ">>> feeled\n",
      "be\n",
      ">>> beger\n",
      ">>> bes\n",
      ">>> is\n",
      ">>> am\n",
      ">>> will\n",
      ">>> being\n",
      ">>> were\n",
      ">>> been\n",
      ">>> beren\n",
      ">>> was\n",
      "right\n",
      ">>> rights\n",
      ">>> rightest\n",
      ">>> righter\n",
      ">>> righted\n",
      ">>> righting\n",
      "guru\n",
      ">>> guru\n",
      ">>> gurus\n",
      "beginner\n",
      ">>> beginners\n",
      "collide\n",
      ">>> collode\n",
      ">>> collided\n",
      ">>> collides\n",
      ">>> colliding\n",
      "genetically\n",
      "precedent\n",
      ">>> precedents\n",
      "refugee\n",
      ">>> refugees\n",
      ">>> refugeeing\n",
      "secular\n",
      ">>> seculars\n",
      "pasta\n",
      ">>> pastas\n",
      "beaver\n",
      ">>> beavered\n",
      ">>> beavers\n",
      ">>> beavering\n",
      "tiger\n",
      ">>> tigers\n",
      ">>> tigering\n",
      ">>> tigered\n",
      "lion\n",
      ">>> lions\n",
      "pen\n",
      ">>> penning\n",
      ">>> pened\n",
      ">>> pens\n",
      ">>> penned\n",
      "coconut\n",
      ">>> coconuts\n",
      "dynamite\n",
      ">>> dynamites\n",
      ">>> dynamited\n",
      ">>> dynamiting\n",
      "spoon\n",
      ">>> spoons\n",
      ">>> spoonin\n",
      ">>> spooned\n",
      ">>> spooning\n",
      "ocean\n",
      ">>> ocean\n",
      ">>> oceaner\n",
      ">>> oceans\n",
      "mug\n",
      ">>> mugging\n",
      ">>> mugged\n",
      ">>> mugs\n",
      "shampoo\n",
      ">>> shampoos\n",
      ">>> shampooing\n",
      ">>> shampooed\n",
      "buzzer\n",
      ">>> buzzers\n",
      "exhaustively\n",
      "exhaustively  is not in db\n",
      "apoplexy\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for word in words[0].items():\n",
    "    print (word[1])\n",
    "    wordforms = get_word_forms(word[1].lower())\n",
    "    for wf in wordforms:\n",
    "        print(\">>>\", wf)\n",
    "        headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "              \"lang\":\"en-fr\",\"text\":wf}\n",
    "        ddd = requests.get(url, headers).json()\n",
    "        save_loc = \"./y_lookup/en_fr/\" + wf + \".json\"\n",
    "        with open(save_loc, 'w') as f:\n",
    "            json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "    count += 1\n",
    "    if count > 100:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "the\n",
      "and\n",
      "that\n",
      "for\n",
      "with\n",
      "but\n",
      "you\n",
      "this\n",
      "not\n",
      "have\n",
      "letter\n",
      "plate\n",
      "treat\n",
      "dinner\n",
      "necessary\n",
      "go\n",
      "secret\n",
      "table\n",
      "create\n",
      "warm\n",
      "America\n",
      "nobody\n",
      "entire\n",
      "thousand\n",
      "Belgium\n",
      "anything\n",
      "background\n",
      "child\n",
      "childhood\n",
      "dependable\n",
      "everything\n",
      "grandchild\n",
      "ketchup\n",
      "point\n",
      "strength\n",
      "star\n",
      "thing\n",
      "coincide\n",
      "coincidental\n",
      "dance\n",
      "countryside\n",
      "break\n",
      "mother\n",
      "start\n",
      "tip\n",
      "apologise\n",
      "come\n",
      "curriculum\n",
      "expertise\n",
      "freeze\n",
      "issue\n",
      "peremptory\n",
      "parent\n",
      "member\n",
      "band\n",
      "order\n",
      "hot\n",
      "fly\n",
      "lie\n",
      "select\n",
      "school\n",
      "cafe\n",
      "buy\n",
      "tell\n",
      "international\n",
      "stand\n",
      "fish\n",
      "benefit\n",
      "intelligent\n",
      "follow\n",
      "journey\n",
      "news\n",
      "write\n",
      "approbation\n",
      "read\n",
      "hear\n",
      "feel\n",
      "be\n",
      "right\n",
      "guru\n",
      "beginner\n",
      "collide\n",
      "genetically\n",
      "precedent\n",
      "refugee\n",
      "secular\n",
      "pasta\n",
      "beaver\n",
      "tiger\n",
      "lion\n",
      "pen\n",
      "coconut\n",
      "dynamite\n",
      "spoon\n",
      "ocean\n",
      "mug\n",
      "shampoo\n",
      "buzzer\n",
      "exhaustively\n",
      "apoplexy\n",
      "submissively\n",
      "typhus\n",
      "ravioli\n",
      "meeting\n",
      "actually\n",
      "earring\n",
      "add\n",
      "adjective\n",
      "drawing\n",
      "dream\n",
      "dressed\n",
      "driving\n",
      "drum\n",
      "dry\n",
      "duck\n",
      "advanced\n",
      "during\n",
      "earn\n",
      "singular\n",
      "easily\n",
      "east\n",
      "eighth\n",
      "adventure\n",
      "eighty\n",
      "adverb\n",
      "electric\n",
      "advertisement\n",
      "century\n",
      "size\n",
      "electricity\n",
      "elephant\n",
      "else\n",
      "empty\n",
      "engine\n",
      "engineer\n",
      "enough\n",
      "enter\n",
      "entrance\n",
      "advice\n",
      "aeroplane\n",
      "envelope\n",
      "especially\n",
      "euro\n",
      "even\n",
      "afraid\n",
      "afterwards\n",
      "everyone\n",
      "against\n",
      "everything\n",
      "everywhere\n",
      "aged\n",
      "ago\n",
      "exactly\n",
      "exam\n",
      "examination\n",
      "excellent\n",
      "except\n",
      "exercise\n",
      "exit\n",
      "explain\n",
      "extra\n",
      "fact\n",
      "fail\n",
      "fair\n",
      "fall\n",
      "agree\n",
      "air\n",
      "airport\n",
      "fan\n",
      "fantastic\n",
      "far\n",
      "farmer\n",
      "fashion\n",
      "quite\n",
      "alarm\n",
      "album\n",
      "alcohol\n",
      "almost\n",
      "alone\n",
      "field\n",
      "fifth\n",
      "fifty\n",
      "file\n",
      "fill\n",
      "final\n",
      "finally\n",
      "finger\n",
      "fire\n",
      "fishing\n",
      "fit\n",
      "flight\n",
      "along\n",
      "bedroom\n",
      "beer\n",
      "before\n",
      "begin\n",
      "behind\n",
      "below\n",
      "best\n",
      "synchrophasotron\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for word in words[0].items():\n",
    "    print (word[1])\n",
    "    headers = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\",\n",
    "          \"lang\":\"en-fr\",\"text\":word[1]}\n",
    "    ddd = requests.get(url, headers).json()\n",
    "    save_loc = \"./y_lookup/en_fr_whole_list/\" + word[1] + \".json\"\n",
    "    with open(save_loc, 'w') as f:\n",
    "        json.dump(ddd, f, indent = 4, ensure_ascii=False)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': {},\n",
       " 'def': [{'text': 'book',\n",
       "   'pos': 'noun',\n",
       "   'ts': 'bʊk',\n",
       "   'tr': [{'text': 'книга',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'ж',\n",
       "     'syn': [{'text': 'книжка', 'pos': 'noun', 'gen': 'ж'},\n",
       "      {'text': 'книжечка', 'pos': 'noun', 'gen': 'ж'},\n",
       "      {'text': 'книжица', 'pos': 'noun', 'gen': 'ж'}],\n",
       "     'mean': [{'text': 'volume'}, {'text': 'booklet'}],\n",
       "     'ex': [{'text': 'new book', 'tr': [{'text': 'новая книга'}]},\n",
       "      {'text': 'red data book', 'tr': [{'text': 'красная книга'}]},\n",
       "      {'text': \"children's book\", 'tr': [{'text': 'детская книга'}]},\n",
       "      {'text': 'personal address book',\n",
       "       'tr': [{'text': 'личная адресная книга'}]},\n",
       "      {'text': 'very good book', 'tr': [{'text': 'очень хорошая книга'}]},\n",
       "      {'text': 'recently published book',\n",
       "       'tr': [{'text': 'недавно опубликованная книга'}]},\n",
       "      {'text': 'local phone book', 'tr': [{'text': 'телефонная книга'}]},\n",
       "      {'text': 'book of mormon', 'tr': [{'text': 'книга мормона'}]},\n",
       "      {'text': 'cheque book', 'tr': [{'text': 'чековая книжка'}]},\n",
       "      {'text': 'little book', 'tr': [{'text': 'маленькая книжечка'}]}]},\n",
       "    {'text': 'сборник',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'м',\n",
       "     'mean': [{'text': 'collection'}]},\n",
       "    {'text': 'справочник',\n",
       "     'pos': 'noun',\n",
       "     'gen': 'м',\n",
       "     'mean': [{'text': 'reference book'}]}]},\n",
       "  {'text': 'book',\n",
       "   'pos': 'verb',\n",
       "   'ts': 'bʊk',\n",
       "   'tr': [{'text': 'бронировать',\n",
       "     'pos': 'verb',\n",
       "     'syn': [{'text': 'забронировать', 'pos': 'verb', 'asp': 'сов'}],\n",
       "     'mean': [{'text': 'reserve'}],\n",
       "     'ex': [{'text': 'book a room', 'tr': [{'text': 'забронировать номер'}]}]},\n",
       "    {'text': 'заказывать',\n",
       "     'pos': 'verb',\n",
       "     'asp': 'несов',\n",
       "     'syn': [{'text': 'заказать', 'pos': 'verb', 'asp': 'сов'}],\n",
       "     'mean': [{'text': 'order'}],\n",
       "     'ex': [{'text': 'book tickets',\n",
       "       'tr': [{'text': 'заказывать билеты'}]}]}]},\n",
       "  {'text': 'book',\n",
       "   'pos': 'adjective',\n",
       "   'ts': 'bʊk',\n",
       "   'tr': [{'text': 'книжный',\n",
       "     'pos': 'adjective',\n",
       "     'mean': [{'text': 'literary'}],\n",
       "     'ex': [{'text': 'national book award',\n",
       "       'tr': [{'text': 'национальная книжная премия'}]}]}]}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://dictionary.yandex.net/api/v1/dicservice.json/getLangs\"\n",
    "h = {\"key\":\"dict.1.1.20190828T215834Z.0d8a3da5e08df3a0.85290ff28457230b81cb3b73720d2a0312ca14dc\"}\n",
    "d = requests.get(url, h).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['be-be',\n",
    " 'be-ru',\n",
    " 'bg-ru',\n",
    " 'cs-cs',\n",
    " 'cs-en',\n",
    " 'cs-ru',\n",
    " 'da-en',\n",
    " 'da-ru',\n",
    " 'de-de',\n",
    " 'de-en',\n",
    " 'de-ru',\n",
    " 'de-tr',\n",
    " 'el-en',\n",
    " 'el-ru',\n",
    " 'en-cs',\n",
    " 'en-da',\n",
    " 'en-de',\n",
    " 'en-el',\n",
    " 'en-en',\n",
    " 'en-es',\n",
    " 'en-et',\n",
    " 'en-fi',\n",
    " 'en-fr',\n",
    " 'en-it',\n",
    " 'en-lt',\n",
    " 'en-lv',\n",
    " 'en-nl',\n",
    " 'en-no',\n",
    " 'en-pt',\n",
    " 'en-ru',\n",
    " 'en-sk',\n",
    " 'en-sv',\n",
    " 'en-tr',\n",
    " 'en-uk',\n",
    " 'es-en',\n",
    " 'es-es',\n",
    " 'es-ru',\n",
    " 'et-en',\n",
    " 'et-ru',\n",
    " 'fi-en',\n",
    " 'fi-ru',\n",
    " 'fi-fi',\n",
    " 'fr-fr',\n",
    " 'fr-en',\n",
    " 'fr-ru',\n",
    " 'hu-hu',\n",
    " 'hu-ru',\n",
    " 'it-en',\n",
    " 'it-it',\n",
    " 'it-ru',\n",
    " 'lt-en',\n",
    " 'lt-lt',\n",
    " 'lt-ru',\n",
    " 'lv-en',\n",
    " 'lv-ru',\n",
    " 'mhr-ru',\n",
    " 'mrj-ru',\n",
    " 'nl-en',\n",
    " 'nl-ru',\n",
    " 'no-en',\n",
    " 'no-ru',\n",
    " 'pl-ru',\n",
    " 'pt-en',\n",
    " 'pt-ru',\n",
    " 'ru-be',\n",
    " 'ru-bg',\n",
    " 'ru-cs',\n",
    " 'ru-da',\n",
    " 'ru-de',\n",
    " 'ru-el',\n",
    " 'ru-en',\n",
    " 'ru-es',\n",
    " 'ru-et',\n",
    " 'ru-fi',\n",
    " 'ru-fr',\n",
    " 'ru-hu',\n",
    " 'ru-it',\n",
    " 'ru-lt',\n",
    " 'ru-lv',\n",
    " 'ru-mhr',\n",
    " 'ru-mrj',\n",
    " 'ru-nl',\n",
    " 'ru-no',\n",
    " 'ru-pl',\n",
    " 'ru-pt',\n",
    " 'ru-ru',\n",
    " 'ru-sk',\n",
    " 'ru-sv',\n",
    " 'ru-tr',\n",
    " 'ru-tt',\n",
    " 'ru-uk',\n",
    " 'sk-en',\n",
    " 'sk-ru',\n",
    " 'sv-en',\n",
    " 'sv-ru',\n",
    " 'tr-de',\n",
    " 'tr-en',\n",
    " 'tr-ru',\n",
    " 'tt-ru',\n",
    " 'uk-en',\n",
    " 'uk-ru',\n",
    " 'uk-uk',\n",
    " 'zh-ru']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
